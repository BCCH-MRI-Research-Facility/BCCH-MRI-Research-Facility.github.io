[
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Docker\n\n\n\n\n\n\nsoftware\n\n\nreproducible\n\n\n\nAutomates the Deployment of Software Applications Inside Containers\n\n\n\n\n\nFeb 13, 2025\n\n\nAlexander Weber\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Tutorials"
    ]
  },
  {
    "objectID": "scienceresources/index.html",
    "href": "scienceresources/index.html",
    "title": "Science Resources",
    "section": "",
    "text": "No matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Science Resources"
    ]
  },
  {
    "objectID": "orientation/instantmessaging.html",
    "href": "orientation/instantmessaging.html",
    "title": "Mattermost: Instant Messaging / Stay in Touch",
    "section": "",
    "text": "Mattermost is an open-source alternative to Slack and Microsoft Teams. Similar to those programs, it is a tool for instant messaging and collaboration.  If you have MRI questions or want to connect with other BCCH MRI Research Facility users, you can use Mattermost to connect with us.",
    "crumbs": [
      "Home",
      "Orientation",
      "Mattermost: Instant Messaging / Stay in Touch"
    ]
  },
  {
    "objectID": "orientation/instantmessaging.html#bcchr-it-instructions",
    "href": "orientation/instantmessaging.html#bcchr-it-instructions",
    "title": "Mattermost: Instant Messaging / Stay in Touch",
    "section": "BCCHR IT Instructions",
    "text": "BCCHR IT Instructions\nIf you are on the BCCHR VPN, you can follow instructions from the BCCHR Hub here:\nhttps://hub.bcchr.ca/display/it/Mattermost\nOtherwise, you can follow these directions:",
    "crumbs": [
      "Home",
      "Orientation",
      "Mattermost: Instant Messaging / Stay in Touch"
    ]
  },
  {
    "objectID": "orientation/instantmessaging.html#setup",
    "href": "orientation/instantmessaging.html#setup",
    "title": "Mattermost: Instant Messaging / Stay in Touch",
    "section": "Setup",
    "text": "Setup\n[NOTE: You will need a BCCHR username and password to gain access.]\n\nFirst thing you will want to do is get invited to our ‘team’:\nClick here to be invited\nIf that link does not work, you can submit a request to IT via support.bcchr.ca or e-mail Alex and ask him to send you an invite using your BCCHR username.\nYou will be asked “Where would you like to view this?” For now, click “View in Browser”\nYou will be asked to login with your BCCHR username and password: \nAfter you are given access, refresh the login (i.e. log out and log back in to https://im.bcchr.ca)",
    "crumbs": [
      "Home",
      "Orientation",
      "Mattermost: Instant Messaging / Stay in Touch"
    ]
  },
  {
    "objectID": "orientation/instantmessaging.html#installation",
    "href": "orientation/instantmessaging.html#installation",
    "title": "Mattermost: Instant Messaging / Stay in Touch",
    "section": "Installation",
    "text": "Installation\n\nBrowser\nAs described above, you can check out Mattermost on your browser first before you decide to install the app on your computer or phone:\nhttps://im.bcchr.ca/\n\n\nDesktop\nFor your computer, you will want to install Mattermost Desktop. Click here to choose your operating system and use their installation guide.\nFor server display name, use: BCCH Research MRI\nFor server URL, use: https://im.bcchr.ca/\nLogin with your BCCHR username and password\n\n\nMobile\nMattermost can also be installed on your phone. Click here to choose your phone operating system.\nFor server display name, use: BCCH Research MRI\nFor server URL, use: https://im.bcchr.ca/\nLogin with your BCCHR username and password",
    "crumbs": [
      "Home",
      "Orientation",
      "Mattermost: Instant Messaging / Stay in Touch"
    ]
  },
  {
    "objectID": "orientation/instantmessaging.html#next-steps",
    "href": "orientation/instantmessaging.html#next-steps",
    "title": "Mattermost: Instant Messaging / Stay in Touch",
    "section": "Next Steps",
    "text": "Next Steps\n\nLearn more\nIf you want to learn more about Mattermost and its features, visit https://docs.mattermost.com/guides/user.html\n\n\nAdd A Photo of Yourself\nIt can be very helpful to others to include a photo or yourself.\n\nGo to Account Settings (Top right on Desktop app, and bottom right on Mobile app)\nClick on Your Profile\nEdit your Profile Picture\n\n\n\nNotifications",
    "crumbs": [
      "Home",
      "Orientation",
      "Mattermost: Instant Messaging / Stay in Touch"
    ]
  },
  {
    "objectID": "mrisoftware/index.html",
    "href": "mrisoftware/index.html",
    "title": "MRI Software",
    "section": "",
    "text": "3D Slicer\n\n\n\nsoftware\n\nvisualization\n\nmedical imaging\n\n\n\n3D Slicer is an open-source software platform used for visualization, segmentation, and modeling of medical imaging data in 3D.\n\n\n\n\n\nFeb 28, 2025\n\n\nAlexander Weber\n\n\n\n\n\n\n\n\n\n\n\n\nfmriprep\n\n\n\n-software -fmri -reproducibility -standardization -preprocessing\n\n\n\nAn automated and standardized way of preprocessing fmri data\n\n\n\n\n\nSep 17, 2025\n\n\nAlexander Weber\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "MRI Software"
    ]
  },
  {
    "objectID": "mrimethods/index.html",
    "href": "mrimethods/index.html",
    "title": "MRI Methods",
    "section": "",
    "text": "EEG-fMRI Data Cleaning & Forward Solution\n\n\n\n\n\n\neeg-fmri\n\n\ncode\n\n\nmatlab\n\n\neeglab\n\n\nmne\n\n\nfreesurfer\n\n\npreprocessing\n\n\neeg\n\n\n\nReverse engineered code from Rodriguez (2016) for removal of the ballistocardiogram artifacts from EEG-fMRI data when the ECG fails to give a good signal. This code requires Matlab and the Signal Processing Toolbox.\n\n\n\n\n\nFeb 13, 2024\n\n\nLynne Williams\n\n\n\n\n\n\n\n\n\n\n\n\nEEG-fMRI Ballistocardiogram Cleaning (Rodriguez method)\n\n\n\n\n\n\neeg-fmri\n\n\ncode\n\n\nmatlab\n\n\npreprocessing\n\n\neeg\n\n\n\nReverse engineered code from Rodriguez (2016) for removal of the ballistocardiogram artifacts from EEG-fMRI data when the ECG fails to give a good signal. This code requires Matlab and the Signal Processing Toolbox.\n\n\n\n\n\nJul 11, 2018\n\n\nLynne Williams\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI Methods"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/gradient-removal.html",
    "href": "mrimethods/eeg-fmri/gradient-removal.html",
    "title": "EEG-fMRI Data Cleaning & Forward Solution",
    "section": "",
    "text": "The purpose of this procedure is to outline the steps necessary for cleaning magnetic resonance (MR) and ballistocardiogram (BCG) artifacts from EEG data obtained during simultaneous EEG-fMRI recordings. The efficacy of multimodal imaging can be significantly compromised without proper artifact removal. Hence, this procedure is crucial for improving the quality and interpretability of EEG data that is collected in conjunction with fMRI studies, particularly in clinical and research settings in the field of medicine.\n\n\nThe procedure for cleaning MR and BCG artifacts from EEG data can be summarized in three main steps:\n\nArtifact Identification: Detecting and marking the artifacts within the EEG data that stem from MR and BCG sources.\nArtifact Removal: Implementing algorithms and techniques designed to exclude or minimize the intrusion of identified artifacts.\nData Validation: Assessing the cleaned EEG data to confirm the efficacy of artifact removal and ensuring the integrity of the signal for further analysis.\n\nThis high-level overview provides a foundational understanding of the procedural steps involved without delving into complex technicalities.\n\n\n\n\n\nUse the subject id from the MR scanner EEG-fMRI session.\ncd BHBMEG&lt;subject_id&gt;\n\n\n\nPut the MRI data in BIDS format using your favourite BIDS conversion tool. I recommend dcm2bids. You can find more information at https://unfmontreal.github.io/Dcm2Bids/3.1.1/. Be sure that the subject is the root BIDS directory (i.e., only one subject in the BIDS directory) to keep the analysis as a N = 1 study.\nInstall anaconda if you haven’t already done so. Go to https://www.anaconda.com/download. Follow the install directions for your workstation and operating system.\nOpen a Terminal window. Check your Python version. At the command prompt, type\nwhich python\nIf the command should return a path with anaconda3 in it. For example,\n/Users/lj/anaconda3/bin/python\nCreate a python environment for dcm2bids. In a text file called environment.yml copy the following:\nname: dcm2bids\\\nchannels:\\\n- conda-forge\\\ndependencies:\\\n- python\\&gt;=3.7\\\n- dcm2niix\\\n- dcm2bids\nand in Terminal create the environment:\nconda env create --file environment.yml*\nActivate your new python environment. By running\nconda activate dcm2bids\nTest your dcm2bids installation.\ndcm2bids *\\--help*\nYou should get the help file for dcm2bids:\nusage: dcm2bids [-h] -d DICOM_DIR [DICOM_DIR ...] -p PARTICIPANT [-s SESSION]  \n                -c CONFIG [-o OUTPUT_DIR] [_--auto_extract_entities]_                [_--bids_validate] [--force_dcm2bids] [--skip_dcm2niix]_                [_--clobber] [-l {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [-v]_Reorganising NIfTI files from dcm2niix into the Brain Imaging Data Structure  \n  \noptions:  \n  -h, _--help            show this help message and exit_  -d DICOM_DIR [DICOM_DIR ...], _--dicom_dir DICOM_DIR [DICOM_DIR ...]_                        DICOM directory(ies) or archive(s) (tar, tar.bz2, tar.gz or zip).  \n  -p PARTICIPANT, _--participant PARTICIPANT_                        Participant ID.  \n  -s SESSION, _--session SESSION_                        Session ID. []  \n  -c CONFIG, _--config CONFIG_                        JSON configuration file (see example/config.json).  \n  -o OUTPUT_DIR, _--output_dir OUTPUT_DIR_                        Output BIDS directory. [/Users/lj]  _--auto_extract_entities_                        If set, it will automatically try to extract entityinformation [task, dir, echo] based on the suffix and datatype. [False]  _--bids_validate       If set, once your conversion is done it will check if your output folder is BIDS valid. [False]_                        bids-validator needs to be installed check: https://github.com/bids-standard/bids-validator_#quickstart_  _--force_dcm2bids      Overwrite previous temporary dcm2bids output if it exists._  _--skip_dcm2niix       Skip dcm2niix conversion. Option -d should contains NIFTI and json files._  _--clobber             Overwrite output if it exists._  -l {DEBUG,INFO,WARNING,ERROR,CRITICAL}, _--log_level {DEBUG,INFO,WARNING,ERROR,CRITICAL}_                        Set logging level to the console. [INFO]  \n  -v, _--version         Report dcm2bids version and the BIDS version._Documentation at https://unfmontreal.github.io/Dcm2Bids/\nMake a directory for your BIDS root directory:\nmkdir BIDS\\\ncd BIDS\nCreate the scaffold for your BIDS root directory:\ndcm2bids_scaffold -o BHBMEG&lt;subject_id\\&gt;\ncd BHBMEG&lt;subject_id&gt;\nDownload your MRI data from XNAT for BHBMEG&lt;subject_id&gt;.\nPut the MRI data in the sourcedata folder created by the scaffold.\nunzip \\&lt;xnat-username\\&gt;-\\&lt;download-date\\&gt;\\_\\&lt;download-time\\&gt;.zip -d\nsourcedata/\nFor example,\nunzip lwilliams-20240109_134304.zip -d sourcedata/\nChange the folder name to mri.\nmv ./sourcedata/\\&lt;xnat-username\\&gt;-\\&lt;download-date\\&gt;\\_\\&lt;download-time\\&gt;\nmri\nFor example,\nmv ./sourcedata/ lwilliams-20240109_134304 mri\nMove any additional MRI sessions into the mri folder (e.g., the structural scans from a corresponding BHBEP session).\nCreate a new .config file in the code folder of your subject’s BIDS folder. You can use any code editor, but here we will use nano.\ncd \\&lt;path to subject's BIDS directory\\&gt;\nnano code/dcm2bids_config.json\nIn the dcm2bids_config.json file in the editor window, add the following:\n{\n\"descriptions\": [ ]\n}\nChange directory back to the sourcedata/mri folder:\ncd &lt;path to BHBMEG&lt;subject_id&gt; directory&gt;/sourcedata/mri\nRun the dcm2bids_helper function:\ndcm2bids_helper -d . \nNote that the period indicates to run dcm2bids_helper function in the current directory. It will create a folder in that directory call tmp_dcm2bids/helper. Here you will find your NIFTI files using the names given by the scanner console. You can check what is in the folder by running ls:\nls tmp_dcm2bids/helper \nFor each scan, there will be 2 files: the NIFTI file and a .json sidecar:\nls tmp/dcm2bids/helper/*\n003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.json\n003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.nii.gz\n003\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n003\\_.\\_RESEARCH\\_\\_BHBMEG_20190507142627.nii.gz\n004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\nYou will use the information in the sidecar file to put your data into BIDS format.\nTo populate the config file, you need to inspect each of the sidecar files one at a time to make sure there is a unique match for the acquisitions. You can use the “SeriesDescription” field to help you.\ncat ./sourcedata/mri/tmp_dcm2bids/helper/\nChange directories into the helper folder:\ncd &lt;Path to BHBMEG&lt;subject_id&gt; folder&gt;/sourcedata/mri/tmp_dcm2bids/helper\nFind the Series Description using grep. For example,\ngrep SeriesDescription *.json\nwhich returns the lines:\n003_._RESEARCH_-_BHBEP_20190314111207.json: \"SeriesDescription\": \"SAG MPRAGE PROMO 0.9x0.9x0.9\",\n003_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n004_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n005_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n006_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n007_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n009_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Hand Stim\",\n010_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n011_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n012_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n013_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"SAG FSPGR 3D .9X.9X.9\"\nWe can add it to our config file.\n{\n\"descriptions\\\": [\n  {\n    \"datatype\": \"anat\",\n    \"suffix\": \"T1w\",\n    \"custom_entities\": \"ses-brainmap\",\n    \"criteria\": {\n      \"SeriesDescription\": \"SAG MPRAGE PROMO 0.9x0.9x0.9\",\n      \"SeriesNumber\": \"3\"\n      }\n    }\n  ]\n}\nMake sure that the “criteria” together identify only one MRI sequence. Look at SeriesNumber to differentiate the files. Continue until you have completed this exercise for all runs for all sequences. You should get something that looks like below:\n{  \n  \"descriptions\": [  \n    {  \n      \"datatype\": \"anat\",  \n      \"suffix\": \"T1W\",  \n      \"custom_entities\": \"ses-brainmap\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"SAG MPRAGE PROMO 0.9x0.9x0.9\",  \n          \"SeriesNumber\": \"3\"        }  \n    },  \n    {  \n      \"datatype\": \"anat\",  \n      \"suffix\": \"T1W\",  \n      \"custom_entities\": \"ses-eegfmri\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"SAG FSPGR 3D .9X.9X.9\",  \n          \"SeriesNumber\": \"13\"        }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-handstim\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Hand Stim\",  \n          \"SeriesNumber\": \"9\"        }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"3\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"4\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"5\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"6\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"7\"        },  \n      \"sidecar_changes\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"        }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"10\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n      {  \n        \"SeriesDescription\": \"fMRI Resting State\",  \n        \"SeriesNumber\": \"11\"      },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"12\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    }  \n  ]  \n}\nOnce you are done, you can run dcm2bids.\ndcm2bids -c code/dcm2bids_config.json -p BHBMEG&lt;subject_id&gt; -d\nsourcedata/mri/ --auto_extract_entities\nCopy the contents of the code directory from a previous eeg-fmri subject to the BHBMEG&lt;subject_id&gt; folder except for the dcm2bids_config.json file you created. Change the information in fmriprep_command.py, convert_mff_to_bids.py, and RemoveBallistocardiogram.mlx to fit your data.\nCreate a folder called mne in the derivatives folder.\nOnce the creation of the BIDS data folder is done, you should have something that looks like:\nBHBMEG004\n├── CHANGES\n├── README\n├── code\n│   ├── B3801539-D436-4BC0-81D9-6F9A82244022 (1).pdf\n│   ├── Channels4Rodriguez2016\n│   ├── RemoveBallistocardiogram.mlx\n│   ├── acpc_align.sh\n│   ├── bounded line-peg\n│   ├── convert_mff_to_bids.py\n│   ├── dcm2bids_config.json\n│   ├── fmriprep_command.sh\n│   └── gradient_removal.m\n├── dataset_description.json\n├── derivatives\n├── participants.json\n├── participants.tsv\n├── sourcedata\n│   ├── mri\n│   │   ├── BHBEP214A\n│   │   │   └── 3\n│   │   └── BHBMEG004B\n│   │   ├── 10\n│   │   ├── 11\n│   │   ├── 12\n│   │   ├── 13\n│   │   ├── 3\n│   │   ├── 4\n│   │   ├── 5\n│   │   ├── 6\n│   │   ├── 7\n│   │   └── 9\n│   └── tmp_dcm2bids\n│   ├── helper\n│   │   ├── 003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.json\n│   │   ├── 003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.nii.gz\n│   │   ├── 003\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 003\\_.\\_RESEARCH\\_\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   └── 013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   └── log\n│   └── helper_20240109-144412.log\n├── sub-BHBMEG004\n│   ├── anat\n│   │   ├── sub-BHBMEG004_ses-brainmap_T1W.json\n│   │   ├── sub-BHBMEG004_ses-brainmap_T1W.nii.gz\n│   │   ├── sub-BHBMEG004_ses-eegfmri_T1W.json\n│   │   └── sub-BHBMEG004_ses-eegfmri_T1W.nii.gz\n│   └── func\\\n│   ├── sub-BHBMEG004_ses-eegfmri_task-handstim_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-handstim_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-01_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-01_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-02_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-02_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-03_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-03_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-04_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-04_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-05_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-05_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-06_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-06_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-07_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-07_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-08_bold.json\n│   └── sub-BHBMEG004_ses-eegfmri_task-rest_run-08_bold.nii.gz\n└── tmp_dcm2bids\n   ├── log\n   │   ├── scaffold_20240109-142635.log\n   │   └── sub-BHBMEG004_20240109-180638.log\n   └── sub-BHBMEG004\nDeactivate the dcm2bids environment in Terminal.\nconda deactivate\n\n\n\nFor the T1 scan(s) for both the BHBEP and BHBMEG sessions, get the freesurfer surfaces and inflated spheres using recon-all. We will need these to align the EEG data with the MRI data. Include the T1 from the brain mapping session if there is one.\nmkdir BIDS/BHBMEG&lt;subject_id&gt;/derivatives/freesurfer\n\nrecon-all -subject sub-BHBMEG\\&lt;subject_id\\&gt; -I \\&lt;path to anat\nfolder\\&gt;/sub-BHBMEG\\&lt;subject_id\\&gt;\\_ses-eegfmri_T1W.nii.gz -i\nsub-BHBMEG\\&lt;subject_id\\&gt;\\_ses-brainmap_T1W.nii.gz -sd \\&lt;path to BIDS\nfolder\\&gt;/BHBMEG\\&lt;subject_id\\&gt;/derivatives/freesurfer -all\nBe sure to record the freesurfer subject as sub-BHBMEG&lt;id_number&gt; in the derivatives/freesurfer folder. Use the instructions at  https://surfer.nmr.mgh.harvard.edu/fswiki/recon-all if you need help setting this up.\n\n\n\nInstall MNE if you haven’t done so already (https://mne.tools/stable/install/manual_install.html#manual-install).\nconda install --channel=conda-forge --name=base conda-libmamba-solver\nconda create --solver=libmamba --override-channels\n--channel=conda-forge --name=mne mne\nIf you don't already have them, also install docker (https://docs.docker.com/desktop/install/mac-install/) and add the fmriprep command to your anaconda installation (https://www.anaconda.com/download) using:\nconda activate mne\npip install fmriprep-docker\nconda deactivate\nThen at the command line enter:\nconda activate mne\n\ncd &lt;BIDS_root&gt;code\nzsh fmriprep_command.sh\nconda deactivate\nThis will preprocess the fMRI data in your BHBMEG&lt;subject_id&gt; folder. Be sure to check the quality assurance document that gets outputted in the derivatives/fmriprep folder. If necessary, adjust fmriprep_command.sh and rerun.\n\n\n\n\nMake a directory called eeg in the sourcedata folder. Copy all the EEG data files from NetStation in .mff format into the sourcedata/eeg folder.\nOpen Spyder from the command line in Terminal.\nSpyder\nChange Spyder's Python interpreter to that in the MNE Environment. Click on the wrench in the toolbar. When the dialogue box opens, select Python Interpreter in the left column. Select the mne python interpreter in the list of the python interpreters on the right. It should have your anaconda3 install in the python interpreter name. Mine is:\n![[Pasted image 20240213120910.png]]\nClick OK.\nOpen the convert_mff_to_bids.py script. It is in the code directory in your subject’s BIDS directory.\nUpdate convert_mff_to_bids.py to list the EEG files from NetStation if you haven't already done so. Enter the filenames for each file. There will be somewhere between 1 and the number of MR resting state + the hand stim task runs of EEG data.\nIf the number of EEG runs is equal to the number of MR runs, use the first and last data points as the EEG run start and end (default). If the number of EEG runs is less than the number of MR runs, use the notes taken during acquisition to determine which MR runs are included in each EEG run. Once the distribution of MR runs in each EEG file is determined, open the data in your favourite viewer and mark timepoints between runs (this is the “quiet” periods between MR gradient artifacts. Pick a point about midway between the 2 MR runs and note the value in milliseconds. This will be the end point of your EEG run. Add one millisecond and this will be the beginning of the next MR run.\nOnce you have determined the break point values, run the script.\n\n\nThe first thing to be created is the bem folder in the derivatives/freesurfer/sub-BHBMEG&lt;subject_id&gt; folder. The make_bem flag in convert_mff_to_bids.py needs to be set to True.\nmake_bem = True\nThe freesurfer recon-all run above needs to have completed before doing this. This will create the head and scalp surfaces from your freesurfer output.\n\n\n\nIf you don't already have a .trans file in your derivatives/mne folder, we will now create one. When you run convert_mff_to_bids.py, the graphical user interface for PyVista will open. Be patient with PyVista, it takes a very long time to load. You will see the outer skin surface with 3 marks. This is the program's best guess at where the nasion and preauricular points are on the scalp surface.\n\nIf the points don't align with the marks from the EEG net, realign the points to correspond to where the EEG net was placed.\nOn the left side, if the Lock fiducials checkbox is checked, uncheck it. Underneath you have LPA. This is the left preauricular point. Click on the radio button beside LPA.\n\nMove the red fiducial marker to the circleish place in front of the ear corresponding to the placement of the preauricular point from the EEG net.\n\nThis gives the relationship between the EEG net placement and the underlying MR anatomy.\nNow click on the radio button beside Nasion. Move the green fiducial marker to where the EEG net is sitting on the face.\n\nRepeat for the right preauricular point (RPA).\n\n\n\n\nFill in the MR runs contained in each EEG run. The script will ask you for this information. Remember to include the opening and closing square brackets (e.g., [3, 4, 5, …]) for the list of MR runs contained in the EEG run. The MR run will not have a value of less than 3 because of the calibration scans preceding the scan.\nEnter the start and end points of each MR run in the EEG data when asked to do so if more than one MR run is in an EEG run (i.e., n_mr_runs &gt; n_eeg_runs) in the convert_mff_to_bids.py script.\nThis script will save a EEGLAB .set file in the derivatives/mne directory for the eeg data corresponding to each MR run labeled with the run numbers in the BIDS directory.\n\n\n\nDownload EEGLAB from https://sccn.ucsd.edu/eeglab/download.php. Follow the install instructions. Add the EEGLAB path to your Matlab path.\nIn the Matlab command window, type\neeglab\nThis will open the following dialogue box:\n\nIf you haven't already done so, add the fMRIb MRI Tools. In the main EEGLAB window, go to File &gt; Manage EEGLAB extensions. This will open another dialogue box:\n\nIn the search bar, type in MRI.\n\nSelect the extension labelled fMRIb (download the latest stable version).\n\nClick Install/Update.\nClose the main EEGLAB window.\n\n\n\n\nChange the Matlab directory to the root of your BIDS folder structure for your subject. In Matlab 2022a, this is:\ncd &lt;path to BIDS root&gt;\nNote that you must use Matlab 2018b or above with the signal processing and the stats toolboxes. If you don’t have the toolboxes, contact BCCHR IT.\nYou want to open the .set file for the EEG-MRI run stored in your derivatives/mne/sub-&lt;subject_id&gt; folder.\nOpen the main EEGLAB window.\neeglab\nClick File &gt; Load existing dataset. Navigate to the derivatives/mne/sub-&lt;subject_id&gt; folder. Select the .set file for the EEG-fMRI run you want to process.\n\nNow we need to get the TRs. They have been saved as an extra channel. They should be stored in channel 258, but we want to check first. Go to Plot &gt; Channel data (scroll).\n\nHowever, the display is too busy. Go to Settings &gt; Number of channels to display. In the pop-up dialogue box, put a number between 10 and 25, depending upon the size of your monitor and what you feel more comfortable with. Click Ok. Scroll down using the bar on the left. Check that TR is listed as channel 258.\n\nThe noise in the data is the gradient artifact.\nNow that we have verified that the TRs are stored in channel 258 click CANCEL.\nNow we need the TRs as events and not as a channel. Go to File &gt; Import event data &gt; From data channel.\n\nEnter 258 for Event channel(s). Be sure to unclick Delete event channel(s) and Delete old events if any. Your display should look like the above dialogue box. Click Ok.\nIn the original dialogue box, go to Tools &gt; FMRIB Tools &gt; FASTR: Remove MRI gradient artifacts. Again, this will open another dialogue box.\n\nThe TRs come one for every volume. Change Artifact timing event to Volume/Section. Make sure that the Artifact timing event reads either TR or the channel that you entered for the TR signal from the previous step (usually chan258, but check to be sure). Check the box beside Adaptive Noise Cancellation.\n\nPress Ok.\nRepeat for all other functional MR runs.\n\n\nThis algorithm uses Cameron Rodriguez's (2016) method to derive the cardiac signal from selected channels in the EEG signal. Please see Rodriguez (2016) for more details.\nIn Matlab open the RemoveBallistocardiogram.mlx file from the code directory in your subject's root BIDS directory. Note that you need to be using Matlab 2022a or above and have the signal processing toolbox installed. If you don't have these, contact BCCHR/UBC IT to find out how to get these for your account.\nOnce RemoveBallistocardiogram.mlx is open, change the data in boxes 1 through 3. In the first box, change e to the number of MR runs of resting state data that you have (e.g., e = 7 MR runs). Next, change the names of the output files in BIDS format to fit your data. You should only have to change the sub-BHBMEG&lt;subject_id&gt;** part and change the number of successful MR runs. This will put .txt files with the SPPeaks of the cardiac signal.\n\nNext, we need to add the path to our gradient removed eeg data in the ./derivatives/mne folder of the root BIDS directory.\n\nOnce you have your directory path set, fill in the names of each of your gradient cleaned EEG runs.\n\nNow in Matlab\\'s LIVE EDITOR window, press the run button (green right pointing arrowhead). The script will loop through your EEG runs for each MR run and output a .txt file for each run. These are the SPPeaks from the cardiac symbol.\n\n\n\n\nNow that you have the files with the SP Peaks, you are ready to remove the ballistocardiogram from your EEG signal. In Matlab, open EEGLAB:\neeglab\nIn the EEGLAB main window, open the first of your gradient cleaned EEG .set files (i.e., sub-BHBMEG&lt;subject_id&gt;_ses-eegfmri_task-rest_run-&lt;run number&gt;_desc-gr_eeg.set) using File \\&gt; Load existing dataset. Once opened, we need to import the .txt file with the events corresponding to the SP Peaks in that particular run. To do that, we go to File &gt; Import event info &gt; From MATLAB array or ASCII file. This will open the following dialogue box:\n\nSelect Browse and select the BHBMEG&lt;subject_id&gt;_ses-eegfmri_task-rest_run-&lt;run number&gt;_desc-gr-sppeak_eeg.txt file corresponding to the EEG run you loaded previously. Add the words latency type to the Input field (column) names box, change the Number of file header lines to 1, the Time unit (sec) to NaN and uncheck the **Auto adjust new events sampling rate. Your dialogue box should look like the following:\n\nPress Ok.\nCheck that the file loaded in the main EEGLAB window. The number of events should increase by at least approximately six hundred to fifteen hundred (give or take a bit to adjust for changes and individual differences in heart rate).\n\nNow we need to go to Tools &gt; fMRIb Tools &gt; Remove pulse artifacts. This will open the following dialogue box:\n\nMake sure QRS/Heartbeat Event is sppeak. Select Optimal Basis Set. Leave the default value of 3 in the Number of PCs to use. Press Ok.\nYou will see the progress in the Matlab Console window.\n\nOnce completed, be sure to save the new file as BHBMEG&lt;subject_id&gt;_ses-eegfmri_task-rest_run-&lt;run number&gt;_desc-gr-sppeak-qrs_eeg.set. Overwrite the gradient cleaned version in the main EEGLAB window.\n\nCheck the data by opening the file in the EEGLAB viewer. Go to Plot &gt; Channel data (scroll. In the plot window, select Display &gt; Remove DC offset and Settings &gt; Number of channels to display. Set the number in the new dialogue box to somewhere between 10 and 20 channels depending upon your preference.\n\nExport the data in .edf format so that the epileptologist can view the file in Natus (the proprietary system used in the EEG department) or NetStation. In the main EEGLAB window, select File \\&gt; Export \\&gt; Data to EDF/BDF/GDF file. If you are asked to install the Biosig extension, press Yes. Save the file as sub-BHBMEG\\&lt;subject_id\\&gt;\\_ses-eegfmri_task-rest_run-\\&lt;run number\\&gt;\\_desc-gr-sppeak-qrs_eeg.edf. Make sure to use the .edf file extension. This will open a dialogue box:\n\nHighlight EDF and press Ok. This will write the EDF file to disk.\nRepeat the Remove Ballistocardiogram using FIMRIB EEGLAB Plugin steps for all other MR/EEG gradient cleaned files (one corresponding to each MR run).\n\n\n\nOnce you are finished cleaning and checking the EEG data, notify the epileptologist who made the referral for spike reading in the EEG file. Ask if they would prefer to read the spikes in Natus or Netstation and prepare the software accordingly. If using Natus, you will need to make arrangements with the EEG department to bring the files over and have them installed on their Natus system. If loading in Natus, be sure to bring the patient’s name and date of birth along with the anonymized .edf files as the records will become part of the EEG department’s records for that patient/subject. If you don’t want the results to become part of the patient’s medical record, make sure the epileptologist reads the data in NetStation in the Brain Mapping Lab.\nOnce the epileptologist has read the spikes in each run, export the events from either Netstation or Natus for each run to a .txt (ASCII file). It is these events that will be used to\n\nrun an event-related design for your fMRI runs in FSL FEAT, and\ncreate epochs and run a beamformer in MNE.\n\nReferences\nRodriguez, C. (2016). Improvements to Simultaneous Electroencepalography-Functional Magnetic Resonance Imaging and Electroencepalographic Source Localization. Dissertation. https://escholarship.org/uc/item/3gg3z2q6",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Data Cleaning & Forward Solution"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/gradient-removal.html#high-level-overview-of-eeg-fmri-artifact-cleaning-procedure",
    "href": "mrimethods/eeg-fmri/gradient-removal.html#high-level-overview-of-eeg-fmri-artifact-cleaning-procedure",
    "title": "EEG-fMRI Data Cleaning & Forward Solution",
    "section": "",
    "text": "The procedure for cleaning MR and BCG artifacts from EEG data can be summarized in three main steps:\n\nArtifact Identification: Detecting and marking the artifacts within the EEG data that stem from MR and BCG sources.\nArtifact Removal: Implementing algorithms and techniques designed to exclude or minimize the intrusion of identified artifacts.\nData Validation: Assessing the cleaned EEG data to confirm the efficacy of artifact removal and ensuring the integrity of the signal for further analysis.\n\nThis high-level overview provides a foundational understanding of the procedural steps involved without delving into complex technicalities.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Data Cleaning & Forward Solution"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/gradient-removal.html#data-preparation",
    "href": "mrimethods/eeg-fmri/gradient-removal.html#data-preparation",
    "title": "EEG-fMRI Data Cleaning & Forward Solution",
    "section": "",
    "text": "Use the subject id from the MR scanner EEG-fMRI session.\ncd BHBMEG&lt;subject_id&gt;\n\n\n\nPut the MRI data in BIDS format using your favourite BIDS conversion tool. I recommend dcm2bids. You can find more information at https://unfmontreal.github.io/Dcm2Bids/3.1.1/. Be sure that the subject is the root BIDS directory (i.e., only one subject in the BIDS directory) to keep the analysis as a N = 1 study.\nInstall anaconda if you haven’t already done so. Go to https://www.anaconda.com/download. Follow the install directions for your workstation and operating system.\nOpen a Terminal window. Check your Python version. At the command prompt, type\nwhich python\nIf the command should return a path with anaconda3 in it. For example,\n/Users/lj/anaconda3/bin/python\nCreate a python environment for dcm2bids. In a text file called environment.yml copy the following:\nname: dcm2bids\\\nchannels:\\\n- conda-forge\\\ndependencies:\\\n- python\\&gt;=3.7\\\n- dcm2niix\\\n- dcm2bids\nand in Terminal create the environment:\nconda env create --file environment.yml*\nActivate your new python environment. By running\nconda activate dcm2bids\nTest your dcm2bids installation.\ndcm2bids *\\--help*\nYou should get the help file for dcm2bids:\nusage: dcm2bids [-h] -d DICOM_DIR [DICOM_DIR ...] -p PARTICIPANT [-s SESSION]  \n                -c CONFIG [-o OUTPUT_DIR] [_--auto_extract_entities]_                [_--bids_validate] [--force_dcm2bids] [--skip_dcm2niix]_                [_--clobber] [-l {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [-v]_Reorganising NIfTI files from dcm2niix into the Brain Imaging Data Structure  \n  \noptions:  \n  -h, _--help            show this help message and exit_  -d DICOM_DIR [DICOM_DIR ...], _--dicom_dir DICOM_DIR [DICOM_DIR ...]_                        DICOM directory(ies) or archive(s) (tar, tar.bz2, tar.gz or zip).  \n  -p PARTICIPANT, _--participant PARTICIPANT_                        Participant ID.  \n  -s SESSION, _--session SESSION_                        Session ID. []  \n  -c CONFIG, _--config CONFIG_                        JSON configuration file (see example/config.json).  \n  -o OUTPUT_DIR, _--output_dir OUTPUT_DIR_                        Output BIDS directory. [/Users/lj]  _--auto_extract_entities_                        If set, it will automatically try to extract entityinformation [task, dir, echo] based on the suffix and datatype. [False]  _--bids_validate       If set, once your conversion is done it will check if your output folder is BIDS valid. [False]_                        bids-validator needs to be installed check: https://github.com/bids-standard/bids-validator_#quickstart_  _--force_dcm2bids      Overwrite previous temporary dcm2bids output if it exists._  _--skip_dcm2niix       Skip dcm2niix conversion. Option -d should contains NIFTI and json files._  _--clobber             Overwrite output if it exists._  -l {DEBUG,INFO,WARNING,ERROR,CRITICAL}, _--log_level {DEBUG,INFO,WARNING,ERROR,CRITICAL}_                        Set logging level to the console. [INFO]  \n  -v, _--version         Report dcm2bids version and the BIDS version._Documentation at https://unfmontreal.github.io/Dcm2Bids/\nMake a directory for your BIDS root directory:\nmkdir BIDS\\\ncd BIDS\nCreate the scaffold for your BIDS root directory:\ndcm2bids_scaffold -o BHBMEG&lt;subject_id\\&gt;\ncd BHBMEG&lt;subject_id&gt;\nDownload your MRI data from XNAT for BHBMEG&lt;subject_id&gt;.\nPut the MRI data in the sourcedata folder created by the scaffold.\nunzip \\&lt;xnat-username\\&gt;-\\&lt;download-date\\&gt;\\_\\&lt;download-time\\&gt;.zip -d\nsourcedata/\nFor example,\nunzip lwilliams-20240109_134304.zip -d sourcedata/\nChange the folder name to mri.\nmv ./sourcedata/\\&lt;xnat-username\\&gt;-\\&lt;download-date\\&gt;\\_\\&lt;download-time\\&gt;\nmri\nFor example,\nmv ./sourcedata/ lwilliams-20240109_134304 mri\nMove any additional MRI sessions into the mri folder (e.g., the structural scans from a corresponding BHBEP session).\nCreate a new .config file in the code folder of your subject’s BIDS folder. You can use any code editor, but here we will use nano.\ncd \\&lt;path to subject's BIDS directory\\&gt;\nnano code/dcm2bids_config.json\nIn the dcm2bids_config.json file in the editor window, add the following:\n{\n\"descriptions\": [ ]\n}\nChange directory back to the sourcedata/mri folder:\ncd &lt;path to BHBMEG&lt;subject_id&gt; directory&gt;/sourcedata/mri\nRun the dcm2bids_helper function:\ndcm2bids_helper -d . \nNote that the period indicates to run dcm2bids_helper function in the current directory. It will create a folder in that directory call tmp_dcm2bids/helper. Here you will find your NIFTI files using the names given by the scanner console. You can check what is in the folder by running ls:\nls tmp_dcm2bids/helper \nFor each scan, there will be 2 files: the NIFTI file and a .json sidecar:\nls tmp/dcm2bids/helper/*\n003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.json\n003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.nii.gz\n003\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n003\\_.\\_RESEARCH\\_\\_BHBMEG_20190507142627.nii.gz\n004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\nYou will use the information in the sidecar file to put your data into BIDS format.\nTo populate the config file, you need to inspect each of the sidecar files one at a time to make sure there is a unique match for the acquisitions. You can use the “SeriesDescription” field to help you.\ncat ./sourcedata/mri/tmp_dcm2bids/helper/\nChange directories into the helper folder:\ncd &lt;Path to BHBMEG&lt;subject_id&gt; folder&gt;/sourcedata/mri/tmp_dcm2bids/helper\nFind the Series Description using grep. For example,\ngrep SeriesDescription *.json\nwhich returns the lines:\n003_._RESEARCH_-_BHBEP_20190314111207.json: \"SeriesDescription\": \"SAG MPRAGE PROMO 0.9x0.9x0.9\",\n003_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n004_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n005_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n006_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n007_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n009_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Hand Stim\",\n010_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n011_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n012_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"fMRI Resting State\",\n013_._RESEARCH_-_BHBMEG_20190507142627.json: \"SeriesDescription\": \"SAG FSPGR 3D .9X.9X.9\"\nWe can add it to our config file.\n{\n\"descriptions\\\": [\n  {\n    \"datatype\": \"anat\",\n    \"suffix\": \"T1w\",\n    \"custom_entities\": \"ses-brainmap\",\n    \"criteria\": {\n      \"SeriesDescription\": \"SAG MPRAGE PROMO 0.9x0.9x0.9\",\n      \"SeriesNumber\": \"3\"\n      }\n    }\n  ]\n}\nMake sure that the “criteria” together identify only one MRI sequence. Look at SeriesNumber to differentiate the files. Continue until you have completed this exercise for all runs for all sequences. You should get something that looks like below:\n{  \n  \"descriptions\": [  \n    {  \n      \"datatype\": \"anat\",  \n      \"suffix\": \"T1W\",  \n      \"custom_entities\": \"ses-brainmap\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"SAG MPRAGE PROMO 0.9x0.9x0.9\",  \n          \"SeriesNumber\": \"3\"        }  \n    },  \n    {  \n      \"datatype\": \"anat\",  \n      \"suffix\": \"T1W\",  \n      \"custom_entities\": \"ses-eegfmri\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"SAG FSPGR 3D .9X.9X.9\",  \n          \"SeriesNumber\": \"13\"        }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-handstim\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Hand Stim\",  \n          \"SeriesNumber\": \"9\"        }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"3\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"4\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"5\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"6\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"7\"        },  \n      \"sidecar_changes\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"        }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"10\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n      {  \n        \"SeriesDescription\": \"fMRI Resting State\",  \n        \"SeriesNumber\": \"11\"      },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    },  \n    {  \n      \"datatype\": \"func\",  \n      \"suffix\": \"bold\",  \n      \"custom_entities\": \"ses-eegfmri_task-rest\",  \n      \"criteria\":  \n        {  \n          \"SeriesDescription\": \"fMRI Resting State\",  \n          \"SeriesNumber\": \"12\"        },  \n      \"sidecar_changes\": {  \n                \"SeriesDescription\": \"fMRI Resting State Eyes Closed\"            }  \n    }  \n  ]  \n}\nOnce you are done, you can run dcm2bids.\ndcm2bids -c code/dcm2bids_config.json -p BHBMEG&lt;subject_id&gt; -d\nsourcedata/mri/ --auto_extract_entities\nCopy the contents of the code directory from a previous eeg-fmri subject to the BHBMEG&lt;subject_id&gt; folder except for the dcm2bids_config.json file you created. Change the information in fmriprep_command.py, convert_mff_to_bids.py, and RemoveBallistocardiogram.mlx to fit your data.\nCreate a folder called mne in the derivatives folder.\nOnce the creation of the BIDS data folder is done, you should have something that looks like:\nBHBMEG004\n├── CHANGES\n├── README\n├── code\n│   ├── B3801539-D436-4BC0-81D9-6F9A82244022 (1).pdf\n│   ├── Channels4Rodriguez2016\n│   ├── RemoveBallistocardiogram.mlx\n│   ├── acpc_align.sh\n│   ├── bounded line-peg\n│   ├── convert_mff_to_bids.py\n│   ├── dcm2bids_config.json\n│   ├── fmriprep_command.sh\n│   └── gradient_removal.m\n├── dataset_description.json\n├── derivatives\n├── participants.json\n├── participants.tsv\n├── sourcedata\n│   ├── mri\n│   │   ├── BHBEP214A\n│   │   │   └── 3\n│   │   └── BHBMEG004B\n│   │   ├── 10\n│   │   ├── 11\n│   │   ├── 12\n│   │   ├── 13\n│   │   ├── 3\n│   │   ├── 4\n│   │   ├── 5\n│   │   ├── 6\n│   │   ├── 7\n│   │   └── 9\n│   └── tmp_dcm2bids\n│   ├── helper\n│   │   ├── 003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.json\n│   │   ├── 003\\_.\\_RESEARCH\\_-\\_BHBEP_20190314111207.nii.gz\n│   │   ├── 003\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 003\\_.\\_RESEARCH\\_\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 004\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 005\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 006\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 007\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 009\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 010\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 011\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   ├── 012\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   │   ├── 013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.json\n│   │   └── 013\\_.\\_RESEARCH\\_-\\_BHBMEG_20190507142627.nii.gz\n│   └── log\n│   └── helper_20240109-144412.log\n├── sub-BHBMEG004\n│   ├── anat\n│   │   ├── sub-BHBMEG004_ses-brainmap_T1W.json\n│   │   ├── sub-BHBMEG004_ses-brainmap_T1W.nii.gz\n│   │   ├── sub-BHBMEG004_ses-eegfmri_T1W.json\n│   │   └── sub-BHBMEG004_ses-eegfmri_T1W.nii.gz\n│   └── func\\\n│   ├── sub-BHBMEG004_ses-eegfmri_task-handstim_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-handstim_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-01_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-01_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-02_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-02_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-03_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-03_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-04_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-04_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-05_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-05_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-06_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-06_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-07_bold.json\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-07_bold.nii.gz\n│   ├── sub-BHBMEG004_ses-eegfmri_task-rest_run-08_bold.json\n│   └── sub-BHBMEG004_ses-eegfmri_task-rest_run-08_bold.nii.gz\n└── tmp_dcm2bids\n   ├── log\n   │   ├── scaffold_20240109-142635.log\n   │   └── sub-BHBMEG004_20240109-180638.log\n   └── sub-BHBMEG004\nDeactivate the dcm2bids environment in Terminal.\nconda deactivate\n\n\n\nFor the T1 scan(s) for both the BHBEP and BHBMEG sessions, get the freesurfer surfaces and inflated spheres using recon-all. We will need these to align the EEG data with the MRI data. Include the T1 from the brain mapping session if there is one.\nmkdir BIDS/BHBMEG&lt;subject_id&gt;/derivatives/freesurfer\n\nrecon-all -subject sub-BHBMEG\\&lt;subject_id\\&gt; -I \\&lt;path to anat\nfolder\\&gt;/sub-BHBMEG\\&lt;subject_id\\&gt;\\_ses-eegfmri_T1W.nii.gz -i\nsub-BHBMEG\\&lt;subject_id\\&gt;\\_ses-brainmap_T1W.nii.gz -sd \\&lt;path to BIDS\nfolder\\&gt;/BHBMEG\\&lt;subject_id\\&gt;/derivatives/freesurfer -all\nBe sure to record the freesurfer subject as sub-BHBMEG&lt;id_number&gt; in the derivatives/freesurfer folder. Use the instructions at  https://surfer.nmr.mgh.harvard.edu/fswiki/recon-all if you need help setting this up.\n\n\n\nInstall MNE if you haven’t done so already (https://mne.tools/stable/install/manual_install.html#manual-install).\nconda install --channel=conda-forge --name=base conda-libmamba-solver\nconda create --solver=libmamba --override-channels\n--channel=conda-forge --name=mne mne\nIf you don't already have them, also install docker (https://docs.docker.com/desktop/install/mac-install/) and add the fmriprep command to your anaconda installation (https://www.anaconda.com/download) using:\nconda activate mne\npip install fmriprep-docker\nconda deactivate\nThen at the command line enter:\nconda activate mne\n\ncd &lt;BIDS_root&gt;code\nzsh fmriprep_command.sh\nconda deactivate\nThis will preprocess the fMRI data in your BHBMEG&lt;subject_id&gt; folder. Be sure to check the quality assurance document that gets outputted in the derivatives/fmriprep folder. If necessary, adjust fmriprep_command.sh and rerun.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Data Cleaning & Forward Solution"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/gradient-removal.html#eeg-cleaning",
    "href": "mrimethods/eeg-fmri/gradient-removal.html#eeg-cleaning",
    "title": "EEG-fMRI Data Cleaning & Forward Solution",
    "section": "",
    "text": "Make a directory called eeg in the sourcedata folder. Copy all the EEG data files from NetStation in .mff format into the sourcedata/eeg folder.\nOpen Spyder from the command line in Terminal.\nSpyder\nChange Spyder's Python interpreter to that in the MNE Environment. Click on the wrench in the toolbar. When the dialogue box opens, select Python Interpreter in the left column. Select the mne python interpreter in the list of the python interpreters on the right. It should have your anaconda3 install in the python interpreter name. Mine is:\n![[Pasted image 20240213120910.png]]\nClick OK.\nOpen the convert_mff_to_bids.py script. It is in the code directory in your subject’s BIDS directory.\nUpdate convert_mff_to_bids.py to list the EEG files from NetStation if you haven't already done so. Enter the filenames for each file. There will be somewhere between 1 and the number of MR resting state + the hand stim task runs of EEG data.\nIf the number of EEG runs is equal to the number of MR runs, use the first and last data points as the EEG run start and end (default). If the number of EEG runs is less than the number of MR runs, use the notes taken during acquisition to determine which MR runs are included in each EEG run. Once the distribution of MR runs in each EEG file is determined, open the data in your favourite viewer and mark timepoints between runs (this is the “quiet” periods between MR gradient artifacts. Pick a point about midway between the 2 MR runs and note the value in milliseconds. This will be the end point of your EEG run. Add one millisecond and this will be the beginning of the next MR run.\nOnce you have determined the break point values, run the script.\n\n\nThe first thing to be created is the bem folder in the derivatives/freesurfer/sub-BHBMEG&lt;subject_id&gt; folder. The make_bem flag in convert_mff_to_bids.py needs to be set to True.\nmake_bem = True\nThe freesurfer recon-all run above needs to have completed before doing this. This will create the head and scalp surfaces from your freesurfer output.\n\n\n\nIf you don't already have a .trans file in your derivatives/mne folder, we will now create one. When you run convert_mff_to_bids.py, the graphical user interface for PyVista will open. Be patient with PyVista, it takes a very long time to load. You will see the outer skin surface with 3 marks. This is the program's best guess at where the nasion and preauricular points are on the scalp surface.\n\nIf the points don't align with the marks from the EEG net, realign the points to correspond to where the EEG net was placed.\nOn the left side, if the Lock fiducials checkbox is checked, uncheck it. Underneath you have LPA. This is the left preauricular point. Click on the radio button beside LPA.\n\nMove the red fiducial marker to the circleish place in front of the ear corresponding to the placement of the preauricular point from the EEG net.\n\nThis gives the relationship between the EEG net placement and the underlying MR anatomy.\nNow click on the radio button beside Nasion. Move the green fiducial marker to where the EEG net is sitting on the face.\n\nRepeat for the right preauricular point (RPA).\n\n\n\n\nFill in the MR runs contained in each EEG run. The script will ask you for this information. Remember to include the opening and closing square brackets (e.g., [3, 4, 5, …]) for the list of MR runs contained in the EEG run. The MR run will not have a value of less than 3 because of the calibration scans preceding the scan.\nEnter the start and end points of each MR run in the EEG data when asked to do so if more than one MR run is in an EEG run (i.e., n_mr_runs &gt; n_eeg_runs) in the convert_mff_to_bids.py script.\nThis script will save a EEGLAB .set file in the derivatives/mne directory for the eeg data corresponding to each MR run labeled with the run numbers in the BIDS directory.\n\n\n\nDownload EEGLAB from https://sccn.ucsd.edu/eeglab/download.php. Follow the install instructions. Add the EEGLAB path to your Matlab path.\nIn the Matlab command window, type\neeglab\nThis will open the following dialogue box:\n\nIf you haven't already done so, add the fMRIb MRI Tools. In the main EEGLAB window, go to File &gt; Manage EEGLAB extensions. This will open another dialogue box:\n\nIn the search bar, type in MRI.\n\nSelect the extension labelled fMRIb (download the latest stable version).\n\nClick Install/Update.\nClose the main EEGLAB window.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Data Cleaning & Forward Solution"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/gradient-removal.html#remove-mr-gradient-artifact-using-fimrib-eeglab-plugin",
    "href": "mrimethods/eeg-fmri/gradient-removal.html#remove-mr-gradient-artifact-using-fimrib-eeglab-plugin",
    "title": "EEG-fMRI Data Cleaning & Forward Solution",
    "section": "",
    "text": "Change the Matlab directory to the root of your BIDS folder structure for your subject. In Matlab 2022a, this is:\ncd &lt;path to BIDS root&gt;\nNote that you must use Matlab 2018b or above with the signal processing and the stats toolboxes. If you don’t have the toolboxes, contact BCCHR IT.\nYou want to open the .set file for the EEG-MRI run stored in your derivatives/mne/sub-&lt;subject_id&gt; folder.\nOpen the main EEGLAB window.\neeglab\nClick File &gt; Load existing dataset. Navigate to the derivatives/mne/sub-&lt;subject_id&gt; folder. Select the .set file for the EEG-fMRI run you want to process.\n\nNow we need to get the TRs. They have been saved as an extra channel. They should be stored in channel 258, but we want to check first. Go to Plot &gt; Channel data (scroll).\n\nHowever, the display is too busy. Go to Settings &gt; Number of channels to display. In the pop-up dialogue box, put a number between 10 and 25, depending upon the size of your monitor and what you feel more comfortable with. Click Ok. Scroll down using the bar on the left. Check that TR is listed as channel 258.\n\nThe noise in the data is the gradient artifact.\nNow that we have verified that the TRs are stored in channel 258 click CANCEL.\nNow we need the TRs as events and not as a channel. Go to File &gt; Import event data &gt; From data channel.\n\nEnter 258 for Event channel(s). Be sure to unclick Delete event channel(s) and Delete old events if any. Your display should look like the above dialogue box. Click Ok.\nIn the original dialogue box, go to Tools &gt; FMRIB Tools &gt; FASTR: Remove MRI gradient artifacts. Again, this will open another dialogue box.\n\nThe TRs come one for every volume. Change Artifact timing event to Volume/Section. Make sure that the Artifact timing event reads either TR or the channel that you entered for the TR signal from the previous step (usually chan258, but check to be sure). Check the box beside Adaptive Noise Cancellation.\n\nPress Ok.\nRepeat for all other functional MR runs.\n\n\nThis algorithm uses Cameron Rodriguez's (2016) method to derive the cardiac signal from selected channels in the EEG signal. Please see Rodriguez (2016) for more details.\nIn Matlab open the RemoveBallistocardiogram.mlx file from the code directory in your subject's root BIDS directory. Note that you need to be using Matlab 2022a or above and have the signal processing toolbox installed. If you don't have these, contact BCCHR/UBC IT to find out how to get these for your account.\nOnce RemoveBallistocardiogram.mlx is open, change the data in boxes 1 through 3. In the first box, change e to the number of MR runs of resting state data that you have (e.g., e = 7 MR runs). Next, change the names of the output files in BIDS format to fit your data. You should only have to change the sub-BHBMEG&lt;subject_id&gt;** part and change the number of successful MR runs. This will put .txt files with the SPPeaks of the cardiac signal.\n\nNext, we need to add the path to our gradient removed eeg data in the ./derivatives/mne folder of the root BIDS directory.\n\nOnce you have your directory path set, fill in the names of each of your gradient cleaned EEG runs.\n\nNow in Matlab\\'s LIVE EDITOR window, press the run button (green right pointing arrowhead). The script will loop through your EEG runs for each MR run and output a .txt file for each run. These are the SPPeaks from the cardiac symbol.\n\n\n\n\nNow that you have the files with the SP Peaks, you are ready to remove the ballistocardiogram from your EEG signal. In Matlab, open EEGLAB:\neeglab\nIn the EEGLAB main window, open the first of your gradient cleaned EEG .set files (i.e., sub-BHBMEG&lt;subject_id&gt;_ses-eegfmri_task-rest_run-&lt;run number&gt;_desc-gr_eeg.set) using File \\&gt; Load existing dataset. Once opened, we need to import the .txt file with the events corresponding to the SP Peaks in that particular run. To do that, we go to File &gt; Import event info &gt; From MATLAB array or ASCII file. This will open the following dialogue box:\n\nSelect Browse and select the BHBMEG&lt;subject_id&gt;_ses-eegfmri_task-rest_run-&lt;run number&gt;_desc-gr-sppeak_eeg.txt file corresponding to the EEG run you loaded previously. Add the words latency type to the Input field (column) names box, change the Number of file header lines to 1, the Time unit (sec) to NaN and uncheck the **Auto adjust new events sampling rate. Your dialogue box should look like the following:\n\nPress Ok.\nCheck that the file loaded in the main EEGLAB window. The number of events should increase by at least approximately six hundred to fifteen hundred (give or take a bit to adjust for changes and individual differences in heart rate).\n\nNow we need to go to Tools &gt; fMRIb Tools &gt; Remove pulse artifacts. This will open the following dialogue box:\n\nMake sure QRS/Heartbeat Event is sppeak. Select Optimal Basis Set. Leave the default value of 3 in the Number of PCs to use. Press Ok.\nYou will see the progress in the Matlab Console window.\n\nOnce completed, be sure to save the new file as BHBMEG&lt;subject_id&gt;_ses-eegfmri_task-rest_run-&lt;run number&gt;_desc-gr-sppeak-qrs_eeg.set. Overwrite the gradient cleaned version in the main EEGLAB window.\n\nCheck the data by opening the file in the EEGLAB viewer. Go to Plot &gt; Channel data (scroll. In the plot window, select Display &gt; Remove DC offset and Settings &gt; Number of channels to display. Set the number in the new dialogue box to somewhere between 10 and 20 channels depending upon your preference.\n\nExport the data in .edf format so that the epileptologist can view the file in Natus (the proprietary system used in the EEG department) or NetStation. In the main EEGLAB window, select File \\&gt; Export \\&gt; Data to EDF/BDF/GDF file. If you are asked to install the Biosig extension, press Yes. Save the file as sub-BHBMEG\\&lt;subject_id\\&gt;\\_ses-eegfmri_task-rest_run-\\&lt;run number\\&gt;\\_desc-gr-sppeak-qrs_eeg.edf. Make sure to use the .edf file extension. This will open a dialogue box:\n\nHighlight EDF and press Ok. This will write the EDF file to disk.\nRepeat the Remove Ballistocardiogram using FIMRIB EEGLAB Plugin steps for all other MR/EEG gradient cleaned files (one corresponding to each MR run).\n\n\n\nOnce you are finished cleaning and checking the EEG data, notify the epileptologist who made the referral for spike reading in the EEG file. Ask if they would prefer to read the spikes in Natus or Netstation and prepare the software accordingly. If using Natus, you will need to make arrangements with the EEG department to bring the files over and have them installed on their Natus system. If loading in Natus, be sure to bring the patient’s name and date of birth along with the anonymized .edf files as the records will become part of the EEG department’s records for that patient/subject. If you don’t want the results to become part of the patient’s medical record, make sure the epileptologist reads the data in NetStation in the Brain Mapping Lab.\nOnce the epileptologist has read the spikes in each run, export the events from either Netstation or Natus for each run to a .txt (ASCII file). It is these events that will be used to\n\nrun an event-related design for your fMRI runs in FSL FEAT, and\ncreate epochs and run a beamformer in MNE.\n\nReferences\nRodriguez, C. (2016). Improvements to Simultaneous Electroencepalography-Functional Magnetic Resonance Imaging and Electroencepalographic Source Localization. Dissertation. https://escholarship.org/uc/item/3gg3z2q6",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Data Cleaning & Forward Solution"
    ]
  },
  {
    "objectID": "mrimethods/anat/acpc-align.html",
    "href": "mrimethods/anat/acpc-align.html",
    "title": "Align AC PC to horizontal",
    "section": "",
    "text": "Align to the Anterior Commissure-Posterior Commisure Line\nIf you need help determining where the Anterior and Posterior Commisures are in your image, please see Commissural Pathways, available online, or Catani & Thiebaut de Schotten (2012), which is available from the UBC Library.\n\nDownload the acpc_align script from here. Be sure the code is in the folder where your participant’s structural scans are.\nMake the acpc_align.sh script executable.\n\nchmod 755 acpc_align.sh\n\nOpen your structural scan in your faviourite viewer. Here we are using FSLeyes.\n\n\n\n\nFSLeyes\n\n\n\nGet the X, Y and Z coordinates of the anterior commissure in voxel space by placing your cursor on the anterior commissure. The anterior commissure is located in the anterior wall of the third ventricle. It runs transversely anterior to the anterior columns of the fornix, above the basal forebrain and beneath the medial and ventral aspect of the anterior limb of the internal capsule. Save the X, Y, and Z coordinates.\n\n\n\n\nAnterior Commissure\n\n\n\nGet the X, Y, and Z coordinates of the posterior commissure in voxel space by placing your cursor on the posterior commissure. The posterior commissure is the inferior lamina or stalk of the pineal gland. Save the X, Y, and Z coordinates.\n\n\n\n\nPosterior Commissure\n\n\n\nIn your terminal (MacOS or Linux) run\n\nacpc_align subjectbrain.nii.gz -a x y z -p x y z\nto produce the realigned image for subject-specific to acpc alignment.\nIn the example, this is\nsh acpc_align.sh T1_reoriented.nii.gz -a 93 130 153 -p 93 98 152\n\n\n\nacpc aligned image\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anat",
      "Align AC PC to horizontal"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BCCH MRI Research Facility Wiki Page",
    "section": "",
    "text": "Overview\nThis page serves as a Wiki and collection of resources for users of BC Children’s MRI Research Facility\nYou can search this page using the magnifying glass in the top right 🔍\nOr you can navigate using the sidebar on the left 👈\nIf you have MRI questions or want to connect with other BCCH MRI Research Facility users, you can use Mattermost to connect with us (similar to Slack or Microsoft Teams).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The BC Children’s Hospital MRI Research Facility is one of Canada’s top centres for pediatric brain mapping and imaging sciences research.\nA core facility of the BC Children’s Hospital’s Research Institute and affiliated with the University of British Columbia, the child-friendly environment of the MRI Research Facility is ideally situated to optimize opportunities for widespread research collaborations with the highly skilled staff found at BC Children’s Hospital and BC Women’s Hospital + Health Centre.\nThe MRI Research Facility features a GE Discovery MR750 3.0 Tesla MRI scanner for functional MRI, structural MRI and spectroscopy. The current scanner software version is DV26.0_R03.\nTogether, with our child-friendly MRI simulator, the MRI Research Facility provides an excellent platform for performing research studies on babies and young children.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "investigatorresources/index.html",
    "href": "investigatorresources/index.html",
    "title": "Investigator Resources",
    "section": "",
    "text": "No matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Investigator Resources"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/ballistocardiogram-removal.html",
    "href": "mrimethods/eeg-fmri/ballistocardiogram-removal.html",
    "title": "EEG-fMRI Ballistocardiogram Cleaning (Rodriguez method)",
    "section": "",
    "text": "Reversed engineered code from Rodriguez (2016) for removal of the ballistocardiogram artifacts from EEG-fMRI data when the ECG fails to give a good signal. Cameron’s dissertation is included in this repository for transparency’s sake. This code requires Matlab and the Signal Processing Toolbox.\nIf you use this code, please cite:\nRodriguez, Cameron. (2016). Improvements to Simultaneous Electroencepalography-Functional Magnetic Resonance Imaging and Electroencepalographic Source Localization. PhD Thesis for the University of California, Los Angeles.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Ballistocardiogram Cleaning (Rodriguez method)"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/ballistocardiogram-removal.html#please-cite-rodriguez-2016",
    "href": "mrimethods/eeg-fmri/ballistocardiogram-removal.html#please-cite-rodriguez-2016",
    "title": "EEG-fMRI Ballistocardiogram Cleaning (Rodriguez method)",
    "section": "",
    "text": "Reversed engineered code from Rodriguez (2016) for removal of the ballistocardiogram artifacts from EEG-fMRI data when the ECG fails to give a good signal. Cameron’s dissertation is included in this repository for transparency’s sake. This code requires Matlab and the Signal Processing Toolbox.\nIf you use this code, please cite:\nRodriguez, Cameron. (2016). Improvements to Simultaneous Electroencepalography-Functional Magnetic Resonance Imaging and Electroencepalographic Source Localization. PhD Thesis for the University of California, Los Angeles.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Ballistocardiogram Cleaning (Rodriguez method)"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/ballistocardiogram-removal.html#removing-the-ballistocardiogram-with-no-ecg",
    "href": "mrimethods/eeg-fmri/ballistocardiogram-removal.html#removing-the-ballistocardiogram-with-no-ecg",
    "title": "EEG-fMRI Ballistocardiogram Cleaning (Rodriguez method)",
    "section": "Removing the Ballistocardiogram with no ECG",
    "text": "Removing the Ballistocardiogram with no ECG\n\nRemove the MR gradient signal from the EEG using your favourite software (here we used FIMRIB’s EEG tools [Niazy et al., 2005] in EEGLAB [Delorme & Makeig, 2004] saved as a .set file).\nSet up the paths to your data and change directories into the srcdir directory. Change the path to suit your dataset.\n\nsrcdir = fullfile('/Volumes', 'Lynne_32G', 'EEG', 'src');\ncd(scrdir)\n\nLoad the EEG run. Change to suit your dataset.\n\nEEG = load('-mat', [srcdir 'BHBMEG007_20210511_040344_GR.set']);\n\nGet the sampling rate. In the .set file, it is named srate and is an embedded structure of EEG. This may differ depending on the software used to remove the MR gradient noise.\n\nFs = EEG.srate\n\nRemove remaining gradient artifact at beginning and cut off part when net was removed at the end by cropping the signal.\nRestrict signal to channels included in Rodriguez 2016 for ballistocardiogram estimation (FT9 [EGI 67], TP9 [EGI 94], T7 [EGI 69], P7 [EGI 96], FT10 [EGI 219], TP10 [EGI 190], T8 [EGI 202], P8 [EGI 179]).\n\nchannels2keep = [67 94 69 96 219 190 202 179];\nstart = round(EEG.event(2).latency); % skip first TR as there is often gradient noise still in the signal  \ndata = double(EEG.data(channels2keep, start:end) );\n\nsubplot(8,1,1), plot(data(1,1:1500))\nsubplot(8,1,2), plot(data(2,1:1500))\nsubplot(8,1,3), plot(data(3,1:1500))\nsubplot(8,1,4), plot(data(4,1:1500))\nsubplot(8,1,5), plot(data(5,1:1500))\nsubplot(8,1,6), plot(data(6,1:1500))\nsubplot(8,1,7), plot(data(7,1:1500))\nsubplot(8,1,8), plot(data(8,1:1500))\n\n\n\nChannels to derive cardiac signal from.\n\n\n\nSubtract mean from each channel.\n\ndata = data - mean(data,2)\n\nCreate mean left and right channels\n\nLmean = mean(data(1:4,:));\nRmean = mean(data(5:8,:));\n\nRereference channels\n\nTake the difference of Lmean and Rmean\n\n\nLR = Lmean - Rmean;\nplot(LR(35000:40000))\n\n\n\nRereferenced channels.\n\n\n\nClean up output Step 10.1: LR mean Signal Conditioning/Filtering\n\n\nFilter series 1: Low Bandwidth\n\n0.75 Hz 6th order Butterworth filter\n10 Hz 12th order Butterworth filter\n\n\n% Generate filters\nLBW.highpass = designfilt('highpassiir', 'FilterOrder', 6, ...\n    'HalfPowerFrequency', 0.75, 'SampleRate', Fs)\n\nLBW.lowpass = designfilt('lowpassiir', 'FilterOrder', 12, ...\n    'HalfPowerFrequency', 10, 'SampleRate', Fs)\n\n% Filter Lmean\ntemp = filtfilt(LBW.highpass, Lmean);\nLmean_LBW = filtfilt(LBW.lowpass, temp);\n\n% Filter Rmean\ntemp = filtfilt(LBW.highpass, Rmean);\nRmean_LBW = filtfilt(LBW.lowpass, temp);\n\n% Filter again (only with the difference this time)\ntemp = filtfilt(LBW.highpass, (Lmean_LBW - Rmean_LBW));\nLR_LBW = filtfilt(LBW.lowpass, temp);\n\nFilter series 2: High Bandwidth\n\n0.75 Hz 6th order Butterworth filter\n50 Hz 12th order Butterworth filter\n\n\n% Generate filters\nHBW.highpass = designfilt('highpassiir', 'FilterOrder', 6, ...\n    'HalfPowerFrequency', 0.75, 'SampleRate', Fs)\n\nHBW.lowpass = designfilt('lowpassiir', 'FilterOrder', 12, ...\n    'HalfPowerFrequency', 50, 'SampleRate', Fs)\n\n% Filter Lmean\ntemp = filtfilt(HBW.highpass, Lmean);\nLmean_HBW = filtfilt(HBW.lowpass, temp);\n\n% Filter Rmean\ntemp = filtfilt(HBW.highpass, Rmean);\nRmean_HBW = filtfilt(HBW.lowpass, temp);\n\n% Filter again (only with the difference this time)\ntemp = filtfilt(HBW.highpass, (Lmean_HBW - Rmean_HBW));\nLR_HBW = filtfilt(HBW.lowpass, temp);\n\nNormalize Low Bandwidth Signal\n\nFirst take the first drivative\n\n\n\\[ x'[n] = \\frac{d}{dn}x[n], \\] where \\(x[n]\\) is the time domain representation of the LR mean signal\nLR_LBW_diff = diff(LR_LBW)/(Fs);\nThen normalize the difference representation\n\\[ x'[n] = \\frac{x'[n]}{\\text{max}\\|x'[n]\\|}\\]\nLR_LBW_norm = LR_LBW_diff./max(abs(LR_LBW_diff));\n\nGet the positive Shannon Energy envelope:\n\n\\[SEE[n] = -\\bigg( \\big&lt; ( &lt; x'_N[n], x'*_N[n] &gt; ), log( &lt; x'[n], x'*_N[n] &gt; ) \\big&gt; \\bigg)\\]\nwhen \\(x'*_N\\) is the complex conjugate. However, in most cases, this simplifies into real numbers as\n\\[SEE[n] = \\big&lt; x'[n]^2,\\ln(x'_N[n]^2)\\big&gt;\\]\nLR_LBW_SEE = -((LR_LBW_norm.^2).*log(LR_LBW_norm.^2));\nplot(LR_LBW_SEE(1:5000))\n\n\n\nShannon Entropy Envelope\n\n\nStep 10.2 : LR mean Peak Detection Round 1 – SEE\n\nFind peak SEE (Assumption of maximum of 120 beats per minute supine heart rate.)\n\nbeatspermin = 120;\nprominence_factor = 0.1;\n%[pks, locs] =  findpeaks (LR_LBW_SEE, ...\n%    'MinPeakProminence', prominence_factor, ... \n%    'MinPeakDistance', (Fs/(beatspermin/60)));\n[pks, locs] =  findpeaks (LR_LBW_SEE, ...\n    'MinPeakDistance', (Fs/(beatspermin/60)));\n\nFind local minima and maxima for each peak\n\nrange2plot = 1:5000;\nt = 1:length(LR_LBW_SEE);\n\nsubplot(2,1,1)\nplot(t(range2plot), LR_LBW_SEE(range2plot))\nlocs_small = locs(and(locs &lt;= max(range2plot), locs &gt;= min(range2plot)));\nhold on\nplot(t(locs_small), pks(and(locs &lt;= max(range2plot), locs &gt;= min(range2plot))),  'r*')\nhold off\n\n\n\nPeak Shannon Entropy\n\n\n\nGet local minima and maxima for SEE peaks\n\nTolerance of \\(\\pm200\\) ms (don’t know if this is too much – QRS complex takes .2 seconds (~200 ms))\ntol = 200; \n\nfor idx = 1:length(locs)\n    minloc = locs(idx) - tol;\n    if minloc &lt; 1\n        minloc = 1;\n    end\n    maxloc = locs(idx) + tol;\n    if maxloc &gt; length(LR_LBW_SEE)\n        maxloc = length(LR_LBW_SEE);\n    end\n    \n    [tmp, loc_tmp] = findpeaks(-LR_LBW(minloc:maxloc));\n    tmp = -tmp;\n    if length(tmp) &gt;= 1\n        minima.value(idx) = tmp(1);\n    else\n        minima.value(idx) = LR_LBW(locs(idx));\n    end\n    minima.loc(idx) = minloc + find(LR_LBW(minloc:maxloc) == minima.value(idx)) - 1;\n    \n    [tmp, loc_tmp] = findpeaks(LR_LBW(minloc:maxloc));\n    if isempty(tmp)\n        tmp = 0;\n    end\n    if length(tmp) &gt; 1\n        maxima.value(idx) = tmp(2);\n    elseif tmp == 0\n        maxima.value(idx) = max(LR_LBW(minloc:maxloc));\n    else\n        maxima.value(idx) = tmp(1);\n    end\n    maxima.loc(idx) = minloc + find(LR_LBW(minloc:maxloc) == maxima.value(idx)) - 1;\nend\n\nCompare values with average across all sets and flag all sets with at least one value greater than 1.5 standard deviations from mean.\n\nFirst the amplitude\n\n\nthreshstd = 1.5;\n\nmaxima.mean.value = mean(maxima.value);\nmaxima.std.value = std(maxima.value);\n\nmaxima2throw = or(maxima.value &gt;= maxima.mean.value + (threshstd*maxima.std.value), maxima.value &lt;= maxima.mean.value - (threshstd*maxima.std.value));\n\nminima.mean.value = mean(minima.value);\nminima.std.value = std(minima.value);\n\nminima2throw = or(minima.value &gt;= minima.mean.value + (threshstd*minima.std.value), minima.value &lt;= minima.mean.value - (threshstd*minima.std.value));\n- Then the temporal distance from SEE peak\nmaxima.dist.value = abs(locs - maxima.loc);\nmaxima.dist.mean = mean(maxima.dist.value);\nmaxima.dist.std = std(maxima.dist.value);\n\nmaximadist2throw = or(maxima.dist.value &gt;= maxima.dist.mean + (threshstd*maxima.dist.std), maxima.dist.value &lt;= maxima.dist.mean - (threshstd*maxima.dist.std));\n\nminima.dist.value = abs(locs - minima.loc);\nminima.dist.mean = mean(minima.dist.value);\nminima.dist.std = std(minima.dist.value);\n\nminimadist2throw = or(minima.dist.value &gt;= minima.dist.mean + (threshstd*minima.dist.std), minima.dist.value &lt;= minima.dist.mean - (threshstd*minima.dist.std));\n- And now temporal distance between minma and maxima\nmaximaminima.dist.value = abs(maxima.loc - minima.loc);\nmaximaminima.dist.mean = mean(maximaminima.dist.value);\nmaximaminima.dist.std = std(maximaminima.dist.value);\n\nmaximaminimadist2throw = or(maximaminima.dist.value &gt;= maximaminima.dist.mean + (threshstd*maximaminima.dist.std), maximaminima.dist.value &lt;= maximaminima.dist.mean - (threshstd*maximaminima.dist.std));\n- Now discard any peaks locations that are outside 1.5 standard deviations from the mean\npeaks2throw = (maxima2throw + maximadist2throw + maximaminimadist2throw + minima2throw + minimadist2throw &gt; 0);\n\nmaxima.thresh.value = maxima.value(~peaks2throw);\nmaxima.thresh.loc = maxima.loc(~peaks2throw);\nminima.thresh.value = minima.value(~peaks2throw);\nminima.thresh.loc = minima.loc(~peaks2throw);\n\n% Now plot them\nplot(t(range2plot), LR_LBW(range2plot))\nhold on\nlocs_small = locs(and(locs &lt;= max(range2plot), locs &gt;= min(range2plot)));\nplot(t(locs_small), LR_LBW(and(locs &lt;= max(range2plot), locs &gt;= min(range2plot))),  'r*');\nplot(t(maxima.thresh.loc(and(maxima.thresh.loc &lt;= max(range2plot), maxima.thresh.loc &gt;= min(range2plot)))), maxima.thresh.value(and(maxima.thresh.loc &lt;= max(range2plot), maxima.thresh.loc &gt;= min(range2plot))),  'g*')\nplot(t(minima.thresh.loc(and(minima.thresh.loc &lt;= max(range2plot), minima.thresh.loc &gt;= min(range2plot)))), minima.thresh.value(and(minima.thresh.loc &lt;= max(range2plot), minima.thresh.loc &gt;= min(range2plot))),  'b*')\nhold off\n\n\n\nPeak distances.\n\n\n% clear all figures\nclf\n- Remove any between peak distances with an interbeat interval greater than 1.5 seconds (~40 beats per min) and less than 0.5 seconds (~120 beats per minute).\nIBI.value = diff(minima.thresh.loc);\nIBI.thresh1 = 1.5*Fs;\nIBI.thresh2 = 0.5*Fs;\n\nIBI2keep = and(IBI.value &lt; IBI.thresh1, IBI.value &gt; IBI.thresh2);\n- From the remaining IBI, take the maximum of the mean IBI minus 2 standard deviations or 0.5 seconds. This is the window for creating the ballistocardiogram template.\nIBI.thresh.value = IBI.value(IBI2keep);\nIBI.thresh.mean = mean(IBI.thresh.value);\nIBI.thresh.std = std(IBI.thresh.value);\n\nIBIwindow = round(max(0.5*Fs, IBI.thresh.mean - (2*IBI.thresh.std)));\n- Compute average LR_LBW of IBIwindow size (like 'ERPs') centered on max peak\nIBI2keep4minima = logical([IBI2keep 0]);\nminima.thresh.valueIBI = minima.thresh.value(IBI2keep4minima);\nminima.thresh.locIBI = minima.thresh.loc(IBI2keep4minima);\n\nif mod(IBIwindow,2) ~= 0\n    IBIwindow = IBIwindow - 1;\nend\n\n\ncount = 1;\nfor i = 1:length(minima.thresh.locIBI)\n    minloc = minima.thresh.locIBI(i) - IBIwindow/2;\n    maxloc = minima.thresh.locIBI(i) + IBIwindow/2;\n    if minloc &lt; 1 | maxloc &gt; minima.thresh.locIBI(end) - IBIwindow/2;\n        continue\n    end\n    template.matrix(count,:) = LR_LBW(minloc:maxloc);\n    count=count+1;\nend\n\ntemplate.mean = mean(template.matrix);\ntemplate.std = std(template.matrix);\n- Compute BCG window.\n\nAdd path to the bounded line package. The bounded line package can be downloaded from [here](https://github.com/kakearney/boundedline-pkg/). Change path to fit your data. \naddpath('~/Documents/MATLAB/kakearney-boundedline-pkg-8179f9a/boundedline/')\naddpath('~/Documents/MATLAB/kakearney-boundedline-pkg-8179f9a/catuneven/')\naddpath('~/Documents/MATLAB/kakearney-boundedline-pkg-8179f9a/Inpaint_nans/')\naddpath('~/Documents/MATLAB/kakearney-boundedline-pkg-8179f9a/readmeExtras/')\naddpath('~/Documents/MATLAB/kakearney-boundedline-pkg-8179f9a/singlepatch/')\nboundedline(1-IBIwindow/2:1+IBIwindow/2, template.mean, template.std, 1-IBIwindow/2:1+IBIwindow/2, repmat(0, 1, length(template.std)), 0, 'r-')\n\nuthr = template.mean + template.std;\nlthr = template.mean - template.std;    \n\n\ncentre = IBIwindow/2+1;\n\ncutoff = find(uthr &lt; 0);\n\nmindistance = max(cutoff) - min(cutoff);\n\nGet cross correlations for each time window lag in data\n\nlngX = length(LR_LBW);\nlngY = length(template.mean);\nassert(lngX &gt;= lngY);\nlags = 0:(lngX-lngY);\nfor i = lags\n   c(i+1) = xcorr(LR_LBW(i+1:i+lngY) - mean(LR_LBW(i+1:i+lngY)), template.mean - mean(template.mean),0,'coeff');\nend\n\nclf\nplot(lags(range2plot),c(range2plot));\nxlabel('lags'); title('normalized cross-correlation');\n\n\n\nNormalized Cross Correlation\n\n\n[pks2, loc2] = findpeaks(c, 'MinPeakDistance', mindistance);\n\nqpkloc = loc2 + (maxima.thresh.loc(1) - loc2(1));\n\nplot(LR_LBW(1:5000))\nhold on\nplot(qpkloc(and(qpkloc &lt;= max(1:5000), qpkloc &gt;= min(1:5000))), LR_LBW(qpkloc(and(qpkloc &lt;= max(1:5000), qpkloc &gt;= min(1:5000)))), 'r*')\nhold off\n\n\nCross correlation error checking\n\nmeanIBI = movmean(diff(qpkloc),4) - mean(diff(qpkloc)); % center the data \ndeviationIBI = diff(qpkloc) - mean(diff(qpkloc));\n\nclf\nboundedline(1:length(meanIBI), meanIBI, 100, 'r-', 1:length(meanIBI), deviationIBI, 0, 'b-')\n\nFind the deviations greater than and less than a 100 ms (0.1 second) window around moving average\nqpkloc_new = qpkloc;\n\ndeviationIBIlocs = find(deviationIBI &gt; meanIBI + 100 | deviationIBI &lt; meanIBI - 100);\nif ~isempty(deviationIBIlocs)\n    disp('There are deviations outside threshold');\n    if length(deviationIBIlocs) &gt;= 1\n        bipolar = find(diff(deviationIBIlocs) == 1);\n        if ~isempty(bipolar)\n           for pk = 1:length(bipolar)\n                   [tmp3, loc3]  =  findpeaks(c(qpkloc(deviationIBIlocs(pk))-100:qpkloc(deviationIBIlocs(pk))+100));\n               if isempty(tmp3)\n                   continue\n               else\n                   [m,idx] = max(tmp3);\n                   qpkloc_new(deviationIBIlocs(pk)) = qpkloc(deviationIBIlocs(pk)) - 100 + loc3(idx);   \n                   % Done 2018/12/11 -- TODO: SHIFT PEAK POINT IN qpkloc to largest local peak in cross correlation signal\n               end\n           end\n        end\n    end\nend\n\nApply timing to larger bandwidth\n\n[HBWpks, HBWlocs] = findpeaks(LR_HBW, 'MinPeakDistance', mindistance);\ntemp = ismembertol(HBWlocs, qpkloc, 20, 'DataScale', 2); % put 0.01 second tolerance on match (although Rodriguez's dissertation doesn't specify what is meant by 'near') &lt;-- this is obviously wrong as it takes a vector of 900 and reduces it to 37. Still need to figure this one out.\nSPpeaks = HBWlocs; % HBWlocs(temp);\n\nplot(1:length(LR_HBW(range2plot)), LR_HBW(range2plot), SPpeaks(ismember(SPpeaks, range2plot)), LR_HBW(SPpeaks(ismember(SPpeaks, range2plot))), 'r*')\n\n\nSave to text file that needs minimal modifcations to be imported into EEGLAB\n\nT = table([SPpeaks' + start - 1], [repmat('sppeak', length(SPpeaks), 1)], ...\n    'VariableNames', {'latency', 'type'});\nwritetable(T,[srcdir nom],'Delimiter','\\t')\n\nDo qrs cleaning in EEGLAB using the FIMRIB tools.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "EEG-fMRI",
      "EEG-fMRI Ballistocardiogram Cleaning (Rodriguez method)"
    ]
  },
  {
    "objectID": "mrimethods/eeg-fmri/index.html",
    "href": "mrimethods/eeg-fmri/index.html",
    "title": "EEG-fMRI",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "mriscanners/index.html",
    "href": "mriscanners/index.html",
    "title": "MRI Scanners",
    "section": "",
    "text": "No matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI Scanners"
    ]
  },
  {
    "objectID": "orientation/index.html",
    "href": "orientation/index.html",
    "title": "Orientation",
    "section": "",
    "text": "MRI Safety FAQ\n\n\n\n\n\n\nsafety\n\n\nmri\n\n\n\nYour frequently asked MRI safety questions answered\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMattermost: Instant Messaging / Stay in Touch\n\n\n\n\n\n\ncollaboration\n\n\nsoftware\n\n\ninstant messaging\n\n\n\nInstant messaging app for users of the research MRI facility\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Orientation"
    ]
  },
  {
    "objectID": "orientation/mrisafetyfaq.html",
    "href": "orientation/mrisafetyfaq.html",
    "title": "MRI Safety FAQ",
    "section": "",
    "text": "Frequencly Asked Questions Answered\n[Note: the following was taken from here.\nThis is a rough guide; some things may no longer be accurate for our facility]\n\nCan you have an MRI if you have fillings, retainers or braces on your teeth?\n\nThe metal in most fillings is not affected by the MRI system’s magnetic field. However, the fillings may cause some distortion of the images if you are having a scan of the brain or facial area.\n\nCan I have an MRI exam if I have a permanent retainer or braces on my teeth?\n\nThe recommendation is not to scan a subject with retainers or braces because of distortion can occur that can affect the quality of the data.\n\nCan you have make-up on for an MRI?\n\nSome cosmetics contain metals that can interact with MRI magnets, so on the day of the MRI, if possible, ask your subject to not wear makeup. Also, minimize hairspray and forgo antiperspirants and sunscreens, which contain metals, that could interfere with the scan and reduce the quality of the images.\n\nCan you put on deodorant for an MRI?\n\nPlease refrain from wearing any powder, perfumes, deodorant and/or lotions on your underarms and breasts prior to the procedure. Since the MRI is a magnet, please let us know if you have any metal in or on your body.\n\nIs titanium safe for MRI?\n\nTitanium implants are MRI compatible. Subjects with nontitanium implants should inform the MRI safety officer 7 days prior to the procedure to check with the manufacture for condition that allow safely conducting an MRI scan.\n\nWhat do you wear for an MRI?\n\nIf the subject’s clothes have any metal fasteners or metallic design in close proximity to the anatomy being scanned, you should ask the subject to change into a hospital gown. If the clothing is made of a stretchy material, the subject must change into a hospital gown as a precautionary measure of safety to prevent possible burns to the skin. A locker will be supplied to secure their belongings.\n\nIs it safe to have an MRI while pregnant?\n\nUnless the research MRI study involves prenatal or pregnant subjects, pregnant subjects are excluded.\n\nAre piercings safe for MRI?\n\nIt is the policy to remove all piercings before you scan your subject to avoid any possible burns due to heating of the metal used in the piercings. Piercings, if not removed, i.e. earrings, will cause distortion to the image.\n\n\nSafety of frequently asked implant MRI compatibility:\nIf a subject has an implant it must be reviewed by the MRI safety officer before the subject can be scanned. Written documentation must be provided to determine the type of implant.\n\nHearing Aids and Other Hearing Systems\nUnsafe.\n\n\nHeart Valves and Annuloplasty Rings\nSafe to scan at 3T or less.\n\n\nHemostatic clips, other clips, fasteners, staples\nSafe at 3T or less.\n\n\nIUDs and other contraceptive devices\n\nCopper T and Copper 7: Safe at 1.5 T or less.\nCopper T 380A: Safe at 3T or less.\nStainless steel IUDs: Unsafe.\nMirena IUD: Safe.\nImplanon implant: Safe.\nEssure: Safe to scan at 3T or less.\n\n\n\nMagnetically activated implants and devices\nUnsafe.\n\n\nNeurostimulation system\nUnsafe.\n\n\nOtologic Implants\nUnsafe.\n\n\nPenile Implants\nUnsafe – Duraphase or Omniphase.\n\n\nPessary/Pessaries\nUnsafe – metallic. Safe – nonmetallic.\n\n\nOrthopedic implants\nMust check manufactures conditions for MRI safety.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Orientation",
      "MRI Safety FAQ"
    ]
  },
  {
    "objectID": "studentresources/index.html",
    "href": "studentresources/index.html",
    "title": "Student Resources",
    "section": "",
    "text": "No matching items\n\n Back to top",
    "crumbs": [
      "Home",
      "Student Resources"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html",
    "href": "tutorials/Docker/docker.html",
    "title": "Docker",
    "section": "",
    "text": "an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nLet’s begin with a lengthy analogy that will hopefully motivate the usefulness of Docker (and send light on the wordy definitions, as given above). I have summarised the analogy from this post, so have a look there if anything immediately below does not make sense.\nLet’s think about shipping containers (part of Docker logo). Before shipping containers were used, international trade was inefficient, workers would fill ships up with cargo of different sizes and requirements (e.g. fragile or refrigerated). Depending on the size and shape of the ship vs. the size and shape of the cargo a lot of the time the cargo was not compatible or would result in it perishing/breaking.\nThen came along containers,\n\nContainers were standardised so that loading and unloading became efficient.\nAnything that happened inside the container was the containers fault and not the ships. The merchants were responsible for securing everything inside.\nThe containers only need a sturdy enough ship to carry them (i.e. they don’t care about model, shape or size).\n\nWhat happens if the ship sinks (i.e. computer goes down, OS runs out of memory), unlike a shipping container where its contents are lost. You use a Dockerfile (analogous to a seed) to sprout up a new container. Re-creating the container is very easy, they can also be set-up so that backups are kept or that another container will be automatically birthed and carry on the task if one goes down.\nWhy hasn’t Docker taken over. Similar to international trade, shipping containers are very helpful, but they need someone to load, unload and ship. In the same way, the infrastructure necessary to fully take advantage of Docker is still on its way.\nTake away\n\nDocker containers package everything needed. e.g. If you order a car building kit you would get a big container containing all the different parts in smaller boxes vs. picking all individual parts at separate stores.\nPredictable environment. To re-make another car you can re-order the same package and be confident you’ll get the exact same kit vs. having to revisit all the stores again, likely to make a mistake.\nDocker isolated from other packages. e.g. If you are building your car alongside a motorbike project you would two separate piles for your parts vs. having all the parts in one pile and accidentally using a screw that may initially fit but ends up falling out and ruining the car.\n\n\n\n\n\n\n\ndockervsvirtual\n\n\nBelow is a summary of the following post, I would encourage reading through the post for more detail.\n\n\nA virtual machine is a virtual server that emulates a hardware server. i.e Hardware not in your computer is used to create a full computer environment that you can use to run an application. For example parallels can be used as an Apple user to run Windows applications on their Apple computer. NOTE: You will be in a Windows environment so you cannot run an Apple application but you can transfer files back and forth.\n\n\n\nDocker uses containers that are platform independent (run across Windows and Linux). It’s main purpose is to run microservice applications, exactly what we need for neuroimaging applications. Docker is not a one-size-fits-all solution.\n\n\n\n\nDocker isolates individual applications vs. VM isolating entire systems (i.e. you can use containers inside VM’s)\nA Docker container can boot in less than a second vs. VM can take up to a few minutes\nUse a Docker container to package code and its dependencies so that the code is easily shared with Dev, QA and IT.\nDocker has version control, can be compared to GitHub\n\nNOTE: Docker containers are lightweight compared to VM’s however, similar to having an empty or even unused container on a ship. It is advised to remove the container, remember (like a seed) it is easy to re-create the container if you have a Docker File.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html#what-is-docker",
    "href": "tutorials/Docker/docker.html#what-is-docker",
    "title": "Docker",
    "section": "",
    "text": "an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nLet’s begin with a lengthy analogy that will hopefully motivate the usefulness of Docker (and send light on the wordy definitions, as given above). I have summarised the analogy from this post, so have a look there if anything immediately below does not make sense.\nLet’s think about shipping containers (part of Docker logo). Before shipping containers were used, international trade was inefficient, workers would fill ships up with cargo of different sizes and requirements (e.g. fragile or refrigerated). Depending on the size and shape of the ship vs. the size and shape of the cargo a lot of the time the cargo was not compatible or would result in it perishing/breaking.\nThen came along containers,\n\nContainers were standardised so that loading and unloading became efficient.\nAnything that happened inside the container was the containers fault and not the ships. The merchants were responsible for securing everything inside.\nThe containers only need a sturdy enough ship to carry them (i.e. they don’t care about model, shape or size).\n\nWhat happens if the ship sinks (i.e. computer goes down, OS runs out of memory), unlike a shipping container where its contents are lost. You use a Dockerfile (analogous to a seed) to sprout up a new container. Re-creating the container is very easy, they can also be set-up so that backups are kept or that another container will be automatically birthed and carry on the task if one goes down.\nWhy hasn’t Docker taken over. Similar to international trade, shipping containers are very helpful, but they need someone to load, unload and ship. In the same way, the infrastructure necessary to fully take advantage of Docker is still on its way.\nTake away\n\nDocker containers package everything needed. e.g. If you order a car building kit you would get a big container containing all the different parts in smaller boxes vs. picking all individual parts at separate stores.\nPredictable environment. To re-make another car you can re-order the same package and be confident you’ll get the exact same kit vs. having to revisit all the stores again, likely to make a mistake.\nDocker isolated from other packages. e.g. If you are building your car alongside a motorbike project you would two separate piles for your parts vs. having all the parts in one pile and accidentally using a screw that may initially fit but ends up falling out and ruining the car.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html#docker-vs.-virtual-machine",
    "href": "tutorials/Docker/docker.html#docker-vs.-virtual-machine",
    "title": "Docker",
    "section": "",
    "text": "dockervsvirtual\n\n\nBelow is a summary of the following post, I would encourage reading through the post for more detail.\n\n\nA virtual machine is a virtual server that emulates a hardware server. i.e Hardware not in your computer is used to create a full computer environment that you can use to run an application. For example parallels can be used as an Apple user to run Windows applications on their Apple computer. NOTE: You will be in a Windows environment so you cannot run an Apple application but you can transfer files back and forth.\n\n\n\nDocker uses containers that are platform independent (run across Windows and Linux). It’s main purpose is to run microservice applications, exactly what we need for neuroimaging applications. Docker is not a one-size-fits-all solution.\n\n\n\n\nDocker isolates individual applications vs. VM isolating entire systems (i.e. you can use containers inside VM’s)\nA Docker container can boot in less than a second vs. VM can take up to a few minutes\nUse a Docker container to package code and its dependencies so that the code is easily shared with Dev, QA and IT.\nDocker has version control, can be compared to GitHub\n\nNOTE: Docker containers are lightweight compared to VM’s however, similar to having an empty or even unused container on a ship. It is advised to remove the container, remember (like a seed) it is easy to re-create the container if you have a Docker File.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html#getting-started",
    "href": "tutorials/Docker/docker.html#getting-started",
    "title": "Docker",
    "section": "Getting Started",
    "text": "Getting Started\nDocker’s own Getting Started Guide",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html#install",
    "href": "tutorials/Docker/docker.html#install",
    "title": "Docker",
    "section": "Install",
    "text": "Install\nMac, Linux, and Windows",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html#terminology",
    "href": "tutorials/Docker/docker.html#terminology",
    "title": "Docker",
    "section": "Terminology",
    "text": "Terminology\n\nImages: The blueprints of our application which form the basis of containers.\nContainers: Created from Docker images and run the actual application.\nDocker Daemon: The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\nDocker Client: The command line tool that allows the user to interact with the daemon.\nDocker Hub: A registry of Docker images (similar idea to GitHub or HomeBrew).",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html#set-up",
    "href": "tutorials/Docker/docker.html#set-up",
    "title": "Docker",
    "section": "Set-up",
    "text": "Set-up\nTest the install with\ndocker run hello-world",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/Docker/docker.html#commands",
    "href": "tutorials/Docker/docker.html#commands",
    "title": "Docker",
    "section": "Commands",
    "text": "Commands\nTo take an image from the Docker Hub\ndocker pull &lt;name-of-image&gt;\nTo check the images on the system\ndocker images\nTo run a Docker container based on the image. NOTE: Without any arguments, generally not much will happen. If you run a neuroimaging function, generally the function usage will be output.\ndocker run &lt;name-of-image&gt;\nHere is a helpful link for docker run options: link\nUsing the container ID output from ‘docker images’ you can remove the container to free up space.\ndocker rm &lt;name-of-image&gt;\nIf the Docker container creates/manipulates files, unless otherwise specified these files are part of the container and will be removed when the container is. Read more about it here You need to pass the -v flag. For example: - Inside the container the app creates /usr/src/app/logs - To map to host machine -v ~/logs:/usr/src/app/logs",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html",
    "href": "tutorials/docker.html",
    "title": "Docker",
    "section": "",
    "text": "dockerlogo",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html#what-is-docker",
    "href": "tutorials/docker.html#what-is-docker",
    "title": "Docker",
    "section": "What is Docker",
    "text": "What is Docker\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nLet’s begin with a lengthy analogy that will hopefully motivate the usefulness of Docker (and send light on the wordy definitions, as given above). I have summarised the analogy from this post, so have a look there if anything immediately below does not make sense.\nLet’s think about shipping containers (part of Docker logo). Before shipping containers were used, international trade was inefficient, workers would fill ships up with cargo of different sizes and requirements (e.g. fragile or refrigerated). Depending on the size and shape of the ship vs. the size and shape of the cargo a lot of the time the cargo was not compatible or would result in it perishing/breaking.\nThen came along containers,\n\nContainers were standardised so that loading and unloading became efficient.\nAnything that happened inside the container was the containers fault and not the ships. The merchants were responsible for securing everything inside.\nThe containers only need a sturdy enough ship to carry them (i.e. they don’t care about model, shape or size).\n\nWhat happens if the ship sinks (i.e. computer goes down, OS runs out of memory), unlike a shipping container where its contents are lost. You use a Dockerfile (analogous to a seed) to sprout up a new container. Re-creating the container is very easy, they can also be set-up so that backups are kept or that another container will be automatically birthed and carry on the task if one goes down.\nWhy hasn’t Docker taken over. Similar to international trade, shipping containers are very helpful, but they need someone to load, unload and ship. In the same way, the infrastructure necessary to fully take advantage of Docker is still on its way.\nTake away\n\nDocker containers package everything needed. e.g. If you order a car building kit you would get a big container containing all the different parts in smaller boxes vs. picking all individual parts at separate stores.\nPredictable environment. To re-make another car you can re-order the same package and be confident you’ll get the exact same kit vs. having to revisit all the stores again, likely to make a mistake.\nDocker isolated from other packages. e.g. If you are building your car alongside a motorbike project you would two separate piles for your parts vs. having all the parts in one pile and accidentally using a screw that may initially fit but ends up falling out and ruining the car.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html#docker-vs.-virtual-machine",
    "href": "tutorials/docker.html#docker-vs.-virtual-machine",
    "title": "Docker",
    "section": "Docker vs. Virtual Machine",
    "text": "Docker vs. Virtual Machine\n\nBelow is a summary of the following post, I would encourage reading through the post for more detail.\n\nVirtual Machine\nA virtual machine is a virtual server that emulates a hardware server. i.e Hardware not in your computer is used to create a full computer environment that you can use to run an application. For example parallels can be used as an Apple user to run Windows applications on their Apple computer. NOTE: You will be in a Windows environment so you cannot run an Apple application but you can transfer files back and forth.\n\n\nDocker\nDocker uses containers that are platform independent (run across Windows and Linux). It’s main purpose is to run microservice applications, exactly what we need for neuroimaging applications. Docker is not a one-size-fits-all solution.\n\n\nDifferences\n\nDocker isolates individual applications vs. VM isolating entire systems (i.e. you can use containers inside VM’s)\nA Docker container can boot in less than a second vs. VM can take up to a few minutes\nUse a Docker container to package code and its dependencies so that the code is easily shared with Dev, QA and IT.\nDocker has version control, can be compared to GitHub\n\nNOTE: Docker containers are lightweight compared to VM’s however, similar to having an empty or even unused container on a ship. It is advised to remove the container, remember (like a seed) it is easy to re-create the container if you have a Docker File.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html#getting-started",
    "href": "tutorials/docker.html#getting-started",
    "title": "Docker",
    "section": "Getting Started",
    "text": "Getting Started\nDocker’s own Getting Started Guide",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html#install",
    "href": "tutorials/docker.html#install",
    "title": "Docker",
    "section": "Install",
    "text": "Install\nMac, Linux, and Windows",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html#terminology",
    "href": "tutorials/docker.html#terminology",
    "title": "Docker",
    "section": "Terminology",
    "text": "Terminology\n\nImages: The blueprints of our application which form the basis of containers.\nContainers: Created from Docker images and run the actual application.\nDocker Daemon: The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\nDocker Client: The command line tool that allows the user to interact with the daemon.\nDocker Hub: A registry of Docker images (similar idea to GitHub or HomeBrew).",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html#set-up",
    "href": "tutorials/docker.html#set-up",
    "title": "Docker",
    "section": "Set-up",
    "text": "Set-up\nTest the install with\ndocker run hello-world",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "tutorials/docker.html#commands",
    "href": "tutorials/docker.html#commands",
    "title": "Docker",
    "section": "Commands",
    "text": "Commands\nTo take an image from the Docker Hub\ndocker pull &lt;name-of-image&gt;\nTo check the images on the system\ndocker images\nTo run a Docker container based on the image. NOTE: Without any arguments, generally not much will happen. If you run a neuroimaging function, generally the function usage will be output.\ndocker run &lt;name-of-image&gt;\nHere is a helpful link for docker run options: link\nUsing the container ID output from ‘docker images’ you can remove the container to free up space.\ndocker rm &lt;name-of-image&gt;\nIf the Docker container creates/manipulates files, unless otherwise specified these files are part of the container and will be removed when the container is. Read more about it here\nYou will need to pass the -v flag.\nFor example:\n\nInside the container the app creates /usr/src/app/logs\nTo map to host machine -v ~/logs:/usr/src/app/logs",
    "crumbs": [
      "Home",
      "Tutorials",
      "Docker"
    ]
  },
  {
    "objectID": "mrisoftware/3D-Slicer.html#loading-nifti-files",
    "href": "mrisoftware/3D-Slicer.html#loading-nifti-files",
    "title": "3D Slicer",
    "section": "Loading Nifti Files",
    "text": "Loading Nifti Files\nMany medical imaging files come in what’s called the ‘NIfTI’ format (stands for Neuroimaging Informatics Technology Initiative). These files will end in either .nii or a compressed version .nii.gz\nTo load a .nii or .nii.gz file:\nFile &gt; Add Data &gt; Choose File(s) to Add\n\nNow you should be able to view three different ‘slices’ of your brain: \nUsing your mouse, hover over the view you want to interact with, and use your scroll wheel to navigate through slices. Or hold down CTRL while using the scroll wheel to zoom in.\nTo move the image around (in case you want to zoom into a particular area), hold down your scroll-wheel and move your cursor.",
    "crumbs": [
      "Home",
      "MRI Software",
      "3D Slicer"
    ]
  },
  {
    "objectID": "mrisoftware/3D-Slicer.html#d-rendering",
    "href": "mrisoftware/3D-Slicer.html#d-rendering",
    "title": "3D Slicer",
    "section": "3D Rendering",
    "text": "3D Rendering\nWould you like to see your brain in 3D and perform a ‘virtual dissection’?\n\nClick on the \"Welcome to Slicer\" drop down menu (Top left)\nClick on \"Volume Rendering\"\nClick on the 'closed eye' in the top left (beside the name of the image)\nYou can then do a digital dissection by clicking on Crop: Enable; Display ROI, then click on the green, red, blue whatever circles to move the planes to dissect\n\nHere is a gif that shows these steps in detail [Note: if the image is too small, right-click, and press ‘Open Image in New Tab’]\n\nMore info: slicer.readthedocs.io/en/latest/user_guide/modules/volumerendering.html",
    "crumbs": [
      "Home",
      "MRI Software",
      "3D Slicer"
    ]
  },
  {
    "objectID": "mrisoftware/3D-Slicer.html#manual",
    "href": "mrisoftware/3D-Slicer.html#manual",
    "title": "3D Slicer",
    "section": "Manual",
    "text": "Manual\nslicer.readthedocs.io/en/latest/",
    "crumbs": [
      "Home",
      "MRI Software",
      "3D Slicer"
    ]
  },
  {
    "objectID": "mrisoftware/3D-Slicer.html#faq",
    "href": "mrisoftware/3D-Slicer.html#faq",
    "title": "3D Slicer",
    "section": "FAQ",
    "text": "FAQ\nslicer.org/wiki/Documentation/4.5/FAQ/General",
    "crumbs": [
      "Home",
      "MRI Software",
      "3D Slicer"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html",
    "href": "mrimethods/Anatomical.html",
    "title": "Anatomical",
    "section": "",
    "text": "This is a large topic, and you probably want to check out ANTs\nBut here is a nice tutorial to get you started on the ideas (linear, non-linear, inverse, apply warp, etc.)\nhttps://fsl.fmrib.ox.ac.uk/fslcourse/graduate/lectures/practicals/registration/\n\n\nhttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT/FAQ#How_do_I_make_my_own_study-specific_template_image_with_FLIRT.3F\nOr try ANTs: antsMultivariateTemplateConstruction2.sh\n\n\n\nGenerally you want to register low to high resolution. Then, if that isn't he actual direction you want to go, you would invert the registration matrix and apply it in the oppposite direction.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#make-your-own-template",
    "href": "mrimethods/Anatomical.html#make-your-own-template",
    "title": "Anatomical",
    "section": "",
    "text": "https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT/FAQ#How_do_I_make_my_own_study-specific_template_image_with_FLIRT.3F\nOr try ANTs: antsMultivariateTemplateConstruction2.sh",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#tips",
    "href": "mrimethods/Anatomical.html#tips",
    "title": "Anatomical",
    "section": "",
    "text": "Generally you want to register low to high resolution. Then, if that isn't he actual direction you want to go, you would invert the registration matrix and apply it in the oppposite direction.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#comparison-of-methods",
    "href": "mrimethods/Anatomical.html#comparison-of-methods",
    "title": "Anatomical",
    "section": "Comparison of methods",
    "text": "Comparison of methods\nCheck this paper out\n\n\"The overall analysis of the skull stripping techniques against the “silver-standard” consensus showed that STAPLE, ANTS and BEaST achieved the highest Dice coefficient metrics and had smaller variance (standard deviation), therefore they have high agreement with the consensus mask and their performance was consistent. STAPLE's Dice coefficient was significantly different compared to all the methods, except for ANTs and MBWSS.\nThe Dice coefficient metric represents a compromise between sensitivity (including brain tissue) and specificity (not including non-brain tissue). STAPLE, OPTIBET, BEaST and ANTS were found to be robust techniques; their Dice coefficients were higher than 0.9 for all 359 subjects assessed (Table 7). BSE, BET are the less consistent (robust) with higher standard deviations. MBWSS has the fourth highest Dice coefficient, it suffers from failing in some cases (13 subjects; Dice &lt;0.9) by leaving a big portion of the brain out of the segmentation mask. If we excluded these failures, MBWSS would have an average Dice coefficient of 97.58 1.4 ± 9, which is close to the results obtained by ANTs and STAPLE.\"\n\nMy own attempts, looking at ANTsPyNet, optimized bet, BEaST, and FastSurfer are as follows:",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#bet",
    "href": "mrimethods/Anatomical.html#bet",
    "title": "Anatomical",
    "section": "BET",
    "text": "BET\nThe go-to method (although probably shouldn't be)\nSee FSL's page\nSee this paper on optimizing: https://pubmed.ncbi.nlm.nih.gov/22484407/\n\n\"The removal of the neck slices, either externally or within BET, has a marked positive effect on the brain extraction quality. BET option \"B\" with f=0.1 after removal of the neck slices seems to work best for all acquisition protocols. \"\n\nrobustfov -i T1w.nii.gz -r T1w_noneck\nbet T1w_noneck.nii.gz brain -m -R -B -f 0.1",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#dskullstrip-afni",
    "href": "mrimethods/Anatomical.html#dskullstrip-afni",
    "title": "Anatomical",
    "section": "3dSkullStrip (AFNI)",
    "text": "3dSkullStrip (AFNI)\nTo run, type 3dSkullStrip to get a list of inputs/parameters\nBasic run:\n3dSkullStrip -input input_image.nii.gz -prefix output_image.nii.gz\n[note: final product may be deobliqued and no longer align with original image…]\nOR you can run @SSwarper [note: takes about 1hr]:\n@SSwarper -input ../T1w.nii.gz -base MNI152_2009_template_SSW.nii.gz -subid sub-001 -odir group/o.aw_sub-001\n[note: final product may be deobliqued and no longer align with original image…]",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#ants",
    "href": "mrimethods/Anatomical.html#ants",
    "title": "Anatomical",
    "section": "ANTs",
    "text": "ANTs\nhttps://andysbrainbook.readthedocs.io/en/latest/ANTs/ANTs_Overview.html\nhttps://dpaniukov.github.io/2016/06/06/brain-extraction-with-ants.html\nantsBrainExtraction.sh -d 3 -a sub-001_T1w_202.nii.gz -e brainWithSkullTemplate.nii.gz -m brainPrior.nii.gz -o anat_Stripped.nii\nThe option “-d 3” means that it is a three-dimensional image; “-a” indicates the anatomical image to be stripped; and “-e” is used to supply a template for skull-stripping. (which ones?) “-m” will generate a brain mask, and “-o” is the label for the output.\nTemplates can be downloaded here: https://figshare.com/articles/dataset/ANTs_ANTsR_Brain_Templates/915436\n[note: final product may be aligned to the template you use and no longer aligned with original image]",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#antspynet",
    "href": "mrimethods/Anatomical.html#antspynet",
    "title": "Anatomical",
    "section": "ANTsPyNet",
    "text": "ANTsPyNet\nI actually prefer this version of ANTs Brain Extraction to the above.\nhttps://github.com/ANTsX/ANTsPy\nand https://github.com/ANTsX/ANTsPyNet\npip install antspyx\npip install antspynet\nIn Python:\nfrom antspynet import brain_extraction\nimport os\nfrom ants import atropos, get_ants_data, image_read, resample_image, get_mask\nexample_filename = os.path.join('../..', 'T1w.nii.gz')\nimages = image_read(example_filename)\nprobability_brain_mask = brain_extraction(images, modality=\"t1\")\nprobability_brain_mask.to_file('antsbrainmask.nii.gz')\nBash:\nflirt -in antsbrainmask.nii.gz -ref ../../T1w.nii.gz -out antsbrainmask_orig.nii.gz -applyxfm -usesqform\nfslmaths antsbrainmask_orig.nii.gz -thr 0.5 -bin antsbrainmask_orig.nii.gz\nfslmaths antsbrainmask_orig.nii.gz -mul ../../T1w.nii.gz antsbrain.nii.gz\nOr just use this one simple script: https://github.com/WeberLab/SkullStrip_ANTsPyNet/tree/main",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#beast",
    "href": "mrimethods/Anatomical.html#beast",
    "title": "Anatomical",
    "section": "BEaST",
    "text": "BEaST\nBEaST is a software toolkit to extract brain from hight resolution T1 images. BEaST refers to Brain Extraction based on non-local Segmentation Technique which was published in Neuroimage by Eskildsen and colleagues (Eskildsen et al, 2012).\nInstallation and tutorial can be found here:\nhttps://rpubs.com/conge/beast_intro\nCitation: Simon F. Eskildsen, Pierrick Coupé, Vladimir Fonov, José V. Manjón, Kelvin K. Leung, Nicolas Guizard, Shafik N. Wassef, Lasse R. Østergaard, D. Louis Collins, and The Alzheimer's Disease Neuroimaging Initiative, BEaST: Brain extraction based on nonlocal segmentation technique, NeuroImage, vol. 59(3), pp. 2362-2373. ISSN 1053-8119, 10.1016/j.neuroimage.2011.09.012.\n\nInstallation on Ubuntu:\nDependencies\n\nYou might need these installed:\n\nsudo apt install cmake-curses-gui\n\nFirst install MINC\n\ncd ~/Downloads\nwget http://packages.bic.mni.mcgill.ca/minc-toolkit/Debian/minc-toolkit-1.9.18-20200813-Ubuntu_20.04-x86_64.deb\nsudo dpkg -i minc-toolkit-1.9.18-20200813-Ubuntu_20.04-x86_64.deb\nsource /opt/minc/1.9.18/minc-toolkit-config.sh\nexport LIBMINC_DIR=/opt/minc/1.9.18/lib/cmake/\n[note: please add source /opt/minc/1.9.18/minc-toolkit-config.sh into your bashrc file so you don't have to do it every time before running the toolkit:]\necho \"source /opt/minc/minc-toolkit-config.sh\" &gt;&gt; ~/.bashrc;\n[NOTE: if the above doesn't work, try: https://github.com/BIC-MNI/minc-toolkit-v2 ]\n\nInstall NIfTI libraries\n\nsudo apt-get install libnifti-dev\n\nInstall library for Hierarchical Data Format 5 support\n\nsudo apt-get install libhdf5-dev\nBEaST Proper\n\nDownload source code:\n\ncd ~/Downloads\ngh repo clone BIC-MNI/BEaST #note: this uses the git hub cli client; see [[[git]]]\ncd BEaST\n\nCompile and Install BEaST\n\nRun the code below to configure the installation.\nccmake CMakeLists.txt\nAt the step, you need to make sure all the path is correct in the CMakeList.txt. Type “c” to configure the installation and type “g” to generate configuration. If everything is correct, runt the code below to install BEaST to your system.\nFor me, I needed to edit and include: /opt/minc/1.9.18/lib/cmake/\nMore Troubleshooting:\n\nNIFTI_ROOT should be set to /usr if you installed NIfTI libraries\n\nusing the package libnifti-dev\n\nIf the compiler cannot find hdf5.h you probably need to install\n\nlibhdf5-serial-dev\n\nIf you get the message: \"Could not find module FindLIBMINC.cmake or\n\na configuration file for package LIBMINC.\", you must point to the\ndirectory containing either FindLIBMINC.cmake or\nLIBMINCConfig.cmake. If you have installed MINC Tool Kit,\nhttp://www.bic.mni.mcgill.ca/ServicesSoftware/ServicesSoftwareMincToolKit ,\nthe directory is most likely /opt/minc/lib\n\nmake sure fsl's path isn't before your major ones: put this at the very end of your ~/.bashrc file:\n\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$PATH\nmake\nsudo make install\n\nInstall BEaST Libraries\n\ncd ~/Downloads\nwget http://packages.bic.mni.mcgill.ca/tgz/beast-library-1.1.tar.gz\ntar xzf beast-library-1.1.tar.gz\nsudo mv beast-library-1.1 /opt/minc/1.9.18share/\n\n\nScript to run BEaST:\n#!/usr/bin/env bash\n# \n# BEaSTSkullStrip.sh\n# Using BEaST to do SkullStriping\n# [see here](https://github.com/FCP-INDI/C-PAC/wiki/Concise-Installation-Guide-for-BEaST) for instructions for BEaST.\n# \n# Qingyang Li\n# 2013-07-29\n\n# With major edits from Alex W\n#  \n# The script requires FSL, AFNI, BEaST, and MINC toolkit.\n\nSECONDS=0\n\nMincPATH='/opt/minc/1.9.18'\nsource $MincPATH/minc-toolkit-config.sh\n\nMincLibPATH=\"$MincPATH/share/beast-library-1.1/\"\n\nMNItemplatePATH=~/Atlas/\nMNI_DATAPATH=~/Atlas/\n\ncwd=$PWD\n\nif [ $# -lt 1  ]\nthen\n  echo \" USAGE ::  \"\n  echo \"  BEaSTSkullStrip.sh &lt;input&gt; [output prefix] \" \n  echo \"   input: anatomical image with skull, in nifti format \" \n  echo \"   output: The program will output two nifti files \" \n  echo \"      1) a skull stripped brain image; \"  \n  echo \"      2) a skull stripped brain mask. \"\n  echo \"   Option: output prefix: the filename of the output files without extention\"\n  echo \" Example: BEaSTSkullStrip.sh ~/data/head.nii.gz ~/brain \" \n  exit\nfi\n\nif [ $# -eq 1 ]\nthen\n  inputDir=$(dirname $1)\n  if [ $inputDir == \".\" ]; then\n    inputDir=$cwd\n  fi\n\n  filename=$(basename $1)\n  inputFile=$inputDir/$filename\n\n  extension=\"${filename##*.}\"\n  if [ $extension == \"gz\" ]; then\n    filename=\"${filename%.*}\"\n  fi\n  filename=\"${filename%.*}\"\n\n  outputDir=$inputDir\n  out=$inputDir/${filename}_brain\n\nelse\n  outputDir=$(dirname $2)\n\n  if [ $outputDir == \".\" ]; then\n    outputDir=$cwd\n    outfile=$(basename $2)\n    out=$outputDir/$outfile\n  else\n    mkdir -p $outputDir\n    out=$2\n  fi\n\n  inputDir=$(dirname $1)\n  filename=$(basename $1)\n  inputFile=$inputDir/$filename\n\n  extension=\"${filename##*.}\"\n  if [ $extension == \"gz\" ]; then\n    filename=\"${filename%.*}\"\n  fi\n  filename=\"${filename%.*}\"\n\nfi\n\necho \" ++ input directory is $inputDir\"\necho \" ++ input basename is $filename\"\necho \" ++ output directory is $outputDir\"\necho \" ++ output will be $out\"\ntmpdir=$(mktemp -d $outputDir/tmp.XXXXXXXXXX)\necho \" ++ working dir will be $tmpdir\"\n\ncd $tmpdir\n\nimcp ${inputFile} head\n\nheadfile=$(ls head.*)\necho $headfile\nheadextension=\"${headfile##*.}\"\nif [ $headextension == \"gz\" ]; then\n  gunzip $headfile\nfi\n\nnii2mnc head.nii head.mnc\n\n# Normalize the input\nbeast_normalize head.mnc head_mni.mnc anat2mni.xfm -modeldir $MNItemplatePATH\n\n# Run BEaST to do SkullStripping\n# configuration file can be replaced by $MincLibPATH/default.2mm.conf or $MincLibPATH/default.4mm.conf\n\nmincbeast -fill -median -conf $MincLibPATH/default.1mm.conf $MincLibPATH head_mni.mnc brain_mask_mni.mnc\n\n# Transform brain mask to it's original space\nmincresample -invert_transformation -like head.mnc -transformation anat2mni.xfm brain_mask_mni.mnc brain_mask.mnc\n\n# Convert image from MNC to NII format.\n\nmnc2nii brain_mask.mnc brain_mask_tmp.nii\n\n# Resample mask to original image\nflirt -in brain_mask_tmp.nii -ref head.nii -applyxfm -usesqform -out brain_mask\nfslmaths brain_mask -thr 0.5 -bin brain_mask\n\n# Generate and output brain image and brain mask\nfslmaths head.nii -mul brain_mask ${out}_brain\nimmv brain_mask ${out}_brain_mask\n\n# delete all intermediate files\ncd $cwd\nrm -rf $tmpdir\n\nduration=$SECONDS\necho \"$(($duration / 60)) minutes and $(($duration % 60)) seconds elapsed.\"\n\n\nGit Script\nhttps://github.com/WeberLab/BEaST",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#Anatomical-T1T2CombinationTechnique",
    "href": "mrimethods/Anatomical.html#Anatomical-T1T2CombinationTechnique",
    "title": "Anatomical",
    "section": "T1T2 Combination Technique",
    "text": "T1T2 Combination Technique\nThis idea comes from: https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.25560 :\nEssentially run your skullstripping algorithm on a combination image (CI) of T1w and scaled T2w (sT2w):\nCI = (T1w-sT2w)/(T1w+sT2w)\n\"The CI image values range from −1.0 to 1.0, with negative values in the CSF, positive values in the WM, and values near 0 in the GM. To facilitate visualization and image processing, the CI was scaled within the union mask to set the minimum at zero and the median at a value where the median intensity of all voxels equaled that of the T1w image.\"\n\"The scaling factor was calculated to adjust the graymatter (GM) voxel intensities in the T2w image so that their median value equaled that of the GM voxel intensities in the T1w image.\"\n\"The scaling factor was calculated as MG_T1/MG_T2, where MG_T1 and MG_T2 are the median values of GM voxels of T1w and T2w, respectively. The term sT2w designates the T2w image scaled by this value: sT2w = MG_T1/MG_T2 × T2w.\"\n\"The T2w image was aligned to the T1w image using align_epi_anat.py script in Analysis of Functional NeuroImages (AFNI)\"\n\nExample\nFirst I bias field corrected my images (see above)\n\nRegister T2 to T1\n\nflirt -in T2w_N4.nii.gz -ref T1w_N4.nii.gz -out T2-to-T1\n\nFind the scaling factor (median T1 of gm / median T2 of gm)\n\nI just guessed this by clicking around: 3.45\nThen multiply the T2 image by this factor:\nfslmaths T2-to-T1.nii.gz -mul 3.45 sT2w.nii.gz\n\nNow calculate the CI image:\n\nfslmaths T1w_N4.nii.gz -sub sT2w.nii.gz T1w-sT2w\nfslmaths T1w_N4.nii.gz -add sT2w.nii.gz T1w+sT2w\nfslmaths T1w-sT2w.nii.gz -div T1w+sT2w.nii.gz CI\n\nNext find the median value of all the T1w file\n\nfslstats T1w_N4.nii.gz -P 50\nIn my case it turned out to be: 388\n\nThen set the CI to be between 0 and 1, then multiply by that median number:\n\nfslmaths CI.nii.gz -add 1 -div 2 -mul 388 sCI\n\nNow apply bet?\n\nrobustfov -i sCI.nii.gz -r sCI_robust\nbet sCI_robust.nii.gz sCI_robust_brain -o -m -f 0.1 -B\n::::\nThis failed pretty remarkably…",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#robex",
    "href": "mrimethods/Anatomical.html#robex",
    "title": "Anatomical",
    "section": "ROBEX",
    "text": "ROBEX\nROBEX receives a volume as an input and provides the stripped output.\nOptional: Corresponding binary mask. However, the volume has to be in the same orientation as the provided reference volume.\nROBEX works with any ITK-friendly format e.g. HDR/images, nii, dcm\nOn LINUX: runROBEX.sh inputFile strippedFile [outputMaskFile]\nRead about ROBEX in Robust Brain Extraction Across Datasets and Comparison with Publicly Available Methods\nDownload",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#flabel",
    "href": "mrimethods/Anatomical.html#flabel",
    "title": "Anatomical",
    "section": "fLABEL",
    "text": "fLABEL\nSkullStrippingToolkit https://www.nitrc.org/projects/skulltoolkit\n\"A matlab-based code for skull stripping on infant and adult MR images.\"\n\nSetup\nCurrently installed on AdaL:\nAdd this to your ~/bashrc file:\nexport LABELDIR=/usr/local/skullStrippingToolkit\nexport PATH=${PATH}:${LABELDIR}/bin\nthen add this to your ~/matlab/startup.m file:\n%----------- add fLABEL for brain extraction ---------------%\nLABELDIR = '/usr/local/skullStrippingToolkit'\naddpath(genpath(LABELDIR));\n%-----------------------------------------------------%\nafter your source ~/.bashrc you should be good to go:\n\n\nTo Run:\n\nThe input image should be in nii format (better to perform bias correction first using tools such as N3).\nType in fLABEL and the program option will be listed.\nFollow the example to run the algorithm.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#neonates",
    "href": "mrimethods/Anatomical.html#neonates",
    "title": "Anatomical",
    "section": "Neonates",
    "text": "Neonates\nUnlike adults, using a T2w image for brain extraction may work best with a T2 image. Be careful, as a lot of brain extraction methods have been optimized on T1w images.\n\ndHCP Anatomical Pipeline\nInstallation:\nFirst step is make sure docker is installed and that your user is added to the docker usergroup\nInstallation and Troubleshooting for docker should be found here: fmri-prep\nNext step:\ndocker pull biomedia/dhcp-structural-pipeline:latest\nThis will take a while to download and install\nExample Script\n######################################################˚\n# Uses dHCP anatomical docker image to segment brains\n# from images in Raw5 folder\n# Step 1) N4 correction\n# Step 2) Register T1 to T2\n# Step 3) Run dHCP pipeline\n######################################################˚\n# change lines necessary: 17, 20, 39, 40, 41 && unnecessary: 23, 35, 47\n\n# based on below parameters this\n# 1) script path = /mnt/WeberLab/Projects/NeonateSucrose/SickKids/dhcp-anat.sh\n# 2) working directory = /home/johann.drayne/dhcptesting/\n# 3) output directory = /home/johann.drayne/dhcptesting/run-single/\n\n# change to the folder where you have this script saved\nhighDir=/mnt/WeberLab/Projects/NeonateSucrose/SickKids\n\n# change to where you want output folder to be\ncurDir=/home/johann.drayne/dhcptesting/\n\n# change after ${curDir} to change name of output folder\ndataDir=${curDir}run-single/\n\ncd ${curDir}\nmkdir ${dataDir}\n\n######################################################˚\n# rename for T1, T2 and age variables\n######################################################˚\n\nsubjectid=MS040054 # change this to whatever, dHCP likes using the subject name\n                   # in their scripts\n\nagescan=34                  # rename to the subjects age at scan\nogt1=${subject}/t1/*.nii.gz # rename to your t1 file\nogt2=${subject}/t2/*.nii.gz # rename to your t2 file\n\nbaset1=$(basename ${ogt1})\nbaset2=$(basename ${ogt2})\n\ntempDir=${curDir}temp-dhcp-working-${subjectid}/\nmkdir ${tempDir}\n\n######################################################˚\n# N4 correction\n######################################################˚\n\nN4t1=${tempDir}N4_${subjectid}t1.nii.gz\nN4t2=${tempDir}N4_${subjectid}t2.nii.gz\n\nN4BiasFieldCorrection -d 3 -i ${ogt1} -o ${N4t1}\nN4BiasFieldCorrection -d 3 -i ${ogt2} -o ${N4t2}\n\ncp -f  ${N4t2} ${dataDir}\nt1=${dataDir}N4_Warped${baset1}\nt2=${dataDir}N4_${baset2}\nmv ${dataDir}N4_${subjectid}t2.nii.gz ${t2}\n\ncd ${highDir}\n\n######################################################˚\n# Register T1 to T2\n######################################################˚\n\necho \"----- Starting Registration ${subjectid} -----\"\n\nantsRegistration --dimensionality 3 --float 0 \\\n        --output [${tempDir},${t1}] \\\n        --interpolation Linear \\\n        --winsorize-image-intensities [0.005,0.995] \\\n        --use-histogram-matching 0 \\\n        --initial-moving-transform [${N4t2},${N4t1},1] \\\n        --transform Rigid[0.08] \\\n        --metric MI[${N4t2},${N4t1},1,64,Regular,0.20] \\\n        --convergence [5000x2500x1000x500,1e-10,10] \\\n        --shrink-factors 8x4x2x1 \\\n        --smoothing-sigmas 0x0x0x0vox\n\necho \"----- Registration completed for ${subjectid} -----\"\n\nrm -r ${tempDir}\n\n#\n##    --verbose 1\n##\n##        --transform Affine[0.08] \\\n##        --metric MI[${N4t2},${N4t1},1,64,Regular,0.25] \\\n##        --convergence [1000x1000x500x200,1e-10,10] \\\n##        --shrink-factors 8x4x2x1 \\\n##        --smoothing-sigmas 3x2x1x0vox\n##\n##        --transform SyN[0.08,3,0] \\\n##        --metric MI[${N4t2},${N4t1},1,64,Regular,0.25] \\\n##        --convergence [300x2000x100x500,1e-10,10] \\\n##        --shrink-factors 8x4x2x1 \\\n##        --smoothing-sigmas 3x2x1x0vox \\\n##        --verbose 1\n\n######################################################˚\n# Run docker image\n######################################################˚\n\ndocker run --rm -t \\\n    -u $(id -u):$(id -g) \\\n    -v ${dataDir}:${dataDir} \\\n    -w ${dataDir} \\\n    biomedia/dhcp-structural-pipeline:latest ${subjectid} session1 ${agescan} -T1 N4_Warped${baset1} -T2 N4_${baset2} -t 16\n\nI have commented the lines that need edited. 5 lines are necessary, 3 lines don't need to be changed but probably better if you do.\nYou'll notice I also commented the Affine and SyN parts of antsRegistration, I found the Rigid transform was pretty decent. The Affine and SyN took very long, maybe this is an option if you are playing with the single subject for the moment or the rigid registration is not good enough.\nYou will also need to get the latest version from dHCP docker pull biomedia/dhcp-structural-pipeline:latest should do the trick\nIf you run this script on Pierce, you will need to change the -t flag on line 115 (to probably 6), this specifies the number of CPU cores\n\ndHCP anat output\nWhen you run the script, sometimes it will fail out before completely finishing. If you are only looking for the segmentations, this is ok as they are almost always completed.\nThis is generally how your output folder will look when running on multiple subjects.\noutput\n    - logs\n    - derivatives\n            - sub-${subid}\n                    - ses-session1\n                            - anat\n                                    - output images\n    - sourcedata\n    - workdir\n            - ${subid}-session1\n                    - bias    \n                    - dofs\n                    - logs\n                    - masks\n                    - N4\n                    - posteriors\n                    - restore\n                    - segmentations\n                    - segmentations-data\n                    - surfaces\n                    - T1\n                    - T2\nThe two most important folder are derivatives (subjects that have fully run) and workdir (subjects that failed in pipeline)\nHere is a rough guide as to what image is what (workdir 1st derivatives equivalent 2nd)\n\nT2 restored: restore/T2/${subid}-session1_restore.nii.gz && sub-${subid}_ses-session1_T2w_restore\nT2 skull strip: restore/T2/${subid}-session1_restore_brain.nii.gz && sub-${subid}_ses-session1_T2w_restore_brain\nT2 tissue segmented: segmentations/${subid}-${sesid}_all_labels.nii.gz && sub-${subid}_ses-session1_drawem_all_labels\n\nMore detail into how the segmentations are indexed\nWhen writing a pipeline for picking these images a nice bash way (no if statement needed) to do it is.\nworkingt2=${dhcpanat}workdir/${subid}-${sesid}/restore/T2/${subid}-${sesid}_restore\nderivt2=${dhcpanat}derivatives/sub-${subid}/ses-${sesid}/anat/sub-${subid}_ses-${sesid}_T2w_restore\n[[ -f ${workingt2}.nii.gz ]] && t2=${workingt2} || t2=${derivt2}\n\nThe first two lines are paths to the image in the derivatives and workdir folder as we don't know.\nThe third line asks if the image exists in the workdir folder, if it does then the variable t2 equals the path to the workdir folder\nIf [[ -f ${workingt2}.nii.gz ]] is false then the variable t2 will equal the path to the derivatives folder",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#ibeat",
    "href": "mrimethods/Anatomical.html#ibeat",
    "title": "Anatomical",
    "section": "iBEAT",
    "text": "iBEAT\nhttps://ibeat.wildapricot.org/\nInfant Brain Extraction and Analysis Toolbox",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#fsl_anat",
    "href": "mrimethods/Anatomical.html#fsl_anat",
    "title": "Anatomical",
    "section": "FSL_ANAT",
    "text": "FSL_ANAT\nfsl_anat -i T1.nii.gz -o output/location\nExample outputs:\nlesionmaskinv.nii.gz T1_biascorr.nii.gz T1_fast_totbias.nii.gz T1_nonroi2roi.mat T1_to_MNI_lin.mat\nlesionmask.nii.gz T1_fast_bias_idxmask.nii.gz T1_fullfov.nii.gz T1_orig2roi.mat T1_to_MNI_lin.nii.gz\nlog.txt T1_fast_bias_init.nii.gz T1_initfast2_brain_mask2.nii.gz T1_orig2std.mat T1_to_MNI_nonlin_coeff.nii.gz\nMNI152_T1_2mm_brain_mask_dil1.nii.gz T1_fast_bias.nii.gz T1_initfast2_brain_mask.nii.gz T1_orig.nii.gz T1_to_MNI_nonlin_field.nii.gz\nMNI_to_T1_nonlin_field.nii.gz T1_fast_bias_vol2.nii.gz T1_initfast2_brain.nii.gz T1_roi2nonroi.mat T1_to_MNI_nonlin_jac.nii.gz\nT1_biascorr_brain_mask.nii.gz T1_fast_bias_vol32.nii.gz T1_initfast2_maskedrestore.nii.gz T1_roi2orig.mat T1_to_MNI_nonlin.nii.gz\nT1_biascorr_brain.nii.gz T1_fast_restore.nii.gz T1_initfast2_restore.nii.gz T1_roi.log T1_to_MNI_nonlin.txt\nT1_biascorr_maskedbrain.nii.gz T1_fast_seg.nii.gz T1.nii.gz T1_std2orig.mat\nSegmentation index:\n3- White Matter\n2- Grey Matter\n1- CSF",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#fmriprep",
    "href": "mrimethods/Anatomical.html#fmriprep",
    "title": "Anatomical",
    "section": "fmriprep",
    "text": "fmriprep\nfmriprep will run freesurfer on the anatomical data",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#freesurfer",
    "href": "mrimethods/Anatomical.html#freesurfer",
    "title": "Anatomical",
    "section": "Freesurfer",
    "text": "Freesurfer\nTo run freesurfer yourself and get cortical segmentation (note: takes HOURS):\nrecon-all -i T1w_robust.nii.gz -s &lt;subid&gt; -sd ${PWD} -all\nTo run freesurfer and just get a simple segmentation:\nrecon-all -i T1w_robust.nii.gz -s &lt;subid&gt; -sd ${PWD} -autorecon1 -autorecon2\nAfter this, you will need to convert to nifti, and reorient:\nmri_convert aseg.auto_noCCseg.mgz aseg.auto_noCCseg.nii.gz\nfslreorient2std aseg.auto_noCCseg.nii.gz aseg.auto_noCCseg_r2std\nflirt -in aseg.auto_noCCseg_r2std.nii.gz -ref T1w_robust.nii.gz -out segment -applyxfm -usesqform\nHere are the tissue labels:\n2 Left_Cerebral_White_Matter\n3 Left_Cerebral_Cortex\n4 Left_Lateral_Ventricle\n5 Left_Inf_Lat_Vent\n7 Left_Cerebellum_White_Matter\n8 Left_Cerebellum_Cortex\n10 Left_Thalamus\n11 Left_Caudate\n12 Left_Putamen\n13 Left_Pallidum\n14 Third_Ventricle\n15 Fourth_Ventricle\n16 Brain_Stem\n17 Left_Hippocampus\n18 Left_Amygdala\n24 CSF\n26 Left_Accumbens_area\n28 Left_VentralDC\n30 Left-vessel\n31 Left-choroid-plexus\n41 Right_Cerebral_White_Matter\n42 Right_Cerebral_Cortex\n43 Right_Lateral_Ventricle\n44 Right_Inf_Lat_Vent\n46 Right_Cerebellum_White_Matter\n47 Right_Cerebellum_Cortex\n49 Right_Thalamus\n50 Right_Caudate\n51 Right_Putamen\n52 Right_Pallidum\n53 Right_Hippocampus\n54 Right_Amygdala\n58 Right_Accumbens_area\n60 Right_VentralDC\n62 Right-vessel\n63 Right-choroid-plexus\n77 WM_hypointensities\n85 Optic-Chiasm",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#volbrain-online-tool",
    "href": "mrimethods/Anatomical.html#volbrain-online-tool",
    "title": "Anatomical",
    "section": "VolBrain (online tool)",
    "text": "VolBrain (online tool)\nhttps://volbrain.net/services",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/Anatomical.html#edit-header-info-on-nifti-files",
    "href": "mrimethods/Anatomical.html#edit-header-info-on-nifti-files",
    "title": "Anatomical",
    "section": "Edit Header Info on Nifti Files",
    "text": "Edit Header Info on Nifti Files\nfsledithd - allows the header information in and image to be edited in a text-based xml-style format (like the output of fslhd -x but with redundant fields removed and some help text provided). Note that the default text editor used is nano, but other editors can be specified by the second argument.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Anatomical"
    ]
  },
  {
    "objectID": "mrimethods/acpc-align.html",
    "href": "mrimethods/acpc-align.html",
    "title": "Align AC PC to horizontal",
    "section": "",
    "text": "Align to the Anterior Commissure-Posterior Commisure Line\nIf you need help determining where the Anterior and Posterior Commisures are in your image, please see Commissural Pathways, available online, or Catani & Thiebaut de Schotten (2012), which is available from the UBC Library.\n\nDownload the acpc_align script from here. Be sure the code is in the folder where your participant’s structural scans are.\nMake the acpc_align.sh script executable.\n\nchmod 755 acpc_align.sh\n\nOpen your structural scan in your faviourite viewer. Here we are using FSLeyes.\n\n\n\n\nFSLeyes\n\n\n\nGet the X, Y and Z coordinates of the anterior commissure in voxel space by placing your cursor on the anterior commissure. The anterior commissure is located in the anterior wall of the third ventricle. It runs transversely anterior to the anterior columns of the fornix, above the basal forebrain and beneath the medial and ventral aspect of the anterior limb of the internal capsule. Save the X, Y, and Z coordinates.\n\n\n\n\nAnterior Commissure\n\n\n\nGet the X, Y, and Z coordinates of the posterior commissure in voxel space by placing your cursor on the posterior commissure. The posterior commissure is the inferior lamina or stalk of the pineal gland. Save the X, Y, and Z coordinates.\n\n\n\n\nPosterior Commissure\n\n\n\nIn your terminal (MacOS or Linux) run\n\nacpc_align subjectbrain.nii.gz -a x y z -p x y z\nto produce the realigned image for subject-specific to acpc alignment.\nIn the example, this is\nsh acpc_align.sh T1_reoriented.nii.gz -a 93 130 153 -p 93 98 152\n\n\n\nacpc aligned image\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Align AC PC to horizontal"
    ]
  },
  {
    "objectID": "mrimethods/ASL.html",
    "href": "mrimethods/ASL.html",
    "title": "Arterial Spin Labelling",
    "section": "",
    "text": "Arterial Spin Labelling can be used to calculate the cerebral blood flow (CBF) in the brain: \"perfusion MRI permits noninvasive quantification of blood flow, which is an important physiological parameter\"\nEssentially it 'tags' arterial blood and then measures it in the brain as it perfuses through (very neat)\nThis is a good resource:\nwww.ncbi.nlm.nih.gov/pmc/articles/PMC4190138/\nAs well as this:\nmri-q.com/quantifying-flow.html",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Arterial Spin Labelling"
    ]
  },
  {
    "objectID": "mrimethods/ASL.html#arterial-spin-labelling",
    "href": "mrimethods/ASL.html#arterial-spin-labelling",
    "title": "Arterial Spin Labelling",
    "section": "",
    "text": "Arterial Spin Labelling can be used to calculate the cerebral blood flow (CBF) in the brain: \"perfusion MRI permits noninvasive quantification of blood flow, which is an important physiological parameter\"\nEssentially it 'tags' arterial blood and then measures it in the brain as it perfuses through (very neat)\nThis is a good resource:\nwww.ncbi.nlm.nih.gov/pmc/articles/PMC4190138/\nAs well as this:\nmri-q.com/quantifying-flow.html",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Arterial Spin Labelling"
    ]
  },
  {
    "objectID": "mrimethods/ASL.html#install-w-docker",
    "href": "mrimethods/ASL.html#install-w-docker",
    "title": "Arterial Spin Labelling",
    "section": "Install w Docker",
    "text": "Install w Docker\nIt is recommended to use a docker or singularity\ndocker pull pennlinc/aslprep:latest",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Arterial Spin Labelling"
    ]
  },
  {
    "objectID": "mrimethods/ASL.html#run-w-docker",
    "href": "mrimethods/ASL.html#run-w-docker",
    "title": "Arterial Spin Labelling",
    "section": "Run w Docker",
    "text": "Run w Docker\nExample:\ndocker run -ti -m 12GB --rm \\\n    -v $HOME/ds000240:/data:ro \\\n    -v $HOME/ds000240-results:/out:rw \\\n    -v $HOME/tmp/ds000240-workdir:/work \\\n    -v ${FREESURFER_HOME}:/fs \\\n    pennlinc/aslprep:&lt;latest-version&gt; \\\n    /data /out/aslprep-&lt;latest-version&gt; \\\n    participant \\\n    --participant-label '01'\n    --fs-license-file ${FREESURFER_HOME}/license.txt\n    -w /work\nExplanation:\ndocker run -ti -m 12GB --rm                         # attach to the container interactively\n-v $HOME/license.txt:/license/license.txt           # mount the freesurfer license directory\n-v $HOME/ds000240:/data:ro                          # mount the data directory to the container directory\n-v $HOME/ds000240-results:/out:rw                   # mount the output directory to the container directory\n-v $HOME/tmp/ds000240-workdir:/work                 # mount working directory\npennlinc/aslprep                                    # the container name: aslprep\n/data                                               # the data directory\n/out/aslprep                                        # the output directory\nparticipant                                         # analysis type: participant\n--participant-label 01                              # select participant 01\n--fs-license-file /license/license.txt              # setting freesurfer license file\n-w /work                                            # setting working directory\nCommand-Line Arguments:\nusage: aslprep [-h] [--version] [--skip_bids_validation]\n               [--participant-label PARTICIPANT_LABEL [PARTICIPANT_LABEL ...]]\n               [--echo-idx ECHO_IDX] [--bids-filter-file FILE]\n               [--anat-derivatives PATH] [--nprocs NPROCS]\n               [--omp-nthreads OMP_NTHREADS] [--mem MEMORY_GB] [--low-mem]\n               [--use-plugin FILE] [--anat-only] [--boilerplate_only]\n               [--md-only-boilerplate] [-v]\n               [--ignore {fieldmaps,slicetiming,sbref} [{fieldmaps,slicetiming,sbref} ...]]\n               [--longitudinal]\n               [--output-spaces [OUTPUT_SPACES [OUTPUT_SPACES ...]]]\n               [--asl2t1w-init {register,header}] [--asl2t1w-dof {6,9,12}]\n               [--force-bbr] [--force-no-bbr] [--m0_scale M0_SCALE]\n               [--random-seed RANDOM_SEED] [--dummy-vols DUMMY_VOLS]\n               [--smooth_kernel SMOOTH_KERNEL] [--scorescrub] [--basil]\n               [--skull-strip-template SKULL_STRIP_TEMPLATE]\n               [--skull-strip-fixed-seed]\n               [--skull-strip-t1w {auto,skip,force}] [--fmap-bspline]\n               [--fmap-no-demean] [--use-syn-sdc] [--force-syn]\n               [--fs-license-file FILE] [-w WORK_DIR] [--clean-workdir]\n               [--resource-monitor] [--reports-only] [--run-uuid RUN_UUID]\n               [--write-graph] [--stop-on-first-crash] [--notrack] [--sloppy]\n               bids_dir output_dir {participant}",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Arterial Spin Labelling"
    ]
  },
  {
    "objectID": "mrimethods/ASL.html#instructions",
    "href": "mrimethods/ASL.html#instructions",
    "title": "Arterial Spin Labelling",
    "section": "Instructions:",
    "text": "Instructions:\nInstructions I wrote: instructions",
    "crumbs": [
      "Home",
      "MRI Methods",
      "Arterial Spin Labelling"
    ]
  },
  {
    "objectID": "tutorials/Python.html#scripts-vs.-modules",
    "href": "tutorials/Python.html#scripts-vs.-modules",
    "title": "Python",
    "section": "Scripts vs. Modules",
    "text": "Scripts vs. Modules\nIt's also important to know the difference between a Script and a Module:\nPython Scripts and Modules\nand\nCode Reuse: Functions and Modules\nA plain text .py file containing Python code that is intended to be directly executed by the user is usually called a script\nA plain text .py file, which contains Python code that is designed to be imported and used from another Python file, is called a module\n\nHow to write a simple script:\nOpen a text file and save it as a filename with the suffix .py:\nOne good program to use for this is Sublime Text. Check it out.\nOr better yet, Visual Studio Code\nOr in terminal:\nnano filename.py\n[note: nano is a built in text editor you can run from the command line. It's very handy]\nAt the top, it should say: #!/usr/bin/env python3\nthen your code. Here's an example:\n#!/usr/bin/env python3\n\nprint('Hello World!')\nAfter, you will have to assign execution permissions to your file:\nchmod +x filename.py\nYou can run the script from the directory with './' before the filename:\n./hello.py\nYou can also store this file in a Script folder, which you can tell Python to search in for scripts:\nexport PATH=\"$PATH:/usr/local/bin/python\"\nwhere /usr/local/bin/python is replaced with the path to the folder you want Python to look for your scripts\nAfter you set the path, you don't need to be in the directory where your script is, and you don't need to include the './' before the script name. You can just type the script name from any directory and it will run it:\nfilename.py\n\n\nHow to write a simple module:\nOpen a text file and write some simple code:\ndef greeting(name):\n  print(\"Hello, \" + name)\nSave this file as something like mymodule.py\nPlace this file in one of the Python paths (see above about scripts)\nTo see what paths Python is searching, start a python environment and type:\nimport sys\nsys.path\nNow import the module in python:\nimport mymodule\nThen run it:\nmymodule.greeting(\"Jonathan\")",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#install-upgrade-packages",
    "href": "tutorials/Python.html#install-upgrade-packages",
    "title": "Python",
    "section": "Install / Upgrade Packages",
    "text": "Install / Upgrade Packages\npip install &lt;package_name&gt;\npip install &lt;package_name&gt; --upgrade",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#troubleshooting",
    "href": "tutorials/Python.html#troubleshooting",
    "title": "Python",
    "section": "Troubleshooting",
    "text": "Troubleshooting\ninstead of pip, you might have to call it this way:\npython3 -m pip",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#numpy",
    "href": "tutorials/Python.html#numpy",
    "title": "Python",
    "section": "numpy",
    "text": "numpy\nnumpy is for numbers\nimport numpy as np",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#pandas",
    "href": "tutorials/Python.html#pandas",
    "title": "Python",
    "section": "pandas",
    "text": "pandas\npandas is for dataframes\nimport pandas as pd",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#scipy",
    "href": "tutorials/Python.html#scipy",
    "title": "Python",
    "section": "scipy",
    "text": "scipy\nSciPy provides algorithms for optimization, integration, interpolation, eigenvalue problems, algebraic equations, differential equations, statistics and many other classes of problems.",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#matplotlib",
    "href": "tutorials/Python.html#matplotlib",
    "title": "Python",
    "section": "matplotlib",
    "text": "matplotlib\nhttps://matplotlib.org/\nMatplotlib is for visualizations\n\nExample\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()             # Create a figure containing a single Axes.\nax.plot([1, 2, 3, 4], [1, 4, 2, 3])  # Plot some data on the Axes.\nplt.show()                           # Show the figure.\n\n\n\nTransparent color\nhttps://stackoverflow.com/questions/62453018/matplotlib-colourmap-from-transparent\nhttps://stackoverflow.com/questions/22669704/correct-way-to-set-color-to-transparent-with-matplotlib-pcolormesh",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#plotly",
    "href": "tutorials/Python.html#plotly",
    "title": "Python",
    "section": "plotly",
    "text": "plotly\nPlotly’s Python graphing library makes interactive, publication-quality graphs.\nplotly.com/python/",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#jupyter",
    "href": "tutorials/Python.html#jupyter",
    "title": "Python",
    "section": "jupyter",
    "text": "jupyter\nThe Jupyter Notebook is the original web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience.\njupyter.org/\npip install jupyter installs the Jupyter Notebook, JupyterLab, and the IPython Kernel",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#nilearn",
    "href": "tutorials/Python.html#nilearn",
    "title": "Python",
    "section": "nilearn",
    "text": "nilearn\nNilearn enables approachable and versatile analyses of brain volumes. It provides statistical and machine-learning tools, with instructive documentation & open community.\nnilearn.github.io/stable/index.html",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#nibabel",
    "href": "tutorials/Python.html#nibabel",
    "title": "Python",
    "section": "nibabel",
    "text": "nibabel\nhttps://nipy.org/nibabel/\n\nRead and write access to common neuroimaging file formats, including: ANALYZE (plain, SPM99, SPM2 and later), GIFTI, NIfTI1, NIfTI2, CIFTI-2, MINC1, MINC2, AFNI BRIK/HEAD, ECAT and Philips PAR/REC. In addition, NiBabel also supports FreeSurfer’s MGH, geometry, annotation and morphometry files, and provides some limited support for DICOM.\nNiBabel’s API gives full or selective access to header information (metadata), and image data is made available via NumPy arrays. For more information, see NiBabel’s documentation site and API reference.\n\nimport nibabel as nib",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#seaborn",
    "href": "tutorials/Python.html#seaborn",
    "title": "Python",
    "section": "seaborn",
    "text": "seaborn\nseaborn is a high level interface for drawing statistical graphics with Matplotlib. It aims to make visualization a central part of exploring and understanding complex datasets.\nseaborn.pydata.org/",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#nibabel-1",
    "href": "tutorials/Python.html#nibabel-1",
    "title": "Python",
    "section": "nibabel",
    "text": "nibabel\nRead and write access to common neuroimaging file formats\ngithub.com/nipy/nibabel",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#nipy",
    "href": "tutorials/Python.html#nipy",
    "title": "Python",
    "section": "nipy",
    "text": "nipy\nThe aim of NIPY is to produce a platform-independent Python environment for the analysis of functional brain imaging data using an open development model.\ngithub.com/nipy/nipy",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#nipype",
    "href": "tutorials/Python.html#nipype",
    "title": "Python",
    "section": "Nipype",
    "text": "Nipype\nNipype, an open-source, community-developed initiative under the umbrella of NiPy, is a Python project that provides a uniform interface to existing neuroimaging software and facilitates interaction between these packages within a single workflow. Nipype provides an environment that encourages interactive exploration of algorithms from different packages (e.g., SPM, FSL, FreeSurfer, AFNI, Slicer, ANTS), eases the design of workflows within and between packages, and reduces the learning curve necessary to use different packages.\ngithub.com/nipy/nipype",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#using-poetry",
    "href": "tutorials/Python.html#using-poetry",
    "title": "Python",
    "section": "Using poetry",
    "text": "Using poetry\nhttps://python-poetry.org/\n\nExample\npoetry init # press enter for defaults or make changes\npoetry add numpy # libraries you want to use\nYou should now have a file called pyproject.toml that looks like this:\n[tool.poetry]\nname = \"poetry\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"\"]\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\nnumpy = \"^1.21.1\"\n\n[build-system]\nrequires = [\"poetry-core&gt;=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\nIf you get an error when trying to install a package that says something like:\n\nThe current project's supported Python range (&gt;=3.10,&lt;4.0) is not compatible with some of the required packages Python requirement:\n\nstochastic requires Python &gt;=3.8,&lt;3.11, so it will not be satisfied for Python &gt;=3.11,&lt;4.0\n\nBecause no versions of stochastic match &gt;0.7.0,&lt;0.8.0\nand stochastic (0.7.0) requires Python &gt;=3.8,&lt;3.11, stochastic is forbidden.\nSo, because pythonncctoolbox depends on stochastic (^0.7.0), version solving failed.\n\nCheck your dependencies Python requirement: The Python requirement can be specified via the python or markers properties\n\nFor stochastic, a possible solution would be to set the `python` property to \"&gt;=3.10,&lt;3.11\"\n\nTry editing the pyproject.toml file so it reads:\n[tool.poetry.dependencies]\npython = \"&gt;=3.10,&lt;3.11\"\nNow type\npoetry update\nThen try to install your package again:\npoetry add stochastic\n\n\nJupyter\nhttps://hippocampus-garden.com/jupyter_poetry_pipenv/\npoetry add -D jupyter # libraries for development use only\nnow just run\npoetry run jupyter notebook",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#using-virtualenv",
    "href": "tutorials/Python.html#using-virtualenv",
    "title": "Python",
    "section": "Using virtualenv",
    "text": "Using virtualenv\nFirst make sure virtualenv is installed:\npip install virtualenv\nNext create a virtual environment\nvirtualenv .venv -p python\nwhere .venv is what I’ve called my virtual environment (and it will place it in the current directory I’m in); and -p python tells the computer which python to use (it will be whatever is which python\nNow activate this environment:\nsource .venv/bin/activate\nNow when you type which python it should show your venv path\nAlso: which pip should do the same\nTo deactivate:\ndeactivate\nInclude in jupyter notebook:\npython -m ipykernel install --user --name=yourvenvname",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#install",
    "href": "tutorials/Python.html#install",
    "title": "Python",
    "section": "Install",
    "text": "Install\ncd ~/Downloads\nwget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -p &lt;dir&gt;\nwhere &lt;dir&gt; is where you want to install (e.g. $HOME/miniconda3)\nthen\nsource &lt;dir&gt;/bin/activate\nUpdate\nconda update conda -y",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#conda-environments",
    "href": "tutorials/Python.html#conda-environments",
    "title": "Python",
    "section": "Conda Environments",
    "text": "Conda Environments\n\nUsing environments are highly recommended as a best practice for running scripts.\nConda environments are easier set-up than pip environments\nConda checks to makes sure all dependancies are met at all times, where as pip does not.\n\nExample\nconda create --name my-python38environment python=3.8\nTo activate:\nconda activate my-python38environment\nTo deactivate:\nconda deactivate\n\nBest practices\nTo list the conda environments:\nconda info --envs\nTo install a specific package into a specific environment\nconda install -n &lt;env_name&gt; &lt;package&gt;\ne.g. conda install -n first_r_python_env numpy\nSometimes you will have to specify a specific channel (similar idea to a repository) as not all packages are in the default conda channel.\nTry to install from the default channel or 'channel-forge' (a community managed channel) as these are the best managed channels.\nconda install -c conda-forge -n &lt;env_name&gt; &lt;package&gt;\nA list of the more common packages and where I found them.\n\nconda install -n &lt;env_name&gt; numpy\nconda install -n &lt;env_name&gt; pandas\nconda install -n &lt;env_name&gt; matplotlib\nconda install -n &lt;env_name&gt; seaborn\nconda install -c conda-forge -n &lt;env_name&gt; rpy2\nconda install -c conda-forge -n &lt;env_name&gt; statsmodels\nconda install -c conda-forge -n &lt;env_name&gt; nilearn\nconda install -c intel -n &lt;env_name&gt; scikit-learn\n\nYour .conda directory may get very large if you install multiple packages and create many virtual Conda environments. Make sure to clean the Conda cache and clean unused packages with:\nconda clean --all\nClean unused Conda environments by first listing the environments with:\nconda env list\n, and then removing unused ones:\nconda env remove --name &lt;yourenvironmentname&gt;\nYou can build Conda environments in different locations to save space on your home directory. You can use the —prefix flag when building your environment. For example:\nconda create myenv --prefix=/work/&lt;mygroup&gt;/&lt;mydirectory&gt;",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#troubleshooting-1",
    "href": "tutorials/Python.html#troubleshooting-1",
    "title": "Python",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you'd prefer that conda's base environment not be activated on startup, set the auto_activate_base parameter to false:\nconda config --set auto_activate_base false",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#nipype-1",
    "href": "tutorials/Python.html#nipype-1",
    "title": "Python",
    "section": "NiPype",
    "text": "NiPype\nNiPype stands for Neuroimaging in Pythong Pipelines and Interfaces (link)\n\nFSL\nFSL can be run from NiPype\nimport nipype.interfaces.fsl as fsl\nHead here for documentation\nExample:\nfrom nipype.interfaces.fsl import Merge\nmerger = Merge()\nmerger.inputs.in_files = ['functional2.nii', 'functional3.nii']\nmerger.inputs.dimension = 't'\nmerger.inputs.output_type = 'NIFTI_GZ'\nmerger.cmdline\n'fslmerge -t functional2_merged.nii.gz functional2.nii functional3.nii'\nmerger.inputs.tr = 2.25\nmerger.cmdline\n'fslmerge -tr functional2_merged.nii.gz functional2.nii functional3.nii 2.25'",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#curve-and-surface-fits",
    "href": "tutorials/Python.html#curve-and-surface-fits",
    "title": "Python",
    "section": "Curve and Surface fits",
    "text": "Curve and Surface fits\nAdding a line or curve of best fit is a quick way of visualising the relationship between data.\nWe have to be mindful of the different ways of fitting data; you can underfit but also overfit.\nPackages to import\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.optimize import curve_fit\nimport pickle\nfrom scipy.stats import spearmanr\nfrom scipy.stats import pearsonr\nimport matplotlib.animation as animation",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#d-function-fit",
    "href": "tutorials/Python.html#d-function-fit",
    "title": "Python",
    "section": "2d function fit",
    "text": "2d function fit\nYou can plot a straight line fit quite easily with modules such as seaborn, you can even fit data with higher order polynomials\nBut for custom control over the fitting function I have provided a basic example script.\nNOTE: It will also produce a residual plot which is good visual way to check the 'goodness' of the fit.\ndef 2d_fit_residual(data, x_name, y_name):\n    \"\"\"\n    2d_fit_residual outputs a curve fitted plot of the input data points based on theoretical fitting function fit_function()\n    it also outputs a residual graph.\n\n    :data: np.array x,y data inputs expected to be in format [[x1, y1], [x2, y2], [x3, y3], ...]\n    :x_name: str x-axis variable name\n    :y_name: str y-axis variable name\n    :return: curve fitted plot and residual graph\n    \"\"\" \n\n    X = data[:,0]\n    Y = data[:,1]\n\n    ######################################################################\n    # custom function. If you change the number of parameters, you will \n    # need to update the guesses variable and any calls to fit_function()\n    ######################################################################\n    def fit_function(X_var, Y1, Y2, C):\n        return Y1*X_var**2 + Y2*X_var + C\n\n    ######################################################################\n    # curve fit part\n    ######################################################################\n    guesses = (1, 1, 1)\n    params, pcov = curve_fit(fit_function, X, Y, guesses, maxfev=10**8)\n\n    ######################################################################\n    # data scatter graph plot\n    ######################################################################\n    fig = plt.figure()\n    ax = plt.axes()\n    plt.title('{} vs. {}'.format(y_name, x_name))\n    plt.xlabel('{}'.format(x_name))\n    plt.ylabel('{}'.format(y_name))\n    ax.scatter(X, Y, c='green')\n\n    ######################################################################\n    # correlation coefficients. Pearsons assumes linear, Spearmans does not\n    ######################################################################\n    corr, _ = spearmanr(X, Y)\n    legs = 'Spearmans correlation: %.3f' % corr\n    # corrlin, _ = pearsonr(X, Y)\n    # legslin = 'Pearsons correlation: %.3f' % corr\n\n    ######################################################################\n    # plot fit onto graph\n    ######################################################################\n    xtheory = np.linspace(min(X), max(X), 10000)\n    z_theory = fit_function(xtheory, params[0], params[1], params[2])\n    ax.plot(xtheory, z_theory, label = legs)\n    ax.legend(loc = 'best')\n    plt.show()\n\n    ######################################################################\n    # residual to check fit\n    ######################################################################\n    y_theory = fit_function(np.array((X)), params[0], params[1], params[2])\n    y_diff = y_theory - Y\n    fig = plt.figure()\n    ax = plt.axes()\n    ax.scatter(X, y_diff, c='red', marker='x')\n    ax.plot(np.array((min(X), max(X))), np.array((0,0)))\n    plt.title('Residual graph of difference between model and data')\n    plt.xlabel('{}'.format(x_name))\n    plt.ylabel('Measureed - Theoretical {}'.format(y_name))\n    plt.show()\nThe reason for the odd input data numpy shape is because of pandas data frames. To create a numpy array for this function from your data frame\n2d_fit_input = df[['x_var', 'y_var']].to_numpy()",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#d-function-fit-1",
    "href": "tutorials/Python.html#d-function-fit-1",
    "title": "Python",
    "section": "3d function fit",
    "text": "3d function fit\ndef scatter_3d(data, Z, x_name, y_name, z_name):\n\n    \"\"\"\n    scatter_3d outputs a 3d surface fitted plot of the input data points based \n    on theoretical fitting function fit_function()\n    Will also retrun a pickle of the 3d plot in the directory you run this\n    script from. \n\n    :data: np.array x,y data inputs expected to be in format [[x1, y1], [x2, y2], [x3, y3], ...]\n    :Z: np.array z data inputs expected to be in format [z1, z2, z3, ...]\n    :x_name: str x-axis variable name\n    :y_name: str y-axis variable name\n    :z_name: str z-axis variable name\n    :return: surface fitted plot and a pickle of the surface fitted plot \n    \"\"\" \n\n    X = data[:,0]\n    Y = data[:,1]\n    def fit_function(data, X1, X2, X3, Y1, Y2, Y3, XY, C1, C2):\n        x = data[:,0]\n        y = data[:,1]\n        return -np.sqrt(X2*x + Y2*y + C1) + np.sqrt(X2*x + Y2*y + C1-(X1*x**2 + Y1*y**2 + XY*x*y + X3*x + Y3*y + C2))\n\n    ######################################################################\n    # curve fit part\n    ######################################################################\n    guesses = (-0.5,  100,  100,  -0.5, 1, 1, 0.0001, 100, -100)\n    params, pcov = curve_fit(fit_function, data, Z, guesses, maxfev=10**8)\n    print('Params = {}'.format(params))\n\n    ######################################################################\n    # 3d scatter graph plot\n    ######################################################################\n    fig = plt.figure()\n    ax = plt.axes(projection = '3d')\n    ax.set_title('{} vs. {} vs. {}'.format(z_name, y_name, x_name))\n    ax.set_xlabel('{}'.format(x_name))\n    ax.set_ylabel('{}'.format(y_name))\n    ax.set_zlabel('{}'.format(z_name))\n    ax.scatter(X, Y, Z, c='red', marker='x', label='data points')\n    ax.legend(loc='upper left')\n\n    ######################################################################\n    # plot fit onto 3d scatter\n    ######################################################################\n    X1 = params[0]\n    X2 = params[1]\n    X3 = params[2]\n    Y1 = params[3]\n    Y2 = params[4]\n    Y3 = params[5]\n    XY = params[6]\n    C1 = params[7]\n    C2 = params[8]\n\n    xtheory = np.linspace(1.05*min(X), max(X), 4000)\n    ytheory = np.linspace(1*min(Y), max(Y), 4000)\n    x_grid, y_grid = np.meshgrid(xtheory, ytheory)\n\n    x = x_grid\n    y = y_grid\n    z_grid = -np.sqrt(X2*x + Y2*y + C1) + np.sqrt(X2*x + Y2*y + C1-(X1*x**2 + Y1*y**2 + XY*x*y + X3*x + Y3*y + C2))\n\n    ax.plot_surface(x_grid, y_grid, z_grid)\n    ax.set_xlim(min(X), max(X))\n    ax.set_ylim(min(Y), max(Y))\n    ax.set_zlim(min(Z), max(Z))\n\n    #Change name if you want the pickle saved in a different folder\n    pickle.dump(fig, open('FigureObject.fig.pickle', 'wb'))\n\n    def rotate(angle):\n        ax.view_init(azim=angle)\n\n    #making an animation\n    rot_animation = animation.FuncAnimation(fig, rotate, frames=np.arange(0, 362, 2), interval=100)\n    rot_animation.save('rotation.gif', dpi=80, writer='imagemagick')\n\n    plt.show()\nSaving the 3d graph in a Pickle allows you send this by email and the receiver can open it and manipulate it vs. sending a stationary 2d plot.\nTo open a pickle\nimport pickle\npath='/Users/johanndrayne/Documents/Python/FigureObject.fig.pickle'\nfigx = pickle.load(open(path, 'rb'))\nfigx.show()",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "tutorials/Python.html#read-matlab-.mat-files",
    "href": "tutorials/Python.html#read-matlab-.mat-files",
    "title": "Python",
    "section": "Read Matlab .mat files:",
    "text": "Read Matlab .mat files:\nsee this thread: https://stackoverflow.com/questions/874461/read-mat-files-in-python\nimport scipy.io\nmat = scipy.io.loadmat('file.mat')\nor\nimport numpy as np\nimport h5py\nf = h5py.File('somefile.mat','r')\ndata = f.get('data/variable1')\ndata = np.array(data) # For converting to a NumPy array",
    "crumbs": [
      "Home",
      "Tutorials",
      "Python"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html",
    "href": "mrimethods/DTI.html",
    "title": "DTI",
    "section": "",
    "text": "Diffusion imaging is a magnetic resonance imaging (MRI) technique that measures the movement of water molecules in tissues, providing insights into tissue microstructure and organization.\n\n\n\nDiffusion-weighted MRI (DWI):\nThis type of MRI uses magnetic field gradients to create an image that is sensitive to the diffusion of water molecules.\nDiffusion Tensor Imaging (DTI):\nDTI builds upon DWI by measuring the direction and magnitude of water diffusion, allowing for the visualization of neural pathways and fiber tracts. Makes some assumptions / approximations that may not hold true.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#dwi-vs-dti",
    "href": "mrimethods/DTI.html#dwi-vs-dti",
    "title": "DTI",
    "section": "",
    "text": "Diffusion-weighted MRI (DWI):\nThis type of MRI uses magnetic field gradients to create an image that is sensitive to the diffusion of water molecules.\nDiffusion Tensor Imaging (DTI):\nDTI builds upon DWI by measuring the direction and magnitude of water diffusion, allowing for the visualization of neural pathways and fiber tracts. Makes some assumptions / approximations that may not hold true.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#DTI-AMICO",
    "href": "mrimethods/DTI.html#DTI-AMICO",
    "title": "DTI",
    "section": "AMICO",
    "text": "AMICO\nhttps://github.com/daducci/AMICO/wiki/Installation\n\n\npip install dmri-amico\n\n\nIn Python:\n\n\nimport amico\namico.setup()\namico.util.fsl2scheme(\"derivatives/qsirecon/sub-Pilot01/dwi/sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.bval\", \"derivatives/qsirecon/sub-Pilot01/dwi/sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.bvec\")\n\nae = amico.Evaluation(\"derivatives/qsirecon\", \"sub-Pilot01/dwi\")\nae.load_data( \"sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.nii.gz\", \"sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.scheme\", mask_filename=\"sub-Pilot01_space-T1w_desc-preproc_fslstd_mask.nii.gz\", b0_thr=0 )\n\nae.set_model(\"NODDI\")\nae.generate_kernels()\n\nae.load_kernels()\nae.fit()\nae.save_results()\n\n\nAs a Python Script you can run:\n\n\nimport amico\nimport os\nimport sys, getopt\n\ndef main(argv):\n   dwifile = ''\n   maskfile = ''\n   opts, args = getopt.getopt(argv,\"hd:m:\",[\"dfile=\",\"mfile=\"])\n   for opt, arg in opts:\n      if opt == '-h':\n         print ('test.py -d &lt;dwifile&gt; -m &lt;maskfile&gt;')\n         sys.exit()\n      elif opt in (\"-d\", \"--dwifile\"):\n         dwifile = arg\n      elif opt in (\"-m\", \"--maskfile\"):\n         maskfile = arg\n\n   basename=os.path.splitext(dwifile)[0]\n   schemefile=basename+'.scheme'\n   bvalfile=basename+'.bval'\n   bvecfile=basename+'.bvec'\n   print ('Bval file is ', bvalfile)\n   print ('Bvec file is ', bvecfile)\n   print ('Scheme file is ', schemefile)\n   print ('Dwi file is ', dwifile)\n   print ('Mask file is ', maskfile)\n\n   amico.setup()\n   amico.util.fsl2scheme(f\"{bvalfile}\", f\"{bvecfile}\")\n\n   ae = amico.Evaluation(\".\", \".\")\n   ae.load_data( f\"{dwifile}\", f\"{schemefile}\", mask_filename=f\"{maskfile}\", b0_thr=0 )\n\n   ae.set_model(\"NODDI\")\n   ae.generate_kernels()\n\n   ae.load_kernels()\n   ae.fit()\n   ae.save_results()\n\nif __name__ == \"__main__\":\n   main(sys.argv[1:])",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#DTI-MatlabToolbox",
    "href": "mrimethods/DTI.html#DTI-MatlabToolbox",
    "title": "DTI",
    "section": "Matlab Toolbox",
    "text": "Matlab Toolbox\nhttp://mig.cs.ucl.ac.uk/index.php?n=Tutorial.NODDImatlab\nThe toolbox requires:\n\nMatlab with the optimization toolbox\nNIFTI Matlab library: nifti_matlab\n\nI first run our https://github.com/WeberLab/Misc/blob/main/dtipipeline.sh script to get a 'topped-up' DTI and brain mask to use\nThen convert to Analyze:\n\n\n3dcalc -a eddy_corrected_data.nii -expr 'a' -prefix eddy_corrected_data.hdr\n3dcalc -a DTI_topupapplied_brain_mask.nii -expr 'a' -prefix DTI_topupapplied_brain_mask.hdr\n\n\nIn Matlab:\n\n\naddpath(genpath('/home/weberam2/matlab/NODDI_tool')) % or whever the NODDI from Github is\naddpath(genpath('/home/weberam2/matlab/nifti_matlab')) % or whever your nifti_matlab is\nCreateROI('eddy_corrected_data.hdr', 'DTI_topupapplied_brain_mask.hdr', 'NODDI_roi.mat');\nprotocol = FSL2Protocol('DTI.bval', 'DTI.bvec');\nnoddi = MakeModel('WatsonSHStickTortIsoV_B0');\nbatch_fitting('NODDI_roi.mat', protocol, noddi, 'FittedParams.mat', 8);\nSaveParamsAsNIfTI('FittedParams.mat', 'NODDI_roi.mat', 'DTI_topupapplied_brain_mask.hdr', 'outputname')",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#DTI-PythonVersion",
    "href": "mrimethods/DTI.html#DTI-PythonVersion",
    "title": "DTI",
    "section": "Python Version",
    "text": "Python Version\nhttps://github.com/dicemt/DTI-NODDI",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#probtrackx-fsl-dti-probtrackxfsl",
    "href": "mrimethods/DTI.html#probtrackx-fsl-dti-probtrackxfsl",
    "title": "DTI",
    "section": "Probtrackx (FSL) {#DTI-Probtrackx(FSL)}",
    "text": "Probtrackx (FSL) {#DTI-Probtrackx(FSL)}\nhttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide\nhttps://fsl.fmrib.ox.ac.uk/fslcourse/2019_Beijing/lectures/FDT/fdt2.html",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#DTI-MRTRIX3",
    "href": "mrimethods/DTI.html#DTI-MRTRIX3",
    "title": "DTI",
    "section": "MRTRIX3",
    "text": "MRTRIX3\nsee mritrix3",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#DTI-RadialDiffusivity",
    "href": "mrimethods/DTI.html#DTI-RadialDiffusivity",
    "title": "DTI",
    "section": "Radial Diffusivity",
    "text": "Radial Diffusivity\nRD = (L2 + L3) / 2\n\n\n$FSLDIR/bin/fslmaths ${dtifit_folder}/${subj}*_L2.nii.gz –add ${dtifit_folder}/${subj}*_L3.nii.gz \\\\\n       -div 2 ${parentDirectory}/RD/${subj}_RD.nii.gz\n\n\n:::::::::::::::::: :::::::::::::::::::: :::::::::::::::::::::::\n\n\nDocument generated by Confluence on Feb 13, 2025 22:21\n\nAtlassian\n\n\n\n:::::::::::::::::::::::::::",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#amico",
    "href": "mrimethods/DTI.html#amico",
    "title": "DTI",
    "section": "AMICO",
    "text": "AMICO\nhttps://github.com/daducci/AMICO/wiki/Installation\npip install dmri-amico\n\nExample:\nIn Python:\nimport amico\namico.setup()\namico.util.fsl2scheme(\"derivatives/qsirecon/sub-Pilot01/dwi/sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.bval\", \"derivatives/qsirecon/sub-Pilot01/dwi/sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.bvec\")\n\nae = amico.Evaluation(\"derivatives/qsirecon\", \"sub-Pilot01/dwi\")\nae.load_data( \"sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.nii.gz\", \"sub-Pilot01_space-T1w_desc-preproc_fslstd_dwi.scheme\", mask_filename=\"sub-Pilot01_space-T1w_desc-preproc_fslstd_mask.nii.gz\", b0_thr=0 )\n\nae.set_model(\"NODDI\")\nae.generate_kernels()\n\nae.load_kernels()\nae.fit()\nae.save_results()\nAs a Python Script you can run:\nimport amico\nimport os\nimport sys, getopt\n\ndef main(argv):\n   dwifile = ''\n   maskfile = ''\n   opts, args = getopt.getopt(argv,\"hd:m:\",[\"dfile=\",\"mfile=\"])\n   for opt, arg in opts:\n      if opt == '-h':\n         print ('test.py -d &lt;dwifile&gt; -m &lt;maskfile&gt;')\n         sys.exit()\n      elif opt in (\"-d\", \"--dwifile\"):\n         dwifile = arg\n      elif opt in (\"-m\", \"--maskfile\"):\n         maskfile = arg\n\n   basename=os.path.splitext(dwifile)[0]\n   schemefile=basename+'.scheme'\n   bvalfile=basename+'.bval'\n   bvecfile=basename+'.bvec'\n   print ('Bval file is ', bvalfile)\n   print ('Bvec file is ', bvecfile)\n   print ('Scheme file is ', schemefile)\n   print ('Dwi file is ', dwifile)\n   print ('Mask file is ', maskfile)\n\n   amico.setup()\n   amico.util.fsl2scheme(f\"{bvalfile}\", f\"{bvecfile}\")\n\n   ae = amico.Evaluation(\".\", \".\")\n   ae.load_data( f\"{dwifile}\", f\"{schemefile}\", mask_filename=f\"{maskfile}\", b0_thr=0 )\n\n   ae.set_model(\"NODDI\")\n   ae.generate_kernels()\n\n   ae.load_kernels()\n   ae.fit()\n   ae.save_results()\n\nif __name__ == \"__main__\":\n   main(sys.argv[1:])",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#matlab-toolbox",
    "href": "mrimethods/DTI.html#matlab-toolbox",
    "title": "DTI",
    "section": "Matlab Toolbox",
    "text": "Matlab Toolbox\nhttp://mig.cs.ucl.ac.uk/index.php?n=Tutorial.NODDImatlab\nThe toolbox requires:\n\nMatlab with the optimization toolbox\nNIFTI Matlab library: nifti_matlab\n\nFirst run the simple DTI script from before (https://github.com/WeberLab/Misc/blob/main/dtipipeline.sh) script to get a 'topped-up' DTI and brain mask to use.\nThen convert to Analyze (AFNI):\n3dcalc -a eddy_corrected_data.nii -expr 'a' -prefix eddy_corrected_data.hdr\n3dcalc -a DTI_topupapplied_brain_mask.nii -expr 'a' -prefix DTI_topupapplied_brain_mask.hdr\nIn Matlab:\naddpath(genpath('/home/weberam2/matlab/NODDI_tool')) % or whever the NODDI from Github is\naddpath(genpath('/home/weberam2/matlab/nifti_matlab')) % or whever your nifti_matlab is\nCreateROI('eddy_corrected_data.hdr', 'DTI_topupapplied_brain_mask.hdr', 'NODDI_roi.mat');\nprotocol = FSL2Protocol('DTI.bval', 'DTI.bvec');\nnoddi = MakeModel('WatsonSHStickTortIsoV_B0');\nbatch_fitting('NODDI_roi.mat', protocol, noddi, 'FittedParams.mat', 8);\nSaveParamsAsNIfTI('FittedParams.mat', 'NODDI_roi.mat', 'DTI_topupapplied_brain_mask.hdr', 'outputname')",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#python-version",
    "href": "mrimethods/DTI.html#python-version",
    "title": "DTI",
    "section": "Python Version",
    "text": "Python Version\nhttps://github.com/dicemt/DTI-NODDI",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#probtrackx-fsl",
    "href": "mrimethods/DTI.html#probtrackx-fsl",
    "title": "DTI",
    "section": "Probtrackx (FSL)",
    "text": "Probtrackx (FSL)\nhttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide\nhttps://fsl.fmrib.ox.ac.uk/fslcourse/2019_Beijing/lectures/FDT/fdt2.html",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#mrtrix3",
    "href": "mrimethods/DTI.html#mrtrix3",
    "title": "DTI",
    "section": "MRTRIX3",
    "text": "MRTRIX3\nsee add link",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/DTI.html#radial-diffusivity",
    "href": "mrimethods/DTI.html#radial-diffusivity",
    "title": "DTI",
    "section": "Radial Diffusivity",
    "text": "Radial Diffusivity\nRD = (L2 + L3) / 2\n$FSLDIR/bin/fslmaths ${dtifit_folder}/${subj}*_L2.nii.gz –add ${dtifit_folder}/${subj}*_L3.nii.gz \\\\\n       -div 2 ${parentDirectory}/RD/${subj}_RD.nii.gz",
    "crumbs": [
      "Home",
      "MRI Methods",
      "DTI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html",
    "href": "mrimethods/fMRI.html",
    "title": "fMRI",
    "section": "",
    "text": "If you prefer to have a working understanding of the issues that fMRI data acquisition and analysis present before playing around with the software, I recommend starting with the introductory material from FSL and then a read of 'The Statistical Analysis of fMRI Data' Martin A.Lindquist\nThis seems like a good resource as well:\nhttps://towardsdatascience.com/exploring-cognitive-differences-via-resting-state-networks-2112bf5291e2\nThis paper has a very thorough example of typical fMRI preprocessing steps:\nhttps://www.frontiersin.org/articles/10.3389/fnins.2017.00685/full\nFree course:\nhttps://ucl.podia.com/designing-and-analysing-fmri-experiments",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#do-you-want-a-low-tr",
    "href": "mrimethods/fMRI.html#do-you-want-a-low-tr",
    "title": "fMRI",
    "section": "Do you want a low TR?",
    "text": "Do you want a low TR?\n\nWhy would someone want low TR?:\n\nImprove temporal precision:\n\nA shorter TR (e.g., &lt; 2 seconds) allows for more frequent sampling of the BOLD signal. Even though the HRF is slow, a faster TR can provide better temporal precision and align the timing of neural events more accurately to the fMRI signal. A shorter TR helps capture fast transitions in brain states, which can be important in cognitive tasks or resting-state analysis. A review of an updated view of the HRF can be found here: 10.1016/j.pneurobio.2021.102174\nIn resting-state fMRI (rs-fMRI), TR &lt; 2 seconds can be important because spontaneous brain activity happens continuously and can exhibit rapid changes over time. A faster TR provides better sampling to capture the fluctuations within resting-state networks and increases the ability to detect temporal correlations between regions of the brain.\nIn task-based fMRI, shorter TR acquisitions (on the order of 1000 ms) provide better discrimination between the activated and nonactivated brain tissue regions than do long-TR acquisitions (on the order of 4000 ms) [10.1002/mrm.1253].\n\nImprove statistical power:\n\nMore time points reduce signal noise and improve the ability to detect subtle changes (shape and timing) in the BOLD signal, improving sensitivity to brain activity. It also helps avoid aliasing with non-neuronal signals of higher-frequency.\n\n\nWhy would someone want to avoid low TRs?:\nSNR is exponentially lower due to the reduced T1 recovery which can occur within the TR [10.1002/9781118633953].\nAt TRs below 1 second, T1 effects become more evident, with reduced contrast between the white and grey matter. This may impact registration algorithms intended to correct head-motion or to register functional data with anatomical images [10.1016/j.neuroimage.2013.05.039].\n\n\nGeneral concensus:\nA TR of ~1000±200 ms is likely to be the optimal range for many studies [10.1016/j.neuroimage.2018.05.011; 10.1002/mrm.27498]\n\n\nReferences:\n\n10.1016/j.pneurobio.2021.102174: Polimeni JR, Lewis LD. Imaging faster neural dynamics with fast fMRI: A need for updated models of the hemodynamic response. _Progress in Neurobiology_. 2021;207:102174\n10.1016/j.neuroimage.2013.05.039: Smith SM, Beckmann CF, Andersson J, et al. Resting-state fMRI in the Human Connectome Project. NeuroImage. 2013;80:144-168.\n10.1002/9781118633953: Brown RW, Cheng YCN, Haacke EM, Thompson MR, Venkatesan R. Magnetic Resonance Imaging: Physical Principles and Sequence Design. John Wiley & Sons; 2014.\n10.1016/j.neuroimage.2018.05.011: Demetriou L, Kowalczyk OS, Tyson G, Bello T, Newbould RD, Wall MB. A comprehensive evaluation of increasing temporal resolution with multiband-accelerated protocols and effects on statistical outcome measures in fMRI. _NeuroImage_. 2018;176:404-416.\n10.1002/mrm.27498: McDowell AR, Carmichael DW. Optimal repetition time reduction for single subject event-related functional magnetic resonance imaging. _Magnetic Resonance in Medicine_. 2019;81(3):1890-1897.\n10.1002/mrm.1253: Constable TR, Spencer DD. Repetition time in echo planar functional MRI. 2001",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#what-kind-of-acceleration-should-i-use",
    "href": "mrimethods/fMRI.html#what-kind-of-acceleration-should-i-use",
    "title": "fMRI",
    "section": "What kind of acceleration should I use?",
    "text": "What kind of acceleration should I use?\nStop Using Crazy Multiband Sequences\n\nDon’t get me wrong, I love multiband, it’s amazing, but I use it very carefully. My ‘standard’ sequence these days is MB = 2 and GRAPPA = 2. This gives a combined 4x (MB*GRAPPA) acceleration, and gives you 40-ish slices of 3x3x3mm voxels — plenty for whole-brain coverage — and a TR of about 1.25s. I’ve pushed it to MB3 on occasion when I wanted to do specific things like use thinner slices to mitigate susceptibility problems in orbito-frontal cortex, but I wouldn’t push it higher than that. (I see no problem in combining multiband with slice-based acceleration like GRAPPA/SENSE, though I know some people are dead-set against it. GRAPPA/SENSE are old, tried-and-tested technology, and they work great. Why not use them and keep the multiband acceleration factor lower?).",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#dummy-scans",
    "href": "mrimethods/fMRI.html#dummy-scans",
    "title": "fMRI",
    "section": "Dummy scans:",
    "text": "Dummy scans:\nRemoval of the first 5 to 10 volumes to allow for steady-state magnetisation:\nfslroi &lt;input&gt; &lt;output&gt; 5 -1\nNote: indexing (in both time and space) starts with 0 not 1! Inputting -1 for a size will set it to the full image extent for that dimension.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#intensity-normalization",
    "href": "mrimethods/fMRI.html#intensity-normalization",
    "title": "fMRI",
    "section": "Intensity Normalization",
    "text": "Intensity Normalization\nfslmaths &lt;input&gt; -ing 10000 &lt;output&gt;",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#nuisance-variable-removal",
    "href": "mrimethods/fMRI.html#nuisance-variable-removal",
    "title": "fMRI",
    "section": "Nuisance Variable Removal",
    "text": "Nuisance Variable Removal\n\nNuisance regression of WM and CSF:\n1-a) With the WM, we can do:\nerode WM mask\nfslmaths WM_mask_in_highres_space -ero WM_mask_in_highres_space_eroded\nregister to BOLD space\nflirt -in WM_mask_in_highres_space_eroded \\\n-applyxfm \\\n-init rest.feat/reg/highres2example_func.mat\\\n-ref  example_func.nii.gz \\\n-out  WM_in_func_space\nfslmeants “Prints average timeseries (intensities) to the screen (or saves to a file)”\nfslmeants -i denoised_func_data.nii.gz -m WM_mask_in_func_space --no_bin -o WM_in_func_bin_timeseries\nNote: (i) The WM_mask_highres_space can be obtained in several ways (e.g., FSL/fast, SPM12/segment)\nWeighted segmentation information is actually good stuff because it allows more certainty for WM voxels (vs. non-WM voxels). To use the information, use --no_bin in fslmeants.\n(ii) You want to erode the WM mask to begin with, in order to increase the certainty (e.g., reducing some boundaries with GM).\n1-b) We can use similar procedure as above to obtain CSF_in_func_bin_timeseries\n1-c) To regress out these two nuisance regressors:\npaste WM_in_func_bin_timeseries and CSF_in_func_bin_timeseries into nuisance_timeseries:\nfslmaths denoised_func_data -Tmean tempMean \nfsl_glm -i denoised_func_data -d nuisance_timeseries -demean -res_out=residual\nNote: use --demean to mean-center the nuisance timeseries so that the residual makes sense. Don’t use --des_norm or --dat_norm because we are looking at the residual (but not the GLM betas) here. You also want to add the mean back so that it is a firm data set (see step (3) below).\n\n\nUsing outputs from fmriprep:\nFrom https://www.biorxiv.org/content/10.1101/2023.06.23.546329v1.full.pdf:\n\n\"Denoising followed the anatomical CompCor (aCompCor) method of removing cardiac and motion artifacts, by regressing out of each individual’s functional data the first 5 principal components corresponding to white matter signal, and the first 5 components corresponding to cerebrospinal fluid signal, as well as six subject-specific realignment parameters (three translations and three rotations) and their first-order temporal derivatives, and nuisance regressors identified by the artifact detection software art: https://www.nitrc.org/projects/artifact_detect/ \"",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#temporal-filtering",
    "href": "mrimethods/fMRI.html#temporal-filtering",
    "title": "fMRI",
    "section": "Temporal Filtering",
    "text": "Temporal Filtering\nFor highpass only (recommended) [note: \"highpass\" removes low-frequency signals. \"lowpass\" does the opposite]\nFor a TR = 2.0 s and I if you want to highpass filter at 100 seconds, this means 50 TRs (volumes), and thus highpass sigma is 25.0 TR (volumes).\nfslmaths &lt;input&gt; -Tmean tempMean\nfslmaths &lt;input&gt; -bptf &lt;sigma&gt; -1 -add tempMean &lt;output&gt;\nimrm tempMean",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#spatial-smoothing",
    "href": "mrimethods/fMRI.html#spatial-smoothing",
    "title": "fMRI",
    "section": "Spatial Smoothing",
    "text": "Spatial Smoothing\nTo smooth or not to smooth? And by how much?\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7462426\n3dmerge -1blur_fwhm &lt;kernel size in mm&gt; -doall -prefix &lt;output&gt; &lt;input&gt;",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#melodic-through-command-line",
    "href": "mrimethods/fMRI.html#melodic-through-command-line",
    "title": "fMRI",
    "section": "Melodic through command line",
    "text": "Melodic through command line\nYou can automate Melodic and not have to use the GUI every time.\nThis requires having a design.fsf file, which Melodic GUI creates every time you run an analysis.\nUsing a sample one, you can edit it with a simple editer, like 'nano'\nnano design.fsf\nYou will have to change the file name and directory, the output directory, possibly the number of volumes, etc.\nWhen you are ready, all you have to do is run:\nfeat design.fsf\nTo suppress the automatic opening of the browser window, make sure to uncheck the \"Progress Watcher\" in the Misc Tab.\nOr, change the featwatcher_yn parameter in the design.fsf from a 1 to a 0:\n# Run Featwatcher\nset fmri(featwatcher_yn) 0\n\neasy fsl_regfilt\ncomponents=$(awk 'NR&gt;1{print $1}' RS=[ FS=] labels)\nfsl_regfilt -i filtered_func_data.nii.gz -d filtered_func_data.ica/melodic_mix -o filtered_func_data_clean.nii.gz -f \"${components}\"",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#signal-classification",
    "href": "mrimethods/fMRI.html#signal-classification",
    "title": "fMRI",
    "section": "Signal Classification",
    "text": "Signal Classification\nFormer MASc student Olivia Campbell going through ICA signal classification: https://drive.google.com/file/d/1hxBUNFbtUHBEKMYWt1bCzclwZSHvSHEH/preview\nSignal classification flowchat:\n\nAnd from FSL:\n\nTips for manual classification: (taken from [https://www.sciencedirect.com/science/article/pii/S1053811916307583 ])\n\nAbout 70% of components should be noise, the other 30% signal\n\nSo for the 70 component ICA,there should be about 20 signals\n\nThe first components that Melodic lists are usually noise\nMake sure that the signal is located in grey matter throughout ALL planes\nThe overall aim is to preserve as much signal as possible\n\nComponents are \"innocent until proven guilty\" (components are signals until proven noise)\n\nUse the spatial map as the main decision maker, then look at time series and power spectra\nGeneral criteria for signal classification:\n\n\nSpatial maps need to be in grey matter, away from main veins, WM, CSF\nTime series should be without sudden or abrupt changes\nPower spectra should largely show power at low frequency\n\nFor automatic classification, you can use FIX, but that requires you to manually train the data…\nor ICA-AROMA which I think is supposed to work well on getting rid of motion… (here's the manual)\nOther options are:\nhttps://fmriprep.org/en/stable/index.html\nhttps://github.com/BMHLab/DiCER\nhttps://www.nitrc.org/projects/artifact_detect",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#fMRI-Install",
    "href": "mrimethods/fMRI.html#fMRI-Install",
    "title": "fMRI",
    "section": "Install",
    "text": "Install\nFIX requires FSL, Matlab (should be installed on all our computers), and R with the following packages installed: 'kernlab' version 0.9.24; 'ROCR' version 1.0.7; 'class' version 7.3.14; 'party' version 1.0.25; 'e1071' version 1.6.7; 'randomForest' version 4.6.12\nTo install these R packages; enter R (type: R) and enter:\ninstall.packages(c(\"kernlab\",\"ROCR\",\"e1071\",\"randomForest\",\"devtools\"))\nThen:\nrequire(devtools)\ninstall_version(\"mvtnorm\", version=\"1.0-8\")\ninstall_version(\"multcomp\", version=\"1.4-8\")\ninstall_version(\"coin\", version=\"1.2-2\")\ninstall_version(\"party\", version=\"1.0-25\")\nWhen asked if you want to update to a newer version, just press enter ('empty line to skip updates')\nTo quit R, type: quit()",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#setup",
    "href": "mrimethods/fMRI.html#setup",
    "title": "fMRI",
    "section": "Setup:",
    "text": "Setup:\n\nUnpack FIX (1.06.15) with tar xvfz fix.tar.gz (or tar xvf fix.tar if your browser has already uncompressed the file).\n\nWe installed in: /usr/local/fix\n\nDownload and install matlab compiled runtime (MCR) for your operating system (mac, linux)\n\nHow to Install MCR\nNOTE: We set our Matlab to be the installed version (which matlab). See the settings.sh file in: /usr/local/fix]\n\nSee the README file for further setup instructions (in /usr/local/fix",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#path",
    "href": "mrimethods/fMRI.html#path",
    "title": "fMRI",
    "section": "Path",
    "text": "Path\nChange your path to include: /usr/local/fix",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#files-needed",
    "href": "mrimethods/fMRI.html#files-needed",
    "title": "fMRI",
    "section": "Files needed",
    "text": "Files needed\n  filtered_func_data.nii.gz          preprocessed 4D data\n  filtered_func_data.ica             melodic (command-line program) full output directory\n  mc/prefiltered_func_data_mcf.par   motion parameters created by mcflirt (in mc subdirectory)\n  mask.nii.gz                        valid mask relating to the 4D data\n  mean_func.nii.gz                   temporal mean of 4D data\n  reg/example_func.nii.gz            example image from 4D data\n  reg/highres.nii.gz                 brain-extracted structural\n  reg/highres2example_func.mat       FLIRT transform from structural to functional space\n  design.fsf                         FEAT/MELODIC setup file; if present, this controls the\n                                       default temporal filtering of motion parameters\nThe design.fsf is optional.\nHere is a sample script that will create the FIX input directory and run FIX. You will need to change the files to suit your needs.\nhighDir=/home/johann.drayne/testing/fix/denoise/\nderivDir=/mnt/WeberLab/Projects/NeonateSucrose/SickKids/derivatives/\n\n#### output Directory ####\noutputDir=${highDir}output\n\n#### training file that you give to FIX ####\nclassifier=/mnt/WeberLab/Data/dHCP/train35.RData\n\n#### skull stripped anatomical image ####\nanat=${highDir}MS040008-session1_restore_brain\n\n#### functional image registered to anat space ####\nfmri2anat=${highDir}_bold2template\n\n#### motion corrected input functional image ####\nfmri=${derivDir}motioncorrectedfmri/_MS040008motioncorrected.nii.gz\n\n#### linear transform of functional to anat space ####\nf2amat=${derivDir}tempmotioncorrectedfmriv01mat/MS040008/_rigid_avgfmri_to_t10GenericAffine.mat\n\nmkdir ${outputDir}\nmkdir ${outputDir}/split/\nmkdir ${outputDir}/reg/\nmkdir ${outputDir}/mc\n\n### splitting 4D file to add example image into output/reg #####\nfslsplit ${fmri} ${outputDir}/split/\ncp ${outputDir}/split/0000.nii.gz ${outputDir}/reg/example_func.nii.gz\necho \"----- Func single volume copied to output -----\"\n\n### creating binary mask using fslmaths #####\nantsApplyTransforms -d 3 -o ${outputDir}/anat2func.nii.gz -t [${f2amat}, 1] -r ${outputDir}/reg/example_func.nii.gz -i ${anat}.nii\nfslmaths ${outputDir}/anat2func -thrp 5 -bin ${outputDir}/mask\necho \"----- Binary Mask Complete -----\"\n\n#### running melodic on fmri #####\nmelodic -i ${derivDir}motioncorrectedfmri/_MS040008motioncorrected -o ${outputDir}/filtered_func_data.ica  -m ${outputDir}/mask --nobet -v\necho \"----- MELODIC Complete -----\"\n\n#### adding mean to output #####\ncp ${outputDir}/filtered_func_data.ica/mean.nii.gz ${outputDir}/mean_func.nii.gz\necho \"----- mean added to output -----\"\n\n#### adding skull stripped anat to output/reg/ #####\ncp ${anat}.nii ${outputDir}/reg/highres.nii.gz\necho \"----- Skull Stripped anat copied to output -----\"\n\n#### adding func to output #####\ncp ${fmri} ${outputDir}/filtered_func_data.nii.gz\necho \"----- Func copied to output -----\"\n\n#### creating .par file in output/mc/ with mcflirt #####\nmcflirt -in ${fmri} -plots -o ${outputDir}/mc/prefiltered_func_data_mcf\necho \"----- mcflirt motion parameters in output -----\"\n\n#### creating .mat transformation from ANTs to FSL format anat -&gt; func space#####\nc3d_affine_tool -itk ${f2amat} -ref ${anat}.nii -src ${outputDir}/reg/example_func.nii.gz -ras2fsl -o ${outputDir}/reg/highres2example_func.mat -inv\necho \"----- .mat file added to output -----\"\n\n### running FIX #####\n/usr/local/fix/fix -f ${outputDir}/\necho \"----- Extracted Feautres -----\"\n\n/usr/local/fix/fix -c ${outputDir}/ ${classifier} 5\necho \"----- Classified ICA Components ------\"\n\n/usr/local/fix/fix -a ${outputDir}/fix4melview_train35_thr5.txt\necho \"----- FIX Complete -----\"",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#identifying-rsns",
    "href": "mrimethods/fMRI.html#identifying-rsns",
    "title": "fMRI",
    "section": "Identifying RSNs",
    "text": "Identifying RSNs\n\nMaps 120, 220 and 320 (“visual”) correspond to medial, occipital pole, and lateral visual areas. The explicitly visual behavioral domains correspond most strongly to these maps, and paradigms cognition–language–orthography and cognition–space correspond to the occipital pole and lateral visual maps, respectively. We presume that the “orthography” correspondence reflects the visual nature of stimuli used in these studies (e.g., written-word forms).\nMap 420 (“default mode network”) includes medial parietal (precuneus and posterior cingulate), bilateral inferior–lateral–parietal and ventromedial frontal cortex. This is often referred to as the default mode network (9), and is possibly the most widely studied RSN in the resting-state FMRI literature. This is also the network that is most commonly seen as deactivating in task-based FMRI experiments; hence, one would not expect this map to correspond strongly to any particular behavioral domain, because more contrasts associated with any given paradigm will, on average, be looking for positive activations rather than deactivations (or negative contrasts). However, there will be some studies that contain a “deactivation” contrast, and hence, we are not surprised that this map is found in our analysis of BrainMap. Indeed, inspection of the full set of experiments reveals that this map does indeed correspond in general to negative contrasts, in particular in cognitive paradigms.\nMap 520 (“cerebellum”) covers the cerebellum. Because of limited field of view of the MRI acquisitions in some of the resting FMRI subjects, more inferior parts of the cerebellum are not included in the multisubject RSN analysis. This corresponds most strongly to action–execution and perception–somesthesis–pain domains.\nMap 620 (“sensorimotor”) includes supplementary motor area, sensorimotor cortex, and secondary somatosensory cortex. This corresponds closely to the activations seen in bimanual motor tasks and was the first resting state network to be identified in FMRI data (1). This corresponds most strongly to the action–execution and perception–somesthesis paradigms.\nMap 720 (“auditory”) includes the superior temporal gyrus, Heschl's gyrus, and posterior insular. It includes primary and association auditory cortices. This corresponds most strongly to action–execution–speech, cognition–language–speech, and perception–audition paradigms.\nMap 820 (“executive control”) covers several medial–frontal areas, including anterior cingulate and paracingulate. This corresponds strongly to several cognition paradigms, as well as action–inhibition, emotion, and perception–somesthesis–pain.\nMaps 920 and 1020 (“frontoparietal”) cover several frontoparietal areas. These are the only maps to be strongly lateralized, and are largely left–right mirrors of each other. They correspond to several cognition/language paradigms. In addition, map 920 corresponds strongly to perception–somesthesis–pain; this is consistent with the insular areas seen (see SI for more detailed figures showing all maps). Map 1020 corresponds strongly to cognition–language paradigms, which is consistent with the Broca's and Wernicke's areas seen in the map (see SI for slices more clearly showing these areas). Given the known lateralization of language function, it is not surprising that these (mirrored) networks have such different behavioral domain associations.\nfrom https://www.pnas.org/content/106/31/13040",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#statistics",
    "href": "mrimethods/fMRI.html#statistics",
    "title": "fMRI",
    "section": "Statistics",
    "text": "Statistics\nFinally you'll want to use FSL's randomise\n\nExample:\nFrom my 2014 paper: https://www.sciencedirect.com/science/article/pii/S0278584614000773\n\nGroup differences were obtained from the Dual Regression program, where the different sets of spatial maps were collected across subjects into single 4D files (one per RSN) and analyzed across groups (OCD vs. Control) using non-parametric statistical analysis involving permutation testing (Beckmann et al., 2009). The number of permutations was set at 10,000 to maximize alpha level calculations (Webster, 2012). As this study is the first ICA analysis of OCD with no specific RSN being tested, it was treated as exploratory in nature (i.e. as a pilot study); thus, correction for multiple-comparisons (Bonferroni) was not performed.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#nodes",
    "href": "mrimethods/fMRI.html#nodes",
    "title": "fMRI",
    "section": "Nodes:",
    "text": "Nodes:\nNodes are how you decide to divide/segment the brain\n\nContiguous vs Non-contiguous:\nContiguous nodes are single ROIs\nNon-contiguous nodes might be two or more ROIs together: for example, similar regions across hemispheres (left and right)\n\n\nBinary or Weighted:\nBinary nodes are thresholded and then given 0 or 1. This is intuitive for graph representation and interpretation\nWeighted is when you do not threshold and just use the raw correlation value. This fits with complex brain organization and allows measurement error and physiological limits (HRF)\n\n\nNode Definition:\n\nAnatomical (e.g. Harvard-Oxford) - not great for resting state fMRI\nFunctional atlases (e.g. Yeo 2011 / Glasser 2016) - not data-driven\nData-driven parcellation (e.g. ICA, clustering, gradients) - data-driven",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#timeseries",
    "href": "mrimethods/fMRI.html#timeseries",
    "title": "fMRI",
    "section": "Timeseries:",
    "text": "Timeseries:\nFrom each node, you will average the time-series of all the voxels within the node to give that node’s time-series\nFor getting the time-series from ICA, use FSL’s dual-regression",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#edges",
    "href": "mrimethods/fMRI.html#edges",
    "title": "fMRI",
    "section": "Edges:",
    "text": "Edges:\nThis is the temporal correlation of the timeseries between nodes\nYou can represent edges in three different ways:\n\npresence or absence of edges\nstrength of edges\nand directionality of edges (hard to estimate with BOLD data)\n\nAlso, there are direct and indirect connections: where node 2 is indirectly correlated with node 3 through node 1\n\nTo avoid this, you can regress out node 1 from both 2 and 3. If 2 and 3 are still correlated, a direct connection exists. When you perform this correction, it is called partial correlation.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#network-matrix",
    "href": "mrimethods/fMRI.html#network-matrix",
    "title": "fMRI",
    "section": "Network Matrix:",
    "text": "Network Matrix:\nColored matrix which tells you node-node strength\n\nHierarchical clustering:\nThis groups common nodes together\nNote: this is not a statistical test",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#group-analysis",
    "href": "mrimethods/fMRI.html#group-analysis",
    "title": "fMRI",
    "section": "Group Analysis",
    "text": "Group Analysis\nNetwork Matrix → take just the upper diagonal → convert this to a single dimension of just edge values → then do this for all subjects to create a 2D matrix where each row is a subject, and each column an edge.\nTo perform group level comparisons, you would then run a GLM",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#xcp-d",
    "href": "mrimethods/fMRI.html#xcp-d",
    "title": "fMRI",
    "section": "XCP-D",
    "text": "XCP-D\nhttps://xcp-d.readthedocs.io/en/latest/index.html\n\nXCP-D paves the final section of the reproducible and scalable route from the MRI scanner to functional connectivity data in the hands of neuroscientists. We developed XCP-D to extend the BIDS and NiPrep apparatus to the point where data is most commonly consumed and analyzed by neuroscientists studying functional connectivity. Thus, with the development of XCP-D, data can be automatically preprocessed and analyzed in BIDS format, using NiPrep-style containerized code, all the way from the scanner to functional connectivity matrices.",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "mrimethods/fMRI.html#conn-toolbox",
    "href": "mrimethods/fMRI.html#conn-toolbox",
    "title": "fMRI",
    "section": "CONN Toolbox",
    "text": "CONN Toolbox\nhttp://www.nitrc.org/projects/conn\nhttps://web.conn-toolbox.org/fmri-methods/denoising-pipeline\n\nTutorial:\nhttps://andysbrainbook.readthedocs.io/en/latest/FunctionalConnectivity/CONN_Overview.html",
    "crumbs": [
      "Home",
      "MRI Methods",
      "fMRI"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html",
    "href": "tutorials/BIDS.html",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "",
    "text": "Attention: the newest version of dcm2bids changed the way they write their config files…\n\n⚠️ Breaking changes alert ⚠️\ndcm2bids&gt;=3.0.0 is not compatible with config files made for v2.1.9 and below. In order to develop dcm2bids new features we had to rewrite some of its code.\nSince v3.0.0, dcm2bids has become more powerful and more flexible while reducing the burden of creating config files. Porting your config file should be relatively easy by following the How-to upgrade page. If you have any issues with it don’t hesitate to report it on Neurostars.\n\nBe sure to check out the dcm2bids website for info about the new conventions: https://unfmontreal.github.io/Dcm2Bids/dev/upgrade/#changes-to-existing-description-and-config-file-keys",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#file-organization",
    "href": "tutorials/BIDS.html#file-organization",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "File organization",
    "text": "File organization\nPart of BIDS is making sure all files are located and named in a standardized format. Generally it should look something like this:\nproject/\n└── subject\n    └── session\n        └── datatype\nMore specifically:\nmyProject/\n└── sub-01\n    └── ses-01\n        └── anat\nHere's an example:\nds001\n├── dataset_description.json\n├── participants.tsv\n├── sub-01\n│   ├── anat\n│   │   ├── sub-01_inplaneT2.nii.gz\n│   │   └── sub-01_T1w.nii.gz\n│   └── func\n│       ├── sub-01_task-balloonanalogrisktask_run-01_bold.nii.gz\n│       ├── sub-01_task-balloonanalogrisktask_run-01_events.tsv\n│       ├── sub-01_task-balloonanalogrisktask_run-02_bold.nii.gz\n│       ├── sub-01_task-balloonanalogrisktask_run-02_events.tsv\n│       ├── sub-01_task-balloonanalogrisktask_run-03_bold.nii.gz\n│       └── sub-01_task-balloonanalogrisktask_run-03_events.tsv\n├── sub-02\n│   ├── anat\n│   │   ├── sub-02_inplaneT2.nii.gz\n│   │   └── sub-02_T1w.nii.gz\n│   └── func\n│       ├── sub-02_task-balloonanalogrisktask_run-01_bold.nii.gz\n│       ├── sub-02_task-balloonanalogrisktask_run-01_events.tsv\n│       ├── sub-02_task-balloonanalogrisktask_run-02_bold.nii.gz\n│       ├── sub-02_task-balloonanalogrisktask_run-02_events.tsv\n│       ├── sub-02_task-balloonanalogrisktask_run-03_bold.nii.gz\n│       └── sub-02_task-balloonanalogrisktask_run-03_events.tsv\n...\n...\n└── task-balloonanalogrisktask_bold.json",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#config-examples-bidsbrainimagingdatastructure-configexamples",
    "href": "tutorials/BIDS.html#config-examples-bidsbrainimagingdatastructure-configexamples",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Config Examples: {#BIDS(BrainImagingDataStructure)-ConfigExamples:}",
    "text": "Config Examples: {#BIDS(BrainImagingDataStructure)-ConfigExamples:}\nExample config file:\nhttps://unfmontreal.github.io/Dcm2Bids/dev/how-to/create-config-file/\nAnother example for T1w, resting-state fMRI, and two short blip-up / blip-down spin-echo sequences for field-map estimation:\n{\n  \"dcm2niixOptions\": \"-b y -ba y -z y -f '%3s_%f_%d_%r'\",\n  \"descriptions\": [\n    {\n      \"datatype\": \"anat\",\n      \"suffix\": \"T1w\",\n      \"criteria\": {\n        \"SeriesDescription\": \"SAG_FSPGR_3D_.9X.9X.9\"\n      }\n    },\n    {\n      \"id\": \"task-rest\",\n      \"datatype\": \"func\",\n      \"suffix\": \"bold\",\n      \"custom_entities\": \"task-rest\",\n      \"criteria\": {\n        \"SeriesDescription\": \"RestStatefMRI_HB4PA1\"\n      },\n      \"sidecar_changes\": {\n        \"TaskName\": \"rest\"\n      }\n    },\n    {\n      \"datatype\": \"fmap\",\n      \"suffix\": \"epi\",\n      \"custom_entities\": \"dir-AP\",\n      \"criteria\": {\n        \"SeriesDescription\": \"SE_fMRI_pepolar_0\",\n        \"PhaseEncodingPolarityGE\": \"Unflipped\"\n      },\n      \"sidecar_changes\": {\n        \"IntendedFor\": [\n          \"task-rest\"\n        ],\n        \"B0FieldIdentifier\": \"pepolar_fmap0\"\n      }\n    },\n    {\n      \"datatype\": \"fmap\",\n      \"suffix\": \"epi\",\n      \"custom_entities\": \"dir-PA\",\n      \"criteria\": {\n        \"SeriesDescription\": \"SE_fMRI_pepolar_1\",\n        \"PhaseEncodingPolarityGE\": \"Flipped\"\n      },\n      \"sidecar_changes\": {\n        \"IntendedFor\": [\n          \"task-rest\"\n        ],\n        \"B0FieldIdentifier\": \"pepolar_fmap0\"\n      }\n    }\n  ]\n}\nNOTE:\nCurrently dcm2bids will create json files for the fmaps that have an IntendedFor field that fmriprep will not correctly know what to do with:\n\"IntendedFor\": \"bids::sub-AHIE001/func/sub-AHIE001_task-rest_dir-AP_bold.nii.gz\",\nYou will need to rewrite this for now to say:\n\"IntendedFor\": \"func/sub-AHIE001_task-rest_dir-AP_bold.nii.gz\",\nYou will notice that both the bids:: and the sub-AHIE001 has been removed.\nfmriprep plans to eventually fix this: https://neurostars.org/t/dcm2bids-version-3-1-0-intended-for-field/27888/4",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#info-and-tutorial-bidsbrainimagingdatastructure-infoandtutorial",
    "href": "tutorials/BIDS.html#info-and-tutorial-bidsbrainimagingdatastructure-infoandtutorial",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Info and Tutorial {#BIDS(BrainImagingDataStructure)-InfoandTutorial}",
    "text": "Info and Tutorial {#BIDS(BrainImagingDataStructure)-InfoandTutorial}\nhttps://unfmontreal.github.io/Dcm2Bids/",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#install-bidsbrainimagingdatastructure-install",
    "href": "tutorials/BIDS.html#install-bidsbrainimagingdatastructure-install",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Install {#BIDS(BrainImagingDataStructure)-Install}",
    "text": "Install {#BIDS(BrainImagingDataStructure)-Install}\nDependencies: you will need dcm2niix installed first though\npip install dcm2bids",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#upgrading-bidsbrainimagingdatastructure-upgrading",
    "href": "tutorials/BIDS.html#upgrading-bidsbrainimagingdatastructure-upgrading",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Upgrading {#BIDS(BrainImagingDataStructure)-Upgrading}",
    "text": "Upgrading {#BIDS(BrainImagingDataStructure)-Upgrading}\npip install --upgrade dcm2bids",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#example-bidsbrainimagingdatastructure-example",
    "href": "tutorials/BIDS.html#example-bidsbrainimagingdatastructure-example",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Example {#BIDS(BrainImagingDataStructure)-Example}",
    "text": "Example {#BIDS(BrainImagingDataStructure)-Example}\nan example folder can be found on CSPeirce: /mnt/WeberLab/Data/2020-09-17_BCCHR_test_AlexW/\nfor this, I did:\n\n\n\ncd /mnt/WeberLab/Data/ && mkdir 2020-09-17_BCCHR_test_AlexW/\n\n\n\ncd 2020-09-17_BCCHR_test_AlexW/\n\n\n\ndcm2bids_scaffold\nThis gives us several generics folders and files.\nOne of the created folders is named 'sourcedata/'\ndata from Xnat was downloaded as a zip into 'sourcedata/'\nthen:\ncd sourcedata\nunzip aweber-20200918_110552.zip\nwhich extracted the dicoms into a folder: aweber-20200918_110552/\ninside of which are numbered subfolders for each acquisition scan\ngoing back to /mnt/WeberLab/Data/2020-09-17_BCCHR_test_AlexW/\ncd ..\n\n\n\ndcm2bids_helper -d sourcedata/aweber-20200918_110552/\nThis command will convert the DICOMs files to NIfTI files and save them inside 'tmp_dcm2bids/helper/'\n[note: with GE data, I find this gives bad naming conventions. Alternatively you could run something like this:\ndcm2niix -o tmp_dcm2bids/helper -b y -ba y -z y -f ‘%3s %f %d %r’ sourcedata/AMW01/\nwhich translates to:\n-o = output folder [note: this folder should already exist]\n-b y = BIDS sidecar yes\n-ba y = anonymize BIDS yes\n-z y = gz compress images yes\n-f ‘%3s %f %d %r’ = filename with series number, folder name,\ndescription and instance number]\nWe should see a list of compressed NIfTI files (nii.gz) with their respective sidecar files (.json).\nFor this example, we will only be interested in three acquisitions:\n003_LONGMBfMRI_SAG_FSPGR_3D_.9X.9X.9_20200917132212.json\n003_LONGMBfMRI_SAG_FSPGR_3D_.9X.9X.9_20200917132212.nii.gz\n005_LONGMBfMRI_fMRI_Resting_State_AP_20200917132212.json\n005_LONGMBfMRI_fMRI_Resting_State_AP_20200917132212.nii.gz\n006_LONGMBfMRI_fMRI_Resting_State_PA_20200917132212.json\n006_LONGMBfMRI_fMRI_Resting_State_PA_20200917132212.nii.gz\n[note: because the dicoms were acquired with two fMRIs with the same name (fMRI_Resting_State_AP) I don't know how to include the third run…]\nI deleted all the other .nii.gz and .json files in order not to confuse BIDS, and also moved the dicom files we aren't interested in to another directory\n\nBuild your configuration file with the help of the content of tmp_dcm2bids/helper:\n\nNow we will create a configuration file called dcm2bids_config.json (this is just an example, it could be whatever we want) in the code/ folder.\nUse any text editor to create the file with the contents:\n{\n    \"descriptions\": [\n        {\n        \"suffix\": \"anat\",\n        \"modalitylabel\": \"T1w\",\n        \"criteria\": {\n            \"SeriesDescription\": \"SAG_FSPGR_3D_.9X.9X.9*\"\n            }\n        },\n        {\n        \"suffix\": \"func\",\n        \"modalitylabel\": \"bold\",\n        \"custom_entities\": \"task-rest-AP\",\n        \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_AP*\"\n            }\n        },\n        {\n        \"suffix\": \"func\",\n        \"modalitylabel\": \"bold\",\n        \"custom_entities\": \"task-rest-PA\",\n        \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_PA*\"\n            }\n        }\n    ]\n}\n\n\n\ndcm2bids -d sourcedata/aweber-20200918_110552/LONGMBfMRI/ -p 01 -c code/dcm2bids_config.json\nA bunch of information is printed to the terminal and we can verify that the data are now in BIDS.\nls sub-01/*\nsub-01/anat:\nsub-01_T1w.json sub-01_T1w.nii.gz\nsub-01/func:\nsub-01_task-rest_bold.json sub-01_task-rest_bold.nii.gz\nFollow the tutorial\ndcm2bids creates log files inside &lt;YOUR_FUTURE_BIDS_FOLDER&gt;/tmp_dcm2bids/log\nRun the bids-validator to check your directory. Don't forget to create a .bidsignore file at the root of your BIDS directory with tmp_dcm2bids/* inside.\nnano .bidsignore\nfill with:\ntmp_dcm2bids",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#automate-bidsbrainimagingdatastructure-automate",
    "href": "tutorials/BIDS.html#automate-bidsbrainimagingdatastructure-automate",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Automate {#BIDS(BrainImagingDataStructure)-Automate}",
    "text": "Automate {#BIDS(BrainImagingDataStructure)-Automate}\nHere is a tutorial on how to create a bash shell script to automate this process\nhttps://reproducibility.stanford.edu/bids-tutorial-series-part-1b/",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#jo-bidsbrainimagingdatastructure-jo",
    "href": "tutorials/BIDS.html#jo-bidsbrainimagingdatastructure-jo",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "jo {#BIDS(BrainImagingDataStructure)-jo}",
    "text": "jo {#BIDS(BrainImagingDataStructure)-jo}\njo is a simple program that makes writing json files easier (I think)\nhttps://github.com/jpmens/jo\nExample:\njo -p descriptions=$(jo -a $(jo dataType=anat modalityLabel=T1w criteria=$(jo SeriesDescription=SAG_FSPGR_3D_.9X_.9X.9*)) $(jo dataType=func modalityLabel=bold customLabels=task-rest-AP criteria=$(jo SeriesDescription=fMRI_Resting_State_AP*)))\nResults in:\n{\n   \"descriptions\": [\n      {\n         \"dataType\": \"anat\",\n         \"modalityLabel\": \"T1w\",\n         \"criteria\": {\n            \"SeriesDescription\": \"SAG_FSPGR_3D_.9X_.9X.9*\"\n         }\n      },\n      {\n         \"dataType\": \"func\",\n         \"modalityLabel\": \"bold\",\n         \"customLabels\": \"task-rest-AP\",\n         \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_AP*\"\n         }\n      }\n   ]\n}",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#install-bidsbrainimagingdatastructure-install.1",
    "href": "tutorials/BIDS.html#install-bidsbrainimagingdatastructure-install.1",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Install {#BIDS(BrainImagingDataStructure)-Install.1}",
    "text": "Install {#BIDS(BrainImagingDataStructure)-Install.1}\nsudo apt-get install nodejs\nsudo apt-get install npm\nsudo npm install -g bids-validator",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#run-bidsbrainimagingdatastructure-run",
    "href": "tutorials/BIDS.html#run-bidsbrainimagingdatastructure-run",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Run {#BIDS(BrainImagingDataStructure)-Run}",
    "text": "Run {#BIDS(BrainImagingDataStructure)-Run}\nbids-validator .\nOr, if you're using docker:\ndocker run -ti --rm -v /path/to/data:/data:ro bids/validator /data\nbut change /path/to/data to where you want\nIf you are in the directory ('.' is your current directory)\nDon't forget to create a .bidsignore file at the root of your BIDS directory with tmp_dcm2bids/* inside.",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#attachments",
    "href": "tutorials/BIDS.html#attachments",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Attachments:",
    "text": "Attachments:\n image-20230720-205258.png (image/png)\n\nDocument generated by Confluence on Feb 13, 2025 22:21\nAtlassian",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#resources",
    "href": "tutorials/BIDS.html#resources",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Resources",
    "text": "Resources\n\nMain website: https://bids.neuroimaging.io/\nHere is a great introductory resource: Getting started with BIDS\nDr. Alexander Weber gave a journal club presentation you can check out here: https://drive.google.com/file/d/1PLNlKTI0xGlbKmIgfwzR8j6nv9RlmxOa/preview",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#config-examples",
    "href": "tutorials/BIDS.html#config-examples",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Config Examples:",
    "text": "Config Examples:\n\n1. Example config file:\nhttps://unfmontreal.github.io/Dcm2Bids/dev/how-to/create-config-file/\n{\n  \"descriptions\": [\n    {\n      \"datatype\": \"anat\",\n      \"suffix\": \"T2w\",\n      \"criteria\": {\n        \"SeriesDescription\": \"*T2*\",\n        \"EchoTime\": 0.1\n      },\n      \"sidecar_changes\": {\n        \"ProtocolName\": \"T2\"\n      }\n    },\n    {\n      \"id\": \"task_rest\",\n      \"datatype\": \"func\",\n      \"suffix\": \"bold\",\n      \"custom_entities\": \"task-rest\",\n      \"criteria\": {\n        \"ProtocolName\": \"func_task-*\",\n        \"ImageType\": [\"ORIG*\", \"PRIMARY\", \"M\", \"MB\", \"ND\", \"MOSAIC\"]\n      }\n    },\n    {\n      \"datatype\": \"fmap\",\n      \"suffix\": \"fmap\",\n      \"criteria\": {\n        \"ProtocolName\": \"*field_mapping*\"\n      },\n      \"sidecar_changes\": {\n        \"IntendedFor\": \"task_rest\"\n      }\n    },\n    {\n      \"id\": \"id_task_learning\",\n      \"datatype\": \"func\",\n      \"suffix\": \"bold\",\n      \"custom_entities\": \"task-learning\",\n      \"criteria\": {\n        \"SeriesDescription\": \"bold_task-learning\"\n      },\n      \"sidecar_changes\": {\n        \"TaskName\": \"learning\"\n      }\n    },\n    {\n      \"datatype\": \"fmap\",\n      \"suffix\": \"epi\",\n      \"criteria\": {\n        \"SeriesDescription\": \"fmap_task-learning\"\n      },\n      \"sidecar_changes\": {\n        \"TaskName\": \"learning\",\n        \"IntendedFor\": \"id_task_learning\"\n      }\n    }\n  ]\n}\nWhere:\n\n“datatype” is the folder the file will be stored under. Can be:\n\nfunc (task based and resting state functional MRI)\ndwi (diffusion weighted imaging)\nfmap (field inhomogeneity mapping data such as field maps)\nanat (structural imaging such as T1, T2, PD, and so on)\nperf (perfusion)\nmeg (magnetoencephalography)\neeg (electroencephalography)\nieeg (intracranial electroencephalography)\nbeh (behavioral)\npet (positron emission tomography)\nmicr (microscopy)\nnirs (near infrared spectroscopy)\nmotion (motion)\nmrs (magnetic resonance spectroscopy)\nphenotype (measurement and survey data)\n\n“suffix” is literally what will be written at the end of the file. Common examples: T1w, T2w, epi, bold, dwi…\n“criteria” is what will be searched in the sidecar json file to identify your file.\n“id” this is literally an id you give this to use later when you need to refer to this scan. An example is when you need to set an “IntendedFor” for the fmap to point to the right func\n“custom_entities” is useful for things like fMRI, when you will want identify if it is rest, or listening, or motor, etc.\n“sidecare_changes” is when you are not happy with the json sidecar that is automatically made, and you need to add some information\n“B0FieldIdentifier” tells us what kind of correction you intend: e.g. “pepolar_fmap0”\n\nSo, the above example has:\n\na T2w scan, that will be stored in the anat folder\nan fMRI scan that will be stored in the func folder; has been given an ‘id’ for later when we need to associate fmaps to it; what kind of task it is (rest)\nan field map scan to be used with the rest task: “IntendedFor”\nanother fMRI scan that will be stored in the func folder; has been given a different ‘id’ for later when we need to associate fmaps to it; what kind of task it is (rest)\nanother field map to be used to correct the learning task\n\n\n\n2. Another example\nAnother example for T1w, resting-state fMRI, and two short blip-up / blip-down spin-echo sequences for field-map estimation:\n{\n  \"dcm2niixOptions\": \"-b y -ba y -z y -f '%3s_%f_%d_%r'\",\n  \"descriptions\": [\n    {\n      \"datatype\": \"anat\",\n      \"suffix\": \"T1w\",\n      \"criteria\": {\n        \"SeriesDescription\": \"SAG_FSPGR_3D_.9X.9X.9\"\n      }\n    },\n    {\n      \"id\": \"task-rest\",\n      \"datatype\": \"func\",\n      \"suffix\": \"bold\",\n      \"custom_entities\": \"task-rest\",\n      \"criteria\": {\n        \"SeriesDescription\": \"RestStatefMRI_HB4PA1\"\n      },\n      \"sidecar_changes\": {\n        \"TaskName\": \"rest\"\n      }\n    },\n    {\n      \"datatype\": \"fmap\",\n      \"suffix\": \"epi\",\n      \"custom_entities\": \"dir-AP\",\n      \"criteria\": {\n        \"SeriesDescription\": \"SE_fMRI_pepolar_0\",\n        \"PhaseEncodingPolarityGE\": \"Unflipped\"\n      },\n      \"sidecar_changes\": {\n        \"IntendedFor\": [\n          \"task-rest\"\n        ],\n        \"B0FieldIdentifier\": \"pepolar_fmap0\"\n      }\n    },\n    {\n      \"datatype\": \"fmap\",\n      \"suffix\": \"epi\",\n      \"custom_entities\": \"dir-PA\",\n      \"criteria\": {\n        \"SeriesDescription\": \"SE_fMRI_pepolar_1\",\n        \"PhaseEncodingPolarityGE\": \"Flipped\"\n      },\n      \"sidecar_changes\": {\n        \"IntendedFor\": [\n          \"task-rest\"\n        ],\n        \"B0FieldIdentifier\": \"pepolar_fmap0\"\n      }\n    }\n  ]\n}\n\n“dcm2niixOptions”: “-b y -ba y -z y -f ‘%3s_%f_%d_%r’”,\n\nThis tells dcm2bids how to run dcm2niix. I found this created more helpful filenames when running dcm2bids_helper\n\n\n\n\n\n\nNote\n\n\n\nCurrently dcm2bids will create json files for the fmaps that have an “IntendedFor”” field that fmriprep will not correctly know what to do with:\n\"IntendedFor\": \"bids::sub-AHIE001/func/sub-AHIE001_task-rest_dir-AP_bold.nii.gz\",\nYou will need to rewrite this for now to say:\n\"IntendedFor\": \"func/sub-AHIE001_task-rest_dir-AP_bold.nii.gz\",\nYou will notice that both the bids:: and the sub-AHIE001 has been removed.\nfmriprep plans to eventually fix this: https://neurostars.org/t/dcm2bids-version-3-1-0-intended-for-field/27888/4\n\nNot super helpful to anybody right now, but the next release will have experimental support for BIDS-URIs. (January 2024)",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#info-and-tutorial",
    "href": "tutorials/BIDS.html#info-and-tutorial",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Info and Tutorial",
    "text": "Info and Tutorial\nhttps://unfmontreal.github.io/Dcm2Bids/",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#install",
    "href": "tutorials/BIDS.html#install",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Install",
    "text": "Install\nDependencies: you will need dcm2niix installed first though\npip install dcm2bids\n\nUpgrading\npip install --upgrade dcm2bids",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#upgrading",
    "href": "tutorials/BIDS.html#upgrading",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Upgrading",
    "text": "Upgrading\npip install --upgrade dcm2bids",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#example",
    "href": "tutorials/BIDS.html#example",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Example",
    "text": "Example\nan example folder can be found on CSPeirce: /mnt/WeberLab/Data/2020-09-17_BCCHR_test_AlexW/\nfor this, I did:\n\n\n\ncd /mnt/WeberLab/Data/ && mkdir 2020-09-17_BCCHR_test_AlexW/\n\n\n\ncd 2020-09-17_BCCHR_test_AlexW/\n\n\n\ndcm2bids_scaffold\nThis gives us several generics folders and files.\nOne of the created folders is named 'sourcedata/'\ndata from Xnat was downloaded as a zip into 'sourcedata/'\nthen:\ncd sourcedata\nunzip aweber-20200918_110552.zip\nwhich extracted the dicoms into a folder: aweber-20200918_110552/\ninside of which are numbered subfolders for each acquisition scan\ngoing back to /mnt/WeberLab/Data/2020-09-17_BCCHR_test_AlexW/\ncd ..\n\n\n\ndcm2bids_helper -d sourcedata/aweber-20200918_110552/\nThis command will convert the DICOMs files to NIfTI files and save them inside 'tmp_dcm2bids/helper/'\n[note: with GE data, I find this gives bad naming conventions. Alternatively you could run something like this:\ndcm2niix -o tmp_dcm2bids/helper -b y -ba y -z y -f ‘%3s %f %d %r’ sourcedata/AMW01/\nwhich translates to:\n-o = output folder [note: this folder should already exist]\n-b y = BIDS sidecar yes\n-ba y = anonymize BIDS yes\n-z y = gz compress images yes\n-f ‘%3s %f %d %r’ = filename with series number, folder name,\ndescription and instance number]\nWe should see a list of compressed NIfTI files (nii.gz) with their respective sidecar files (.json).\nFor this example, we will only be interested in three acquisitions:\n003_LONGMBfMRI_SAG_FSPGR_3D_.9X.9X.9_20200917132212.json\n003_LONGMBfMRI_SAG_FSPGR_3D_.9X.9X.9_20200917132212.nii.gz\n005_LONGMBfMRI_fMRI_Resting_State_AP_20200917132212.json\n005_LONGMBfMRI_fMRI_Resting_State_AP_20200917132212.nii.gz\n006_LONGMBfMRI_fMRI_Resting_State_PA_20200917132212.json\n006_LONGMBfMRI_fMRI_Resting_State_PA_20200917132212.nii.gz\n[note: because the dicoms were acquired with two fMRIs with the same name (fMRI_Resting_State_AP) I don't know how to include the third run…]\nI deleted all the other .nii.gz and .json files in order not to confuse BIDS, and also moved the dicom files we aren't interested in to another directory\n\nBuild your configuration file with the help of the content of tmp_dcm2bids/helper:\n\nNow we will create a configuration file called dcm2bids_config.json (this is just an example, it could be whatever we want) in the code/ folder.\nUse any text editor to create the file with the contents:\n{\n    \"descriptions\": [\n        {\n        \"suffix\": \"anat\",\n        \"modalitylabel\": \"T1w\",\n        \"criteria\": {\n            \"SeriesDescription\": \"SAG_FSPGR_3D_.9X.9X.9*\"\n            }\n        },\n        {\n        \"suffix\": \"func\",\n        \"modalitylabel\": \"bold\",\n        \"custom_entities\": \"task-rest-AP\",\n        \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_AP*\"\n            }\n        },\n        {\n        \"suffix\": \"func\",\n        \"modalitylabel\": \"bold\",\n        \"custom_entities\": \"task-rest-PA\",\n        \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_PA*\"\n            }\n        }\n    ]\n}\n\n\n\ndcm2bids -d sourcedata/aweber-20200918_110552/LONGMBfMRI/ -p 01 -c code/dcm2bids_config.json\nA bunch of information is printed to the terminal and we can verify that the data are now in BIDS.\nls sub-01/*\nsub-01/anat:\nsub-01_T1w.json sub-01_T1w.nii.gz\nsub-01/func:\nsub-01_task-rest_bold.json sub-01_task-rest_bold.nii.gz\nFollow the tutorial\ndcm2bids creates log files inside &lt;YOUR_FUTURE_BIDS_FOLDER&gt;/tmp_dcm2bids/log\nRun the bids-validator to check your directory. Don't forget to create a .bidsignore file at the root of your BIDS directory with tmp_dcm2bids/* inside.\nnano .bidsignore\nfill with:\ntmp_dcm2bids",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#automate",
    "href": "tutorials/BIDS.html#automate",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Automate",
    "text": "Automate\nHere is a tutorial on how to create a bash shell script to automate this process\nhttps://reproducibility.stanford.edu/bids-tutorial-series-part-1b/",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#jo",
    "href": "tutorials/BIDS.html#jo",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "jo",
    "text": "jo\njo is a simple program that makes writing json files easier (I think)\nhttps://github.com/jpmens/jo\nExample:\njo -p descriptions=$(jo -a $(jo datatype=anat suffix=T1w criteria=$(jo SeriesDescription=SAG_FSPGR_3D_.9X_.9X.9*)) $(jo datatype=func suffix=bold custom_entities=task-rest-AP criteria=$(jo SeriesDescription=fMRI_Resting_State_AP*)))\nResults in:\n{\n   \"descriptions\": [\n      {\n         \"datatype\": \"anat\",\n         \"suffix\": \"T1w\",\n         \"criteria\": {\n            \"SeriesDescription\": \"SAG_FSPGR_3D_.9X_.9X.9*\"\n         }\n      },\n      {\n         \"datatype\": \"func\",\n         \"suffix\": \"bold\",\n         \"custom_entities\": \"task-rest-AP\",\n         \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_AP*\"\n         }\n      }\n   ]\n}",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#install-1",
    "href": "tutorials/BIDS.html#install-1",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Install",
    "text": "Install\nsudo apt-get install nodejs\nsudo apt-get install npm\nsudo npm install -g bids-validator",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#run",
    "href": "tutorials/BIDS.html#run",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Run",
    "text": "Run\nbids-validator .\nOr, if you're using docker:\ndocker run -ti --rm -v /your/projects/folder/for/example:/data:ro bids/validator /data\nbut change /your/projects/folder/for/example to where your BIDS files are.\nIf you are in the directory already, then '.' is your current directory.\n\n\n\n\n\n\nImportant\n\n\n\nDon't forget to create a .bidsignore file at the root of your BIDS directory with tmp_dcm2bids/* inside.",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#documentation",
    "href": "tutorials/BIDS.html#documentation",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Documentation",
    "text": "Documentation\nPlease take a look at the documentation to:\n\nInstall dcm2bids\nFollow the tutorial\nSeek for more advanced usage",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html",
    "href": "mrisoftware/fmriprep.html",
    "title": "fmriprep",
    "section": "",
    "text": "fMRI Prep is an automated fMRI preprocessing tool\nYou can visit their website here",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#info-and-tutorials",
    "href": "tutorials/BIDS.html#info-and-tutorials",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Info and Tutorials",
    "text": "Info and Tutorials\nPlease take a look at the documentation to:\n\nInstall dcm2bids\nFollow the tutorial\nSeek for more advanced usage\n\n\nAnother tutorial",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "tutorials/BIDS.html#example-walkthrough",
    "href": "tutorials/BIDS.html#example-walkthrough",
    "title": "BIDS (Brain Imaging Data Structure)",
    "section": "Example Walkthrough",
    "text": "Example Walkthrough\n\n1. Create a root BIDS directory\nCreate and enter your root BIDS directory where everything will live:\ncd /your/projects/folder/for/example/ && mkdir 2020-09-17_BCCHR_test_AlexW/\ncd 2020-09-17_BCCHR_test_AlexW/\n\n\n2. Create the basic architecture of your directory\ndcm2bids_scaffold\nThis gives us several generics folders and files.\nOne of the created folders is named 'sourcedata/'\ndata from Xnat is downloaded as a zip into 'sourcedata/'\n\nthen:\ncd sourcedata\nunzip aweber-20200918_110552.zip\nwhich extracts the dicoms into a folder: aweber-20200918_110552/\n\ninside of which are numbered subfolders for each acquisition scan\ngoing back to /your/projects/folder/for/example/2020-09-17_BCCHR_test_AlexW/\ncd ..\n\n\n3. Use the helper function to get a sense of what NIfTI files and json sidecar files you are working with\ndcm2bids_helper -d sourcedata/aweber-20200918_110552/\nThis command will convert the DICOMs files to NIfTI files and save them inside 'tmp_dcm2bids/helper/'\n[note: with GE data, I find this gives bad naming conventions. Alternatively you could run something like this:\ndcm2niix -o tmp_dcm2bids/helper -b y -ba y -z y -f ‘%3s %f %d %r’ sourcedata/AMW01/\nwhich translates to:\n-o = output folder [note: this folder should already exist]\n-b y = BIDS sidecar yes\n-ba y = anonymize BIDS yes\n-z y = gz compress images yes\n-f '%3s %f %d %r' = filename with series number, folder name,\ndescription and instance number]\nWe should see a list of compressed NIfTI files (nii.gz) with their respective sidecar files (.json).\nFor this example, we will imagine we only have three acquisitions:\n003_LONGMBfMRI_SAG_FSPGR_3D\\_.9X.9X.9_20200917132212.json\\\n003_LONGMBfMRI_SAG_FSPGR_3D\\_.9X.9X.9_20200917132212.nii.gz\\\n005_LONGMBfMRI_fMRI_Resting_State_AP_20200917132212.json\\\n005_LONGMBfMRI_fMRI_Resting_State_AP_20200917132212.nii.gz\\\n006_LONGMBfMRI_fMRI_Resting_State_PA_20200917132212.json\\\n006_LONGMBfMRI_fMRI_Resting_State_PA_20200917132212.nii.gz\n\n\n4. Build your configuration file with the help of the content of\n`tmp_dcm2bids/helper`:\nNow we will create a configuration file called dcm2bids_config.json (this is just an example, it could be whatever we want) in the code/ folder.\nUse any text editor to create the file with the contents:\n{\n    \"descriptions\": [\n        {\n        \"datatype\": \"anat\",\n        \"suffix\": \"T1w\",\n        \"criteria\": {\n            \"SeriesDescription\": \"SAG_FSPGR_3D_.9X.9X.9*\"\n            }\n        },\n        {\n        \"datatype\": \"func\",\n        \"suffix\": \"bold\",\n        \"custom_entities\": \"task-rest-AP\",\n        \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_AP*\"\n            }\n        },\n        {\n        \"datatype\": \"func\",\n        \"suffix\": \"bold\",\n        \"custom_entities\": \"task-rest-PA\",\n        \"criteria\": {\n            \"SeriesDescription\": \"fMRI_Resting_State_PA*\"\n            }\n        }\n    ]\n}\n\n\n5. Run dcm2bids\nNow we are ready to run dcm2bids with our config file:\ndcm2bids -d sourcedata/aweber-20200918_110552/LONGMBfMRI \\\n  -p 01 -c code/dcm2bids_config.json\nwhere: -d is the sourcedirectory; -p is the subject ID; and -c is the config file.\nA bunch of information is printed to the terminal and we can verify that the data are now in BIDS.\nls sub-01/*\nsub-01/anat:\\\nsub-01_T1w.json sub-01_T1w.nii.gz\n\nsub-01/func:\\\nsub-01_task-rest_bold.json sub-01_task-rest_bold.nii.gz\ndcm2bids creates log files inside /your/projects/folder/for/example/tmp_dcm2bids/log\n\n\n7. Validate\nRun the bids-validator to check your directory.\nThis needs to be installed separately; or you can use the web interface and upload your data.\nSee Section 9\n\n\n\n\n\n\nTip\n\n\n\nDon't forget to create a .bidsignore file at the root of your BIDS directory with tmp_dcm2bids/* inside.\necho tmp_dcm2bids &gt;&gt; .bidsignore",
    "crumbs": [
      "Home",
      "Tutorials",
      "BIDS (Brain Imaging Data Structure)"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#options",
    "href": "mrisoftware/fmriprep.html#options",
    "title": "fmriprep",
    "section": "Options",
    "text": "Options\n--dummy-scans NUMBER_DUMMY_SCANS\nRemove first several scans if they have not reached steady state (you can see this in a timeseries of the fmri, where there is a huge drop off from the beginning. Delete these time-points)\n--use-aroma\nadd ICA_AROMA to your preprocessing stream (default: False)\n--fd-spike-threshold REGRESSORS_FD_TH\nThreshold for flagging a frame as an outlier on the basis of framewise displacement (default: 0.5)",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#confounds",
    "href": "mrisoftware/fmriprep.html#confounds",
    "title": "fmriprep",
    "section": "Confounds",
    "text": "Confounds\nAfter it is done running, you should have confounds that are in a .tsv file in the func/ folder\nYou might have a lot of columns of data:\n\n|global_signal | global_signal_derivative1 | global_signal_derivative1_power2 | global_signal_power2 | csf | csf_derivative1 | csf_power2 | csf_derivative1_power2 | white_matter | white_matter_derivative1 | white_matter_derivative1_power2 | white_matter_power2 | csf_wm | tcompcor | std_dvars | dvars | framewise_displacement | rmsd | t_comp_cor_00 | t_comp_cor_01 | t_comp_cor_02 | t_comp_cor_03 | t_comp_cor_04 | t_comp_cor_05 | a_comp_cor_00 | a_comp_cor_01 | a_comp_cor_02 | a_comp_cor_03 | a_comp_cor_04 | a_comp_cor_05 | a_comp_cor_06 | a_comp_cor_07 | a_comp_cor_08 | a_comp_cor_09 | a_comp_cor_10 | a_comp_cor_11 | a_comp_cor_12 | a_comp_cor_13 | a_comp_cor_14 | a_comp_cor_15 | a_comp_cor_16 | a_comp_cor_17 | a_comp_cor_18 | a_comp_cor_19 | a_comp_cor_20 | a_comp_cor_21 | a_comp_cor_22 | a_comp_cor_23 | a_comp_cor_24 | a_comp_cor_25 | a_comp_cor_26 | a_comp_cor_27 | a_comp_cor_28 | a_comp_cor_29 | a_comp_cor_30 | a_comp_cor_31 | a_comp_cor_32 | a_comp_cor_33 | a_comp_cor_34 | a_comp_cor_35 | a_comp_cor_36 | a_comp_cor_37 | a_comp_cor_38 | a_comp_cor_39 | a_comp_cor_40 | a_comp_cor_41 | a_comp_cor_42 | a_comp_cor_43 | a_comp_cor_44 | a_comp_cor_45 | a_comp_cor_46 | a_comp_cor_47 | a_comp_cor_48 | a_comp_cor_49 | a_comp_cor_50 | a_comp_cor_51 | a_comp_cor_52 | a_comp_cor_53 | a_comp_cor_54 | cosine00 | cosine01 | cosine02 | cosine03 | cosine04 | cosine05 | trans_x | trans_x_derivative1 | trans_x_power2 | trans_x_derivative1_power2 | trans_y | trans_y_derivative1 | trans_y_derivative1_power2 | trans_y_power2 | trans_z | trans_z_derivative1 | trans_z_derivative1_power2 | trans_z_power2 | rot_x | rot_x_derivative1 | rot_x_derivative1_power2 | rot_x_power2 | rot_y | rot_y_derivative1 | rot_y_power2 | rot_y_derivative1_power2 | rot_z | rot_z_derivative1 | rot_z_power2 | rot_z_derivative1_power2 | motion_outlier00 | motion_outlier01 | motion_outlier02 | motion_outlier03 | motion_outlier04 | motion_outlier05 | motion_outlier06 | motion_outlier07 | motion_outlier08 | motion_outlier09 | motion_outlier10 | motion_outlier11 | motion_outlier12 | motion_outlier13 | motion_outlier14 | motion_outlier15 | motion_outlier16 | motion_outlier17 | motion_outlier18 | motion_outlier19 | motion_outlier20 | motion_outlier21 | motion_outlier22 | motion_outlier23 | motion_outlier24 | motion_outlier25 | motion_outlier26 | motion_outlier27 | motion_outlier28 | motion_outlier29 | motion_outlier30 | motion_outlier31 | motion_outlier32 | motion_outlier33 | motion_outlier34 | motion_outlier35 | motion_outlier36 | motion_outlier37 | motion_outlier38 | motion_outlier39 | motion_outlier40 | motion_outlier41 | motion_outlier42 | motion_outlier43 | motion_outlier44 | motion_outlier45 | motion_outlier46 | motion_outlier47 | motion_outlier48 | motion_outlier49 | motion_outlier50 | motion_outlier51 | motion_outlier52 | motion_outlier53 | motion_outlier54 | motion_outlier55 | motion_outlier56 | motion_outlier57 | motion_outlier58 | motion_outlier59 | motion_outlier60 | motion_outlier61 | motion_outlier62|\n\nThe most commonly used confounding time series:\n\nEstimated head-motion parameters: trans_x, trans_y, trans_z, rot_x, rot_y, rot_z - the 6 rigid-body motion parameters (3 translations and 3 rotation), estimated relative to a reference image;\nGlobal signals:\n\ncsf - the average signal within anatomically-derived eroded CSF mask;\nwhite_matter - the average signal within the anatomically-derived eroded WM masks;\nglobal_signal - the average signal within the brain mask.\n\n\nJeff says:\n\nI regress the 6 motion correction parameters, framewise_displacement, a_comp_cor_00-a_comp_cor_05, and any nonsteady_state\n\n\nDenoiser\nclone this somewhere in your $PATH (or add the path to your $PATH): https://github.com/arielletambini/denoiser\nExample\nrun_denoise.py --col_names 'a_comp_cor_00' 'a_comp_cor_01' 'a_comp_cor_02' 'a_comp_cor_03' 'a_comp_cor_04' 'a_comp_cor_05' 'framewise_displacement' 'trans_x' 'trans_y' 'trans_z' 'rot_x' 'rot_y' 'rot_z' --hp_filter .009 --lp_filter .1 sub-01_ses-01_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz sub-01_ses-01_task-rest_desc-confounds_timeseries.tsv denoise\nwhere 'denoise' is a folder I made\n\n\nFSL regfilt\nAlternatively you can use fsl_regfilt with numbers corresponding to columns. This this example, I am removing the components that AROMA identified (column numbers are in AROMAnoiseICs.csv):\nfsl_regfilt -i sub-03_ses-02_task-rest_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz -d sub-03_ses-02_task-rest_run-1_desc-MELODIC_mixing.tsv -o filtered_func_data_clean.nii.gz -f $(cat sub-03_ses-02_task-rest_run-1_AROMAnoiseICs.csv)",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#smoothing",
    "href": "mrisoftware/fmriprep.html#smoothing",
    "title": "fmriprep",
    "section": "Smoothing",
    "text": "Smoothing\nYou will need AFNI installed for this:\n3dmerge -1blur_fwhm 5.0 -doall -prefix _blur.nii boldfile.nii.gz\nfor a 5mm smoothing",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#scaling",
    "href": "mrisoftware/fmriprep.html#scaling",
    "title": "fmriprep",
    "section": "Scaling",
    "text": "Scaling\nWe will also have to scale the data to ensure that the mean BOLD signal across the run is 100, so that any deflections can be expressed in percent signal change. We will also use each run’s corresponding mask that was generated by fMRIPrep:\n3dTstat -prefix rm.mean_r${run}.nii r${run}_blur.nii\n3dcalc -a r${run}_blur.nii -b rm.mean_r${run}.nii \\\n       -c sub-08_task-flanker_run-${run}_space-MNI152NLin2009cAsym_res-2_desc-brain_mask.nii.gz                            \\\n       -expr 'c * min(200, a/b*100)*step(a)*step(b)'       \\\n       -prefix r${run}_scale.nii",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#resuming-after-a-crash",
    "href": "mrisoftware/fmriprep.html#resuming-after-a-crash",
    "title": "fmriprep",
    "section": "Resuming After a Crash:",
    "text": "Resuming After a Crash:\nThis might be useful:\nhttps://github.com/nipreps/fmriprep/issues/1731\nOr these options:\nhttps://fmriprep.org/en/stable/usage.html#reusing-precomputed-derivatives\n\nReusing a previous, partial execution of fMRIPrep\nfMRIPrep will pick up where it left off a previous execution, so long as the work directory points to the same location, and this directory has not been changed/manipulated.\n\n\nUsing a previous run of FreeSurfer\nfMRIPrep will automatically reuse previous runs of FreeSurfer if a subject directory named freesurfer/ is found in the output directory (&lt;output_dir&gt;/freesurfer). Reconstructions for each participant will be checked for completeness, and any missing components will be recomputed. You can use the —fs-subjects-dir flag to specify a different location to save FreeSurfer outputs. If precomputed results are found, they will be reused.\n\n\nThe anatomical fast-track\nStarting with version 20.1.0, fMRIPrep has a command-line argument (—anat-derivatives &lt;PATH&gt;) to indicate a path from which the preprocessed information derived from the T1w, T2w (if present) and FLAIR (if present) images. This feature was envisioned to help process very large multi-session datasets where the anatomical images can be averaged (i.e., anatomy is not expected to vary substantially across sessions). An example where this kind of processing would be useful is My Connectome, a dataset that contains 107 sessions for a single-subject. Most of these sessions contain anatomical information which, given the design of the dataset, can be averaged across sessions as no substantial changes should happen. In other words, the anatomical information of the dataset can be considered as cross-sectional. Before version 20.1.0, preprocessing this dataset would be hard for two limitations:\n\nif the dataset were to be processed in just one enormous job (be it in a commercial Cloud or HPC resources), the amount of data to be processed surely would exceed the time limitations per job (and/or related issues, such as restarting from where it left before); or\nif the processing were split in sessions, then fMRIPrep would attempt to re-process the anatomical information for every session.\n\nBecause processing this emerging type of datasets (densely sampled neuroimaging) was impractical with fMRIPrep, the option —anat-derivatives will shortcut the whole anatomical processing.",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#ubuntu",
    "href": "mrisoftware/fmriprep.html#ubuntu",
    "title": "fmriprep",
    "section": "Ubuntu",
    "text": "Ubuntu\nHere are the steps I took to install on Ubuntu:\n\nInstall Docker\nsudo apt-get update\nsudo aptitude install apt-transport-https ca-certificates curl gnupg-agent software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\nwhich printed out:\n&gt; \"pub rsa4096 2017-02-22 [SCEA]\n9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88\nuid [ unknown] Docker Release (CE deb) &lt;docker@docker.com&gt;\nsub rsa4096 2017-02-22 [S]\"\nsudo add-apt-repository \\\n    \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n    $(lsb_release -cs) \\\n    stable\"\n\nsudo apt-get update && sudo apt-get install docker-ce docker-ce-cli containerd.io\n\ndocker run hello-world\nWhich should print:\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n0e03bdcc26d7: Pull complete\nDigest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc\nStatus: Downloaded newer image for hello-world:latest\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n(amd64)\n3. The Docker daemon created a new container from that image which runs the\nexecutable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\nto your terminal.\nTo try something more ambitious, you can run an Ubuntu container with:\ndocker run -it ubuntu bash\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n\n\n\nInstall fmri-prep docker\npython -m pip install --user --upgrade fmriprep-docker\nWhich should print:\n\"Collecting fmriprep-docker\nDownloading fmriprep_docker-20.1.3-py2.py3-none-any.whl (20.1 MB)\n|████████████████████████████████| 20.1 MB 434 kB/s\nInstalling collected packages: fmriprep-docker\nSuccessfully installed fmriprep-docker-20.1.3\"\nBefore you can run frmi-prep, you also need to install FreeSurfer (I know, what a pain) and register for the license (it's free)\nAfter downloading the freesurfer tar.gz file:\ncd $HOME\ntar -zxvpf freesurfer-linux-centos7_x86_64-7.1.1.tar.gz\nexport FREESURFER_HOME=$HOME/freesurfer\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\nAnd to test if it worked:\nwhich freeview\nThen type:\nfmriprep-docker\nWhich should print:\n\"Downloading. This may take a while…\" and then a bunch of other stuff\nIf it doesn't: try restarting your computer",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#if-you-are-getting-a-permission-denied-issue-after-installation",
    "href": "mrisoftware/fmriprep.html#if-you-are-getting-a-permission-denied-issue-after-installation",
    "title": "fmriprep",
    "section": "If you are getting a Permission Denied issue after installation:",
    "text": "If you are getting a Permission Denied issue after installation:\nhttps://docs.docker.com/engine/install/linux-postinstall/",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#running-out-of-space",
    "href": "mrisoftware/fmriprep.html#running-out-of-space",
    "title": "fmriprep",
    "section": "Running Out of Space:",
    "text": "Running Out of Space:\nhttps://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169\nhttps://briancaffey.github.io/2017/11/28/remove-root-partition-bloat-from-docker.html/\n\nExample\nExample of an error message you might get:\n\n-bash: echo: write error: No space left on device\n\nor\n\nError: writing output failed: No space left on device\n\nHow to check out much space is used:\nFirst thing I will do is run df -H to check the state of the drives and how much space they have:\n[]\nFrom the above screenshot, we see that /dev/nvme0n1p5 is mounted on / , and we see that is has 0 space available\nOk, next we want to see which directories are taking up the most space: [note: this took a painful amount of time]\ncd / && sudo du -h --max-depth=1 | sort -n\nWhich gives us:\n[]\n240G in /home looks big\n131G in /var too\n78G in /usr\nrepeating this process (sudo du -h --max-depth=1 | sort -n) eventually reveals that /var/lib/docker has 121G",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-Options",
    "href": "mrisoftware/fmriprep.html#fmriprep-Options",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "--dummy-scans NUMBER_DUMMY_SCANS\n\n\nRemove first several scans if they have not reached steady state (you can see this in a timeseries of the fmri, where there is a huge drop off from the beginning. Delete these time-points)\n\n\n--use-aroma\n\n\nadd ICA_AROMA to your preprocessing stream (default: False)\n\n\n--fd-spike-threshold REGRESSORS_FD_TH\n\n\nThreshold for flagging a frame as an outlier on the basis of framewise displacement (default: 0.5)",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-Confounds",
    "href": "mrisoftware/fmriprep.html#fmriprep-Confounds",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "After it is done running, you should have confounds that are in a .tsv file in the func/ folder\nYou might have a lot of columns of data:\n\n|global_signal | global_signal_derivative1 | global_signal_derivative1_power2 | global_signal_power2 | csf | csf_derivative1 | csf_power2 | csf_derivative1_power2 | white_matter | white_matter_derivative1 | white_matter_derivative1_power2 | white_matter_power2 | csf_wm | tcompcor | std_dvars | dvars | framewise_displacement | rmsd | t_comp_cor_00 | t_comp_cor_01 | t_comp_cor_02 | t_comp_cor_03 | t_comp_cor_04 | t_comp_cor_05 | a_comp_cor_00 | a_comp_cor_01 | a_comp_cor_02 | a_comp_cor_03 | a_comp_cor_04 | a_comp_cor_05 | a_comp_cor_06 | a_comp_cor_07 | a_comp_cor_08 | a_comp_cor_09 | a_comp_cor_10 | a_comp_cor_11 | a_comp_cor_12 | a_comp_cor_13 | a_comp_cor_14 | a_comp_cor_15 | a_comp_cor_16 | a_comp_cor_17 | a_comp_cor_18 | a_comp_cor_19 | a_comp_cor_20 | a_comp_cor_21 | a_comp_cor_22 | a_comp_cor_23 | a_comp_cor_24 | a_comp_cor_25 | a_comp_cor_26 | a_comp_cor_27 | a_comp_cor_28 | a_comp_cor_29 | a_comp_cor_30 | a_comp_cor_31 | a_comp_cor_32 | a_comp_cor_33 | a_comp_cor_34 | a_comp_cor_35 | a_comp_cor_36 | a_comp_cor_37 | a_comp_cor_38 | a_comp_cor_39 | a_comp_cor_40 | a_comp_cor_41 | a_comp_cor_42 | a_comp_cor_43 | a_comp_cor_44 | a_comp_cor_45 | a_comp_cor_46 | a_comp_cor_47 | a_comp_cor_48 | a_comp_cor_49 | a_comp_cor_50 | a_comp_cor_51 | a_comp_cor_52 | a_comp_cor_53 | a_comp_cor_54 | cosine00 | cosine01 | cosine02 | cosine03 | cosine04 | cosine05 | trans_x | trans_x_derivative1 | trans_x_power2 | trans_x_derivative1_power2 | trans_y | trans_y_derivative1 | trans_y_derivative1_power2 | trans_y_power2 | trans_z | trans_z_derivative1 | trans_z_derivative1_power2 | trans_z_power2 | rot_x | rot_x_derivative1 | rot_x_derivative1_power2 | rot_x_power2 | rot_y | rot_y_derivative1 | rot_y_power2 | rot_y_derivative1_power2 | rot_z | rot_z_derivative1 | rot_z_power2 | rot_z_derivative1_power2 | motion_outlier00 | motion_outlier01 | motion_outlier02 | motion_outlier03 | motion_outlier04 | motion_outlier05 | motion_outlier06 | motion_outlier07 | motion_outlier08 | motion_outlier09 | motion_outlier10 | motion_outlier11 | motion_outlier12 | motion_outlier13 | motion_outlier14 | motion_outlier15 | motion_outlier16 | motion_outlier17 | motion_outlier18 | motion_outlier19 | motion_outlier20 | motion_outlier21 | motion_outlier22 | motion_outlier23 | motion_outlier24 | motion_outlier25 | motion_outlier26 | motion_outlier27 | motion_outlier28 | motion_outlier29 | motion_outlier30 | motion_outlier31 | motion_outlier32 | motion_outlier33 | motion_outlier34 | motion_outlier35 | motion_outlier36 | motion_outlier37 | motion_outlier38 | motion_outlier39 | motion_outlier40 | motion_outlier41 | motion_outlier42 | motion_outlier43 | motion_outlier44 | motion_outlier45 | motion_outlier46 | motion_outlier47 | motion_outlier48 | motion_outlier49 | motion_outlier50 | motion_outlier51 | motion_outlier52 | motion_outlier53 | motion_outlier54 | motion_outlier55 | motion_outlier56 | motion_outlier57 | motion_outlier58 | motion_outlier59 | motion_outlier60 | motion_outlier61 | motion_outlier62|\n\nThe most commonly used confounding time series:\n\nEstimated head-motion parameters: trans_x, trans_y, trans_z, rot_x, rot_y, rot_z - the 6 rigid-body motion parameters (3 translations and 3 rotation), estimated relative to a reference image;\nGlobal signals:\n\ncsf - the average signal within anatomically-derived eroded CSF mask;\nwhite_matter - the average signal within the anatomically-derived eroded WM masks;\nglobal_signal - the average signal within the brain mask.\n\n\nJeff says:\n\nI regress the 6 motion correction parameters, framewise_displacement, a_comp_cor_00-a_comp_cor_05, and any nonsteady_state\n\n\n\nclone this somewhere in your $PATH (or add the path to your $PATH): https://github.com/arielletambini/denoiser\nExample\n\n\nrun_denoise.py --col_names 'a_comp_cor_00' 'a_comp_cor_01' 'a_comp_cor_02' 'a_comp_cor_03' 'a_comp_cor_04' 'a_comp_cor_05' 'framewise_displacement' 'trans_x' 'trans_y' 'trans_z' 'rot_x' 'rot_y' 'rot_z' --hp_filter .009 --lp_filter .1 sub-01_ses-01_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz sub-01_ses-01_task-rest_desc-confounds_timeseries.tsv denoise\n\n\nwhere 'denoise' is a folder I made\n\n\n\nAlternatively you can use fsl_regfilt with numbers corresponding to columns. This this example, I am removing the components that AROMA identified (column numbers are in AROMAnoiseICs.csv):\n\n\nfsl_regfilt -i sub-03_ses-02_task-rest_run-1_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz -d sub-03_ses-02_task-rest_run-1_desc-MELODIC_mixing.tsv -o filtered_func_data_clean.nii.gz -f $(cat sub-03_ses-02_task-rest_run-1_AROMAnoiseICs.csv)",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-Smoothing",
    "href": "mrisoftware/fmriprep.html#fmriprep-Smoothing",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "You will need AFNI installed for this:\n\n\n3dmerge -1blur_fwhm 5.0 -doall -prefix _blur.nii boldfile.nii.gz\n\n\nfor a 5mm smoothing",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-Scaling",
    "href": "mrisoftware/fmriprep.html#fmriprep-Scaling",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "We will also have to scale the data to ensure that the mean BOLD signal across the run is 100, so that any deflections can be expressed in percent signal change. We will also use each run’s corresponding mask that was generated by fMRIPrep:\n\n\n3dTstat -prefix rm.mean_r${run}.nii r${run}_blur.nii\n3dcalc -a r${run}_blur.nii -b rm.mean_r${run}.nii \\\n       -c sub-08_task-flanker_run-${run}_space-MNI152NLin2009cAsym_res-2_desc-brain_mask.nii.gz                            \\\n       -expr 'c * min(200, a/b*100)*step(a)*step(b)'       \\\n       -prefix r${run}_scale.nii",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-ResumingAfteraCrash:",
    "href": "mrisoftware/fmriprep.html#fmriprep-ResumingAfteraCrash:",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "This might be useful:\nhttps://github.com/nipreps/fmriprep/issues/1731\nOr these options:\nhttps://fmriprep.org/en/stable/usage.html#reusing-precomputed-derivatives\n\n\nfMRIPrep will pick up where it left off a previous execution, so long as the work directory points to the same location, and this directory has not been changed/manipulated.\n\n\n\nfMRIPrep will automatically reuse previous runs of FreeSurfer if a subject directory named freesurfer/ is found in the output directory (&lt;output_dir&gt;/freesurfer). Reconstructions for each participant will be checked for completeness, and any missing components will be recomputed. You can use the —fs-subjects-dir flag to specify a different location to save FreeSurfer outputs. If precomputed results are found, they will be reused.\n\n\n\nStarting with version 20.1.0, fMRIPrep has a command-line argument (—anat-derivatives &lt;PATH&gt;) to indicate a path from which the preprocessed information derived from the T1w, T2w (if present) and FLAIR (if present) images. This feature was envisioned to help process very large multi-session datasets where the anatomical images can be averaged (i.e., anatomy is not expected to vary substantially across sessions). An example where this kind of processing would be useful is My Connectome, a dataset that contains 107 sessions for a single-subject. Most of these sessions contain anatomical information which, given the design of the dataset, can be averaged across sessions as no substantial changes should happen. In other words, the anatomical information of the dataset can be considered as cross-sectional. Before version 20.1.0, preprocessing this dataset would be hard for two limitations:\n\nif the dataset were to be processed in just one enormous job (be it in a commercial Cloud or HPC resources), the amount of data to be processed surely would exceed the time limitations per job (and/or related issues, such as restarting from where it left before); or\nif the processing were split in sessions, then fMRIPrep would attempt to re-process the anatomical information for every session.\n\nBecause processing this emerging type of datasets (densely sampled neuroimaging) was impractical with fMRIPrep, the option —anat-derivatives will shortcut the whole anatomical processing.",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-Ubuntu",
    "href": "mrisoftware/fmriprep.html#fmriprep-Ubuntu",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "Here are the steps I took to install on Ubuntu:\n\n\n\n\nsudo apt-get update\nsudo aptitude install apt-transport-https ca-certificates curl gnupg-agent software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-key fingerprint 0EBFCD88\n\n\nwhich printed out:\n\"pub rsa4096 2017-02-22 [SCEA]\n9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88\nuid [ unknown] Docker Release (CE deb) &lt;docker@docker.com&gt;\nsub rsa4096 2017-02-22 [S]\"\n\n\nsudo add-apt-repository \\\n    \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n    $(lsb_release -cs) \\\n    stable\"\n\nsudo apt-get update && sudo apt-get install docker-ce docker-ce-cli containerd.io\n\ndocker run hello-world\n\n\nWhich should print:\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n0e03bdcc26d7: Pull complete\nDigest: sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc\nStatus: Downloaded newer image for hello-world:latest\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n(amd64)\n3. The Docker daemon created a new container from that image which runs the\nexecutable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\nto your terminal.\nTo try something more ambitious, you can run an Ubuntu container with:\ndocker run -it ubuntu bash\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n\n\n\n\n\n\npython -m pip install --user --upgrade fmriprep-docker\n\n\nWhich should print:\n\"Collecting fmriprep-docker\nDownloading fmriprep_docker-20.1.3-py2.py3-none-any.whl (20.1 MB)\n|████████████████████████████████| 20.1 MB 434 kB/s\nInstalling collected packages: fmriprep-docker\nSuccessfully installed fmriprep-docker-20.1.3\"\nBefore you can run frmi-prep, you also need to install FreeSurfer (I know, what a pain) and register for the license (it's free)\nAfter downloading the freesurfer tar.gz file:\n\n\ncd $HOME\ntar -zxvpf freesurfer-linux-centos7_x86_64-7.1.1.tar.gz\nexport FREESURFER_HOME=$HOME/freesurfer\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\n\nAnd to test if it worked:\n\n\nwhich freeview\n\n\nThen type:\n\n\nfmriprep-docker\n\n\nWhich should print:\n\"Downloading. This may take a while…\" and then a bunch of other stuff\nIf it doesn't: try restarting your computer",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-IfyouaregettingaPermissionDeniedissueafterinstallation:",
    "href": "mrisoftware/fmriprep.html#fmriprep-IfyouaregettingaPermissionDeniedissueafterinstallation:",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "https://docs.docker.com/engine/install/linux-postinstall/",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  },
  {
    "objectID": "mrisoftware/fmriprep.html#fmriprep-RunningOutofSpace:",
    "href": "mrisoftware/fmriprep.html#fmriprep-RunningOutofSpace:",
    "title": "Weber Research Team : fmriprep",
    "section": "",
    "text": "https://forums.docker.com/t/how-do-i-change-the-docker-image-installation-directory/1169\nhttps://briancaffey.github.io/2017/11/28/remove-root-partition-bloat-from-docker.html/\n\n\nExample of an error message you might get:\n\n-bash: echo: write error: No space left on device\n\nor\n\nError: writing output failed: No space left on device\n\nHow to check out much space is used:\nFirst thing I will do is run df -H to check the state of the drives and how much space they have:\n\nFrom the above screenshot, we see that /dev/nvme0n1p5 is mounted on / , and we see that is has 0 space available\nOk, next we want to see which directories are taking up the most space: [note: this took a painful amount of time]\ncd / && sudo du -h --max-depth=1 | sort -n\nWhich gives us:\n\n240G in /home looks big\n131G in /var too\n78G in /usr\nrepeating this process (sudo du -h --max-depth=1 | sort -n) eventually reveals that /var/lib/docker has 121G",
    "crumbs": [
      "Home",
      "MRI Software",
      "fmriprep"
    ]
  }
]